{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.optim\n",
    "from torch.utils import data\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import clip\n",
    "sys.path.append('/home/guest/Documents/Siraj TM/RSCaMa')\n",
    "from model.model_encoder_attMamba import Encoder, AttentiveEncoder\n",
    "from model.model_decoder import DecoderTransformer\n",
    "from utils_tool.utils import *\n",
    "from data.LEVIR_CC.LEVIRCC import LEVIRCCDataset\n",
    "from torch import nn, einsum\n",
    "\n",
    "from PIL import Image\n",
    "from imageio import imread\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path) as f:\n",
    "        file = json.load(f)\n",
    "    f.close()\n",
    "    return file\n",
    "def save_json(file,path):\n",
    "    with open(path,'w') as f:\n",
    "        json.dump(file,f)\n",
    "    f.close()\n",
    "    print(\"Saved Successfully\")\n",
    "def rem_print(word):\n",
    "    t_word = word\n",
    "    for _ in range(100 - len(t_word)):\n",
    "        word = word + ' '\n",
    "    print(word,end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = load_json('assets/vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/Levir-CC-dataset/images/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(\n",
    "                LEVIRCCDataset('CLIP-ViT-B/32','data/Levir-CC-dataset/images', '/home/guest/Documents/Siraj TM/RSCaMa/data/LEVIR_CC/', 'train', '/home/guest/Documents/Siraj TM/RSCaMa/data/LEVIR_CC/tokens/', word_vocab, 42, 1),\n",
    "                batch_size=64, shuffle=True, num_workers=24, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = data.DataLoader(\n",
    "                LEVIRCCDataset('CLIP-ViT-B/32','data/Levir-CC-dataset/images', '/home/guest/Documents/Siraj TM/RSCaMa/data/LEVIR_CC/', 'val', '/home/guest/Documents/Siraj TM/RSCaMa/data/LEVIR_CC/tokens/', word_vocab , 42, 1),\n",
    "                batch_size=64, shuffle=False, num_workers=24, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder('CLIP-ViT-B/32')\n",
    "encoder.fine_tune(True)\n",
    "encoder_trans = AttentiveEncoder(n_layers=3,\n",
    "                                        feature_size=[7, 7, 768],\n",
    "                                        heads=8, dropout=0.1)\n",
    "decoder = DecoderTransformer(decoder_type='transformer_decoder',embed_dim=768,\n",
    "                                    vocab_size=len(word_vocab), max_lengths=42,\n",
    "                                    word_vocab=word_vocab, n_head=8,\n",
    "                                    n_layers=1, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = torch.optim.Adam(params=encoder.parameters(),\n",
    "                                            lr=1e-4) if True else None\n",
    "encoder_trans_optimizer = torch.optim.Adam(\n",
    "    params=filter(lambda p: p.requires_grad, encoder_trans.parameters()),\n",
    "    lr=1e-4)\n",
    "decoder_optimizer = torch.optim.Adam(\n",
    "    params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
    "    lr=1e-4)\n",
    "\n",
    "# Move to GPU, if available\n",
    "encoder = encoder.cuda()\n",
    "encoder_trans = encoder_trans.cuda()\n",
    "decoder = decoder.cuda()\n",
    "encoder_lr_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer, step_size=5,\n",
    "                                                            gamma=1.0) if True else None\n",
    "encoder_trans_lr_scheduler = torch.optim.lr_scheduler.StepLR(encoder_trans_optimizer, step_size=5,\n",
    "                                                                    gamma=1.0)\n",
    "decoder_lr_scheduler = torch.optim.lr_scheduler.StepLR(decoder_optimizer, step_size=5,\n",
    "                                                            gamma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m,batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "encoder.train()\n",
    "encoder_trans.train()\n",
    "decoder.train()\n",
    "\n",
    "\n",
    "for id,batch_data in enumerate(train_loader):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Getting Data and moving to GPU if possible\n",
    "    imgA = batch_data['imgA']\n",
    "    imgB = batch_data['imgB']\n",
    "    token = batch_data['token']\n",
    "    token_len = batch_data['token_len']\n",
    "    imgA = imgA.cuda()\n",
    "    imgB = imgB.cuda()\n",
    "    token = token.cuda()\n",
    "    token_len = token_len.cuda()\n",
    "    \n",
    "    #Feat1 and Feat2\n",
    "    feat1, feat2 = encoder(imgA, imgB)\n",
    "    feat = encoder_trans(feat1, feat2)\n",
    "    scores, caps_sorted, decode_lengths, sort_ind = decoder(feat, token, token_len)\n",
    "    # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "    targets = caps_sorted[:, 1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RSCaMa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
