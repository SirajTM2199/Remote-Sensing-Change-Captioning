{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.optim\n",
    "from torch.utils import data\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import clip\n",
    "sys.path.append('/home/guest/Documents/Siraj TM/RSCaMa')\n",
    "from model.model_encoder_attMamba import Encoder, AttentiveEncoder\n",
    "from model.model_decoder import DecoderTransformer\n",
    "from utils_tool.utils import *\n",
    "from data.LEVIR_CC.LEVIRCC import LEVIRCCDataset\n",
    "from torch import nn, einsum\n",
    "\n",
    "from PIL import Image\n",
    "from imageio import imread\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 27 14:52:18 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A5000               Off |   00000000:65:00.0  On |                  Off |\n",
      "| 30%   41C    P8              9W /  230W |    6707MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A5000               Off |   00000000:B3:00.0 Off |                  Off |\n",
      "| 30%   34C    P8              8W /  230W |      18MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1672      G   /usr/lib/xorg/Xorg                            170MiB |\n",
      "|    0   N/A  N/A      1848    C+G   ...libexec/gnome-remote-desktop-daemon        205MiB |\n",
      "|    0   N/A  N/A      1903      G   /usr/bin/gnome-shell                           87MiB |\n",
      "|    0   N/A  N/A      3994      G   /opt/google/chrome/chrome                      27MiB |\n",
      "|    0   N/A  N/A    189389      G   /opt/brave.com/brave/brave                     27MiB |\n",
      "|    0   N/A  N/A    374750      G   ...irefox/6103/usr/lib/firefox/firefox        151MiB |\n",
      "|    0   N/A  N/A   1721991      C   ...naconda3/envs/RSCaMa_env/bin/python       5984MiB |\n",
      "|    1   N/A  N/A      1672      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path) as f:\n",
    "        file = json.load(f)\n",
    "    f.close()\n",
    "    return file\n",
    "def save_json(file,path):\n",
    "    with open(path,'w') as f:\n",
    "        json.dump(file,f)\n",
    "    f.close()\n",
    "    print(\"Saved Successfully\")\n",
    "def rem_print(word):\n",
    "    t_word = word\n",
    "    for _ in range(100 - len(t_word)):\n",
    "        word = word + ' '\n",
    "    print(word,end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = load_json('assets/vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/Levir-CC-dataset/images/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(\n",
    "                LEVIRCCDataset('CLIP-ViT-B/32','data/Levir-CC-dataset/images', '/home/guest/Documents/Siraj TM/RSCaMa/data/LEVIR_CC/', 'train', '/home/guest/Documents/Siraj TM/RSCaMa/data/LEVIR_CC/tokens/', word_vocab, 42, 1),\n",
    "                batch_size=64, shuffle=True, num_workers=24, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = data.DataLoader(\n",
    "                LEVIRCCDataset('CLIP-ViT-B/32','data/Levir-CC-dataset/images', '/home/guest/Documents/Siraj TM/RSCaMa/data/LEVIR_CC/', 'val', '/home/guest/Documents/Siraj TM/RSCaMa/data/LEVIR_CC/tokens/', word_vocab , 42, 1),\n",
    "                batch_size=64, shuffle=False, num_workers=24, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_n_layers= 1\n",
      "decoder_type= transformer_decoder\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder('CLIP-ViT-B/32')\n",
    "encoder.fine_tune(True)\n",
    "encoder_trans = AttentiveEncoder(n_layers=3,\n",
    "                                        feature_size=[7, 7, 768],\n",
    "                                        heads=8, dropout=0.1,device=device)\n",
    "decoder = DecoderTransformer(decoder_type='transformer_decoder',embed_dim=768,\n",
    "                                    vocab_size=len(word_vocab), max_lengths=42,\n",
    "                                    word_vocab=word_vocab, n_head=8,\n",
    "                                    n_layers=1, dropout=0.1,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 27 14:53:13 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A5000               Off |   00000000:65:00.0  On |                  Off |\n",
      "| 30%   39C    P8             12W /  230W |    6707MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A5000               Off |   00000000:B3:00.0 Off |                  Off |\n",
      "| 30%   37C    P8              9W /  230W |     594MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1672      G   /usr/lib/xorg/Xorg                            170MiB |\n",
      "|    0   N/A  N/A      1848    C+G   ...libexec/gnome-remote-desktop-daemon        205MiB |\n",
      "|    0   N/A  N/A      1903      G   /usr/bin/gnome-shell                           87MiB |\n",
      "|    0   N/A  N/A      3994      G   /opt/google/chrome/chrome                      27MiB |\n",
      "|    0   N/A  N/A    189389      G   /opt/brave.com/brave/brave                     27MiB |\n",
      "|    0   N/A  N/A    374750      G   ...irefox/6103/usr/lib/firefox/firefox        151MiB |\n",
      "|    0   N/A  N/A   1721991      C   ...naconda3/envs/RSCaMa_env/bin/python       5984MiB |\n",
      "|    1   N/A  N/A      1672      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A   1727032      C   ...naconda3/envs/RSCaMa_env/bin/python        570MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = torch.optim.Adam(params=encoder.parameters(),\n",
    "                                            lr=1e-4) if True else None\n",
    "encoder_trans_optimizer = torch.optim.Adam(\n",
    "    params=filter(lambda p: p.requires_grad, encoder_trans.parameters()),\n",
    "    lr=1e-4)\n",
    "decoder_optimizer = torch.optim.Adam(\n",
    "    params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
    "    lr=1e-4)\n",
    "\n",
    "# Move to GPU, if available\n",
    "encoder.cuda(device)\n",
    "encoder_trans.cuda(device)\n",
    "decoder.cuda(device)\n",
    "encoder_lr_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer, step_size=5,\n",
    "                                                            gamma=1.0) if True else None\n",
    "encoder_trans_lr_scheduler = torch.optim.lr_scheduler.StepLR(encoder_trans_optimizer, step_size=5,\n",
    "                                                                    gamma=1.0)\n",
    "decoder_lr_scheduler = torch.optim.lr_scheduler.StepLR(decoder_optimizer, step_size=5,\n",
    "                                                            gamma=1.0)\n",
    "hist = np.zeros((num_epochs*2 * len(train_loader), 5))\n",
    "\n",
    "l_resizeA = torch.nn.Upsample(size = (256, 256), mode ='bilinear', align_corners = True)\n",
    "l_resizeB = torch.nn.Upsample(size = (256, 256), mode ='bilinear', align_corners = True)\n",
    "index_i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.train()\n",
    "encoder_trans.train()\n",
    "decoder.train()\n",
    "criterion_cap = torch.nn.CrossEntropyLoss().cuda(device)\n",
    "criterion_cap_cls = torch.nn.CrossEntropyLoss().cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 0 | Index:200 | Loss: 1.9663019180297852                                            \n",
      "\n",
      "Training Epoch: 1 | Index:300 | Loss: 1.6814106702804565                                            \n",
      "\n",
      "Training Epoch: 2 | Index:400 | Loss: 1.4670134782791138                                            \n",
      "\n",
      "Training Epoch: 3 | Index:500 | Loss: 1.4476122856140137                                            \n",
      "\n",
      "Training Epoch: 4 | Index:600 | Loss: 1.5778120756149292                                            \n",
      "\n",
      "Training Epoch: 5 | Index:700 | Loss: 1.4598031044006348                                            \n",
      "\n",
      "Training Epoch: 5 | Index:800 | Loss: 1.1045427322387695                                            \n",
      "\n",
      "Training Epoch: 6 | Index:900 | Loss: 1.2898590564727783                                            \n",
      "\n",
      "Training Epoch: 7 | Index:1000 | Loss: 1.4540414810180664                                           \n",
      "\n",
      "Training Epoch: 8 | Index:1100 | Loss: 1.3448060750961304                                           \n",
      "\n",
      "Training Epoch: 9 | Index:1200 | Loss: 1.524630069732666                                            \n",
      "\n",
      "Training Epoch: 10 | Index:1300 | Loss: 1.4229037761688232                                          \n",
      "\n",
      "Training Epoch: 11 | Index:1400 | Loss: 1.067972183227539                                           \n",
      "\n",
      "Training Epoch: 12 | Index:1500 | Loss: 1.6517854928970337                                          \n",
      "\n",
      "Training Epoch: 13 | Index:1600 | Loss: 1.1275403499603271                                          \n",
      "\n",
      "Training Epoch: 14 | Index:1700 | Loss: 1.3882057666778564                                          \n",
      "\n",
      "Training Epoch: 15 | Index:1800 | Loss: 1.0649574995040894                                          \n",
      "\n",
      "Training Epoch: 16 | Index:1900 | Loss: 1.0149077177047732                                          \n",
      "\n",
      "Training Epoch: 17 | Index:2000 | Loss: 1.0253939628601074                                          \n",
      "\n",
      "Training Epoch: 18 | Index:2100 | Loss: 1.1958315372467048                                          \n",
      "\n",
      "Training Epoch: 19 | Index:2200 | Loss: 0.8506316542625427                                          \n",
      "\n",
      "Training Epoch: 20 | Index:2300 | Loss: 0.9925713539123535                                          \n",
      "\n",
      "Training Epoch: 20 | Index:2400 | Loss: 1.0626888275146484                                          \n",
      "\n",
      "Training Epoch: 21 | Index:2500 | Loss: 0.8933848142623901                                          \n",
      "\n",
      "Training Epoch: 22 | Index:2600 | Loss: 1.0221828222274786                                          \n",
      "\n",
      "Training Epoch: 23 | Index:2700 | Loss: 1.0049151182174683                                          \n",
      "\n",
      "Training Epoch: 24 | Index:2800 | Loss: 1.0081024169921875                                          \n",
      "\n",
      "Training Epoch: 24 | Index:2834 | Loss: 0.8911718130111694                                          \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print_freq = 100\n",
    "EPOCHS = num_epochs\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for id,batch_data in enumerate(train_loader):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        accum_steps = 64//64\n",
    "        \n",
    "        #Getting Data and moving to GPU if possible\n",
    "        imgA = batch_data['imgA']\n",
    "        imgB = batch_data['imgB']\n",
    "        token = batch_data['token']\n",
    "        token_len = batch_data['token_len']\n",
    "        imgA = imgA.cuda(device)\n",
    "        imgB = imgB.cuda(device)\n",
    "        token = token.cuda(device)\n",
    "        token_len = token_len.cuda(device)\n",
    "        \n",
    "        #Feat1 and Feat2\n",
    "        feat1, feat2 = encoder(imgA, imgB)\n",
    "        feat = encoder_trans(feat1, feat2)\n",
    "        scores, caps_sorted, decode_lengths, sort_ind = decoder(feat, token, token_len)\n",
    "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "        targets = caps_sorted[:, 1:]\n",
    "        \n",
    "        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n",
    "        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
    "        \n",
    "        loss = criterion_cap(scores, targets.to(torch.int64))\n",
    "        \n",
    "        loss = loss / accum_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if (id + 1) % accum_steps == 0 or (id + 1) == len(train_loader):\n",
    "            decoder_optimizer.step()\n",
    "            encoder_trans_optimizer.step()\n",
    "            if encoder_optimizer is not None:\n",
    "                encoder_optimizer.step()\n",
    "\n",
    "            # Adjust learning rate\n",
    "            decoder_lr_scheduler.step()\n",
    "            encoder_trans_lr_scheduler.step()\n",
    "            if encoder_lr_scheduler is not None:\n",
    "                encoder_lr_scheduler.step()\n",
    "\n",
    "            decoder_optimizer.zero_grad()\n",
    "            encoder_trans_optimizer.zero_grad()\n",
    "            if encoder_optimizer is not None:\n",
    "                encoder_optimizer.zero_grad()\n",
    "                \n",
    "        hist[index_i, 0] = time.time() - start_time #batch_time\n",
    "        hist[index_i, 1] = loss.item()  # train_loss\n",
    "        hist[index_i, 2] = accuracy_v0(scores, targets, 5) #top5\n",
    "        \n",
    "        index_i += 1\n",
    "        \n",
    "        if index_i % (print_freq) == 0:\n",
    "            print(f'Training Epoch: {epoch} | Index:{index_i} | Loss: {loss}\\n')\n",
    "        rem_print(f'Training Epoch: {epoch} | Index:{index_i} | Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc7bf2f0df0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist[:,0],hist[:,1])\n",
    "plt.plot(hist[:,0],hist[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(),'data/Pre-Trained Models/encoder.pth')\n",
    "torch.save(encoder_trans.state_dict(),'data/Pre-Trained Models/encoder_trans.pth')\n",
    "torch.save(decoder.state_dict(),'data/Pre-Trained Models/decoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.to(device)\n",
    "encoder_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(index):\n",
    "    \n",
    "    Path = 'data/Levir-CC-dataset/images/'\n",
    "    Image_name = Captions['images'][index]['filename']\n",
    "    dir_path = Path + '/' +  Captions['images'][index]['filepath']\n",
    "    \n",
    "    ImA =  f\"{dir_path}/A/{Image_name}\"\n",
    "    ImB =  f\"{dir_path}/B/{Image_name}\"\n",
    "       \n",
    "    Ground_truth = Captions['images'][index]['sentences'][0]['raw']\n",
    "    \n",
    "    IMA = preprocess(Image.fromarray(\n",
    "        io.imread(ImA)\n",
    "        )).unsqueeze(0).to(device)\n",
    "    \n",
    "    IMB = preprocess(Image.fromarray(\n",
    "        io.imread(ImB)\n",
    "        )).unsqueeze(0).to(device)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        feat1,feat2 = encoder(IMA,IMB)\n",
    "        feat = encoder_trans(feat1,feat2)\n",
    "        return seq\n",
    "        seq = decoder.sample(feat,k=1)\n",
    "        \n",
    "    pred_seq = [w for w in seq if w not in except_tokens]\n",
    "    caption = [invert[token] for token in pred_seq]\n",
    "    \n",
    "    output = ''\n",
    "    for word in caption:\n",
    "        output += word + ' '\n",
    "        \n",
    "    print(f\"Predicted_Caption : {output}\\tGround Truth : {Ground_truth} \")\n",
    "\n",
    "    fig,axes = plt.subplots(1,2)\n",
    "    axes[0].set_title(f\"Before - Index:{index}\")\n",
    "    axes[0].imshow(np.asarray(Image.open(ImA)))\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].set_title(f\"After - Index:{index}\")\n",
    "    axes[1].imshow(np.asarray(Image.open(ImB)))\n",
    "    axes[1].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 2, 3}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Captions = load_json(\"data/Levir-CC-dataset/LevirCCcaptions.json\")\n",
    "test_path = 'data/Levir-CC-dataset/images/test'\n",
    "invert = {val:key for key,val in word_vocab.items()}\n",
    "except_tokens = {word_vocab['<START>'], word_vocab['<END>'], word_vocab['<NULL>']}\n",
    "except_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 22\u001b[0m, in \u001b[0;36mget_tokens\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m     16\u001b[0m IMB \u001b[38;5;241m=\u001b[39m preprocess(Image\u001b[38;5;241m.\u001b[39mfromarray(\n\u001b[1;32m     17\u001b[0m     io\u001b[38;5;241m.\u001b[39mimread(ImB)\n\u001b[1;32m     18\u001b[0m     ))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m     feat1,feat2 \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mIMB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     feat \u001b[38;5;241m=\u001b[39m encoder_trans(feat1,feat2)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m seq\n",
      "File \u001b[0;32m~/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Siraj TM/RSCaMa/model/model_encoder_attMamba.py:67\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, imageA, imageB)\u001b[0m\n\u001b[1;32m     65\u001b[0m     img_A \u001b[38;5;241m=\u001b[39m imageA\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     66\u001b[0m     img_B \u001b[38;5;241m=\u001b[39m imageB\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 67\u001b[0m     clip_emb_A, img_feat_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_A\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     clip_emb_B, img_feat_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_model\u001b[38;5;241m.\u001b[39mencode_image(img_B)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# feat1 = self.cnn(imageA)  # (batch_size, 2048, image_size/32, image_size/32)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# feat2 = self.cnn(imageB)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/clip/model.py:345\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/clip/model.py:225\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 225\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape = [*, width, grid, grid]\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape = [*, width, grid ** 2]\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape = [*, grid ** 2, width]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "l = get_tokens(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m [random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)]:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mget_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 22\u001b[0m, in \u001b[0;36mget_tokens\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m     16\u001b[0m IMB \u001b[38;5;241m=\u001b[39m preprocess(Image\u001b[38;5;241m.\u001b[39mfromarray(\n\u001b[1;32m     17\u001b[0m     io\u001b[38;5;241m.\u001b[39mimread(ImB)\n\u001b[1;32m     18\u001b[0m     ))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m     feat1,feat2 \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mIMB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     feat \u001b[38;5;241m=\u001b[39m encoder_trans(feat1,feat2)\n\u001b[1;32m     24\u001b[0m     seq \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39msample(feat,k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Siraj TM/RSCaMa/model/model_encoder_attMamba.py:67\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, imageA, imageB)\u001b[0m\n\u001b[1;32m     65\u001b[0m     img_A \u001b[38;5;241m=\u001b[39m imageA\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     66\u001b[0m     img_B \u001b[38;5;241m=\u001b[39m imageB\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 67\u001b[0m     clip_emb_A, img_feat_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_A\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     clip_emb_B, img_feat_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_model\u001b[38;5;241m.\u001b[39mencode_image(img_B)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# feat1 = self.cnn(imageA)  # (batch_size, 2048, image_size/32, image_size/32)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# feat2 = self.cnn(imageB)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/clip/model.py:345\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/clip/model.py:225\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 225\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape = [*, width, grid, grid]\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape = [*, width, grid ** 2]\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape = [*, grid ** 2, width]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "for index in [random.randint(0,1000) for _ in range(10)]:\n",
    "    get_tokens(index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RSCaMa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
