{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.optim\n",
    "from torch.utils import data\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import numpy\n",
    "import clip\n",
    "sys.path.append('/home/guest/Documents/Siraj TM/RSCaMa')\n",
    "from model.model_encoder_attMamba import Encoder, AttentiveEncoder\n",
    "from model.model_decoder import DecoderTransformer\n",
    "from utils_tool.utils import *\n",
    "from data.LEVIR_CC.LEVIRCC import LEVIRCCDataset\n",
    "from torch import nn, einsum\n",
    "\n",
    "from PIL import Image\n",
    "from imageio import imread\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path) as f:\n",
    "        file = json.load(f)\n",
    "    f.close()\n",
    "    return file\n",
    "def save_json(file,path):\n",
    "    with open(path,'w') as f:\n",
    "        json.dump(file,f)\n",
    "    f.close()\n",
    "    print(\"Saved Successfully\")\n",
    "def rem_print(word):\n",
    "    t_word = word\n",
    "    for _ in range(100 - len(t_word)):\n",
    "        word = word + ' '\n",
    "    print(word,end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = load_json('data/Levir-CC-dataset/LevirCCcaptions.json')\n",
    "a.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = load_json('assets/vocab.json')\n",
    "\n",
    "Vocab = {key:word_vocab[key] for key in list(word_vocab.keys())[:468]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_n_layers= 1\n",
      "decoder_type= transformer_decoder\n",
      "load model success!\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder('CLIP-ViT-B/32')\n",
    "encoder_trans = AttentiveEncoder(n_layers=3,\n",
    "                                        feature_size=[7, 7, 768],\n",
    "                                        heads=8, dropout=0.1)\n",
    "decoder = DecoderTransformer(decoder_type='transformer_decoder',embed_dim=768,\n",
    "                                    vocab_size=len(Vocab), max_lengths=42,\n",
    "                                    word_vocab=Vocab, n_head=8,\n",
    "                                    n_layers=1, dropout=0.1)\n",
    "\n",
    "checkpoint = torch.load('data/Pre-Trained Models/RSCaMa.pth',map_location=device)\n",
    "\n",
    "encoder.load_state_dict(checkpoint['encoder_dict'])\n",
    "encoder_trans.load_state_dict(checkpoint['encoder_trans_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_dict'])\n",
    "\n",
    "encoder.eval()\n",
    "encoder = encoder.cuda()\n",
    "encoder_trans.eval()\n",
    "encoder_trans = encoder_trans.cuda()\n",
    "decoder.eval()\n",
    "decoder = decoder.cuda()\n",
    "print('load model success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from zipfile import ZipFile \n",
    "with ZipFile(\"DATA/Levir-CC-dataset.zip\", 'r') as zObject: \n",
    "  \n",
    "    # Extracting all the members of the zip  \n",
    "    # into a specific location. \n",
    "    zObject.extractall( \n",
    "        path=\"DATA/Levir-CC-dataset\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 2, 3}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Captions = load_json(\"data/Levir-CC-dataset/LevirCCcaptions.json\")\n",
    "test_path = 'data/Levir-CC-dataset/images/test'\n",
    "invert = {val:key for key,val in word_vocab.items()}\n",
    "except_tokens = {Vocab['<START>'], Vocab['<END>'], Vocab['<NULL>']}\n",
    "except_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(index):\n",
    "    \n",
    "    Path = 'data/Levir-CC-dataset/images/'\n",
    "    Image_name = Captions['images'][index]['filename']\n",
    "    dir_path = Path + '/' +  Captions['images'][index]['filepath']\n",
    "    \n",
    "    ImA =  f\"{dir_path}/A/{Image_name}\"\n",
    "    ImB =  f\"{dir_path}/B/{Image_name}\"\n",
    "       \n",
    "    Ground_truth = Captions['images'][index]['sentences'][0]['raw']\n",
    "    \n",
    "    IMA = preprocess(Image.fromarray(\n",
    "        io.imread(ImA)\n",
    "        )).unsqueeze(0).to(device)\n",
    "    \n",
    "    IMB = preprocess(Image.fromarray(\n",
    "        io.imread(ImB)\n",
    "        )).unsqueeze(0).to(device)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        feat1,feat2 = encoder(IMA,IMB)\n",
    "        feat = encoder_trans(feat1,feat2)\n",
    "        seq = decoder.sample(feat,k=1)\n",
    "    return feat\n",
    "    pred_seq = [w for w in seq if w not in except_tokens]\n",
    "    caption = [invert[token] for token in pred_seq]\n",
    "    \n",
    "    output = ''\n",
    "    for word in caption:\n",
    "        output += word + ' '\n",
    "        \n",
    "    print(f\"Predicted_Caption : {output}\\tGround Truth : {Ground_truth} \")\n",
    "\n",
    "    fig,axes = plt.subplots(1,2)\n",
    "    axes[0].set_title(f\"Before - Index:{index}\")\n",
    "    axes[0].imshow(np.asarray(Image.open(ImA)))\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].set_title(f\"After - Index:{index}\")\n",
    "    axes[1].imshow(np.asarray(Image.open(ImB)))\n",
    "    axes[1].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for index in [random.randint(0,1000) for _ in range(1)]:\n",
    "    s = get_tokens(index)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "seq = decoder.sample(s,k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 399, 346, 206, 399, 344, 28, 40, 3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYSU-CD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference(ImA,ImB):\n",
    "\n",
    "    \n",
    "    IMA = preprocess(Image.fromarray(\n",
    "        io.imread(ImA)\n",
    "        )).unsqueeze(0).to(device)\n",
    "    \n",
    "    IMB = preprocess(Image.fromarray(\n",
    "        io.imread(ImB)\n",
    "        )).unsqueeze(0).to(device)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        feat1,feat2 = encoder(IMA,IMB)\n",
    "        feat = encoder_trans(feat1,feat2)\n",
    "        seq = decoder.sample(feat,k=1)\n",
    "        \n",
    "    pred_seq = [w for w in seq if w not in except_tokens]\n",
    "    caption = [invert[token] for token in pred_seq]\n",
    "    \n",
    "    return caption\n",
    "    output = ''\n",
    "    for word in caption:\n",
    "        output += word + ' '\n",
    "        \n",
    "    print(f\"Predicted_Caption : {output}\\tGround Truth : {Ground_truth} \")\n",
    "\n",
    "    fig,axes = plt.subplots(1,2)\n",
    "    axes[0].set_title(f\"Before - Index:{index}\")\n",
    "    axes[0].imshow(np.asarray(Image.open(ImA)))\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].set_title(f\"After - Index:{index}\")\n",
    "    axes[1].imshow(np.asarray(Image.open(ImB)))\n",
    "    axes[1].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filepath': 'train',\n",
       " 'filename': 'train_000001.png',\n",
       " 'imgid': 0,\n",
       " 'sentences': [{'tokens': ['there', 'is', 'no', 'difference'],\n",
       "   'raw': ' there is no difference .',\n",
       "   'imgid': 0,\n",
       "   'sentid': 0},\n",
       "  {'tokens': ['the', 'two', 'scenes', 'seem', 'identical'],\n",
       "   'raw': ' the two scenes seem identical .',\n",
       "   'imgid': 0,\n",
       "   'sentid': 1},\n",
       "  {'tokens': ['the', 'scene', 'is', 'the', 'same', 'as', 'before'],\n",
       "   'raw': ' the scene is the same as before .',\n",
       "   'imgid': 0,\n",
       "   'sentid': 2},\n",
       "  {'tokens': ['no', 'change', 'has', 'occurred'],\n",
       "   'raw': ' no change has occurred .',\n",
       "   'imgid': 0,\n",
       "   'sentid': 3},\n",
       "  {'tokens': ['almost', 'nothing', 'has', 'changed'],\n",
       "   'raw': ' almost nothing has changed .',\n",
       "   'imgid': 0,\n",
       "   'sentid': 4}],\n",
       " 'split': 'train',\n",
       " 'changeflag': 0,\n",
       " 'sentids': [0, 1, 2, 3, 4]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['squares', 'replacing', 'have', 'squares', 'replaced', 'as', 'before']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f'/home/guest/Documents/Siraj TM/DATA/subset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(directory):\n",
    "    \n",
    "    for image_name in os.listdir(f'{directory}/{folder}/A'):\n",
    "        \n",
    "        file_path = folder\n",
    "        filename = image_name\n",
    "        file_A = f'{directory}/{folder}/A/{filename}'\n",
    "        file_A = f'{directory}/{folder}/B/{filename}'\n",
    "        result = get_inference(A,B)\n",
    "        tokens = result\n",
    "        raw = ''\n",
    "        \n",
    "        for token in tokens:\n",
    "            raw += token\n",
    "        raw += ' .'\n",
    "        \n",
    "        imgid = 0\n",
    "        sentid = 1\n",
    "        image_info = {\n",
    "            'filepath':file_path,\n",
    "            'filename':filename,\n",
    "            'sentences':{\n",
    "                'tokens':tokens,\n",
    "                'raw':raw,\n",
    "                'imgid':imgid,\n",
    "                'sentid':sentid,\n",
    "                },\n",
    "            'split':folder,\n",
    "        }\n",
    "        \n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00802.jpg'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = f'{directory}/{folder}/A/{filename}'\n",
    "B = f'{directory}/{folder}/B/{filename}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "res = get_inference(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['squares', 'replacing', 'have', 'squares', 'replaced', 'as', 'before']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'OUT', 'A']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(f'{directory}/val')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RSCaMa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
