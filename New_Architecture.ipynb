{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f5701f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085f6d6",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dab035",
   "metadata": {},
   "source": [
    "1) Import word-vocab \n",
    "2) Get the new datasets\n",
    "3) Try training\n",
    "4) Try Testing \n",
    "5) Start paper "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea231d0b",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3678567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import clip\n",
    "import sys\n",
    "sys.path.append('/home/guest/Documents/Siraj TM/RSCaMa')\n",
    "from model.model_decoder import DecoderTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48491b9b",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896df65",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20550db8",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dd8ad1",
   "metadata": {},
   "source": [
    "## Architecture imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fbc4d",
   "metadata": {},
   "source": [
    "### Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a00576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = None\n",
    "        self.stride = stride\n",
    "\n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
    "            self.downsample = nn.Sequential(OrderedDict([\n",
    "                (\"-1\", nn.AvgPool2d(stride)),\n",
    "                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
    "                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.avgpool(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a834f00",
   "metadata": {},
   "source": [
    "### AttentionPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e3f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPool2d(nn.Module):\n",
    "    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.spacial_dim = spacial_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n",
    "        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n",
    "\n",
    "        cls_pos = self.positional_embedding[0:1, :]\n",
    "        spatial_pos = F.interpolate(self.positional_embedding[1:,].reshape(1, self.spacial_dim, self.spacial_dim, self.embed_dim).permute(0, 3, 1, 2), size=(H, W), mode='bilinear')\n",
    "        spatial_pos = spatial_pos.reshape(self.embed_dim, H*W).permute(1, 0)\n",
    "        positional_embedding = torch.cat([cls_pos, spatial_pos], dim=0)\n",
    "\n",
    "        x = x + positional_embedding[:, None, :]\n",
    "        x, _ = F.multi_head_attention_forward(\n",
    "            query=x, key=x, value=x,\n",
    "            embed_dim_to_check=x.shape[-1],\n",
    "            num_heads=self.num_heads,\n",
    "            q_proj_weight=self.q_proj.weight,\n",
    "            k_proj_weight=self.k_proj.weight,\n",
    "            v_proj_weight=self.v_proj.weight,\n",
    "            in_proj_weight=None,\n",
    "            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n",
    "            bias_k=None,\n",
    "            bias_v=None,\n",
    "            add_zero_attn=False,\n",
    "            dropout_p=0,\n",
    "            out_proj_weight=self.c_proj.weight,\n",
    "            out_proj_bias=self.c_proj.bias,\n",
    "            use_separate_proj_weight=True,\n",
    "            training=self.training,\n",
    "            need_weights=False\n",
    "        )\n",
    "\n",
    "        x = x.permute(1, 2, 0)\n",
    "        global_feat = x[:, :, 0]\n",
    "        feature_map = x[:, :, 1:].reshape(B, -1, H, W)\n",
    "        return global_feat, feature_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a01507",
   "metadata": {},
   "source": [
    "### CLIPResNetWithAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ad09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPResNetWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
    "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "    - The final pooling layer is a QKV attention instead of an average pool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, output_dim=1024, input_resolution=224, width=64, pretrained=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.pretrained = pretrained\n",
    "        self.output_dim = output_dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        # the 3-layer stem\n",
    "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # residual layers\n",
    "        self._inplanes = width  # this is a *mutable* variable used during construction\n",
    "        self.layer1 = self._make_layer(width, layers[0])\n",
    "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
    "\n",
    "        embed_dim = width * 32  # the ResNet feature dimension\n",
    "        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, 32, output_dim)\n",
    "        # self.init_weights()\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        pretrained = pretrained or self.pretrained\n",
    "        if isinstance(pretrained, str):\n",
    "            checkpoint = torch.jit.load(pretrained, map_location=device).float().state_dict()\n",
    "\n",
    "            state_dict = {}\n",
    "\n",
    "            for k in checkpoint.keys():\n",
    "                if k.startswith('visual.'):\n",
    "                    new_k = k.replace('visual.', '')\n",
    "                    state_dict[new_k] = checkpoint[k]\n",
    "\n",
    "                    if 'positional_embedding' in new_k:\n",
    "                        if self.attnpool.positional_embedding.shape != state_dict[new_k].shape:\n",
    "                            print(f'Resize the pos_embed shape from {state_dict[new_k].shape} to {self.attnpool.positional_embedding.shape}')\n",
    "                            cls_pos = state_dict[new_k][0:1, :]\n",
    "                            H = W = self.input_resolution // 32\n",
    "                            old_h = int(math.sqrt(state_dict[new_k][1:,].shape[0]))\n",
    "                            spatial_pos = F.interpolate(state_dict[new_k][1:,].reshape(1, old_h, old_h, cls_pos.shape[1]).permute(0, 3, 1, 2), size=(H, W), mode='bilinear')\n",
    "                            spatial_pos = spatial_pos.reshape(cls_pos.shape[1], H*W).permute(1, 0)\n",
    "                            positional_embedding = torch.cat([cls_pos, spatial_pos], dim=0)\n",
    "                            state_dict[new_k] = positional_embedding\n",
    "                            assert self.attnpool.positional_embedding.shape == state_dict[new_k].shape\n",
    "\n",
    "            u, w = self.load_state_dict(state_dict, False)\n",
    "            print(u, w, 'are misaligned params in CLIPResNet')\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "\n",
    "        self._inplanes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self._inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        def stem(x):\n",
    "            for conv, bn in [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:\n",
    "                x = self.relu(bn(conv(x)))\n",
    "            x = self.avgpool(x)\n",
    "            return x\n",
    "\n",
    "        x = x.type(self.conv1.weight.dtype)\n",
    "        x = stem(x)\n",
    "\n",
    "        outs = []\n",
    "        x = self.layer1(x)\n",
    "        outs.append(x)\n",
    "        x = self.layer2(x)\n",
    "        outs.append(x)\n",
    "        x = self.layer3(x)\n",
    "        outs.append(x)\n",
    "        x = self.layer4(x)\n",
    "        outs.append(x)\n",
    "\n",
    "        x_global, x_local = self.attnpool(x)\n",
    "        outs.append([x_global, x_local])\n",
    "\n",
    "        return tuple(outs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637d54f2",
   "metadata": {},
   "source": [
    "### ConvModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cfc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModule(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size,padding=0,stride=1):\n",
    "        super(ConvModule, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=kernel_size,\n",
    "                      padding=padding,stride=stride),\n",
    "            nn.BatchNorm2d(num_features=out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0619a",
   "metadata": {},
   "source": [
    "### Feature Pyramid Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a65206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(nn.Module):\n",
    "    \"\"\"Feature Pyramid Network.\n",
    "\n",
    "    This neck is the implementation of `Feature Pyramid Networks for Object\n",
    "    Detection <https://arxiv.org/abs/1612.03144>`_.\n",
    "\n",
    "    Args:\n",
    "        in_channels (list[int]): Number of input channels per scale.\n",
    "        out_channels (int): Number of output channels (used at each scale).\n",
    "        num_outs (int): Number of output scales.\n",
    "        start_level (int): Index of the start input backbone level used to\n",
    "            build the feature pyramid. Default: 0.\n",
    "        end_level (int): Index of the end input backbone level (exclusive) to\n",
    "            build the feature pyramid. Default: -1, which means the last level.\n",
    "        add_extra_convs (bool | str): If bool, it decides whether to add conv\n",
    "            layers on top of the original feature maps. Default to False.\n",
    "            If True, its actual mode is specified by `extra_convs_on_inputs`.\n",
    "            If str, it specifies the source feature map of the extra convs.\n",
    "            Only the following options are allowed\n",
    "\n",
    "            - 'on_input': Last feat map of neck inputs (i.e. backbone feature).\n",
    "            - 'on_lateral': Last feature map after lateral convs.\n",
    "            - 'on_output': The last output feature map after fpn convs.\n",
    "        extra_convs_on_inputs (bool, deprecated): Whether to apply extra convs\n",
    "            on the original feature from the backbone. If True,\n",
    "            it is equivalent to `add_extra_convs='on_input'`. If False, it is\n",
    "            equivalent to set `add_extra_convs='on_output'`. Default to True.\n",
    "        relu_before_extra_convs (bool): Whether to apply relu before the extra\n",
    "            conv. Default: False.\n",
    "        no_norm_on_lateral (bool): Whether to apply norm on lateral.\n",
    "            Default: False.\n",
    "        conv_cfg (dict): Config dict for convolution layer. Default: None.\n",
    "        norm_cfg (dict): Config dict for normalization layer. Default: None.\n",
    "        act_cfg (dict): Config dict for activation layer in ConvModule.\n",
    "            Default: None.\n",
    "        upsample_cfg (dict): Config dict for interpolate layer.\n",
    "            Default: dict(mode='nearest').\n",
    "        init_cfg (dict or list[dict], optional): Initialization config dict.\n",
    "\n",
    "    Example:\n",
    "        >>> import torch\n",
    "        >>> in_channels = [2, 3, 5, 7]\n",
    "        >>> scales = [340, 170, 84, 43]\n",
    "        >>> inputs = [torch.rand(1, c, s, s)\n",
    "        ...           for c, s in zip(in_channels, scales)]\n",
    "        >>> self = FPN(in_channels, 11, len(in_channels)).eval()\n",
    "        >>> outputs = self.forward(inputs)\n",
    "        >>> for i in range(len(outputs)):\n",
    "        ...     print(f'outputs[{i}].shape = {outputs[i].shape}')\n",
    "        outputs[0].shape = torch.Size([1, 11, 340, 340])\n",
    "        outputs[1].shape = torch.Size([1, 11, 170, 170])\n",
    "        outputs[2].shape = torch.Size([1, 11, 84, 84])\n",
    "        outputs[3].shape = torch.Size([1, 11, 43, 43])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 num_outs,\n",
    "                 start_level=0,\n",
    "                 end_level=-1,\n",
    "                 add_extra_convs=False,\n",
    "                 extra_convs_on_inputs=False,\n",
    "                 relu_before_extra_convs=False,\n",
    "                 no_norm_on_lateral=False,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=None,\n",
    "                 act_cfg=None,\n",
    "                 upsample_cfg=dict(mode='nearest'),\n",
    "                 init_cfg=dict(\n",
    "                     type='Xavier', layer='Conv2d', distribution='uniform')):\n",
    "        super().__init__(init_cfg)\n",
    "        assert isinstance(in_channels, list)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_ins = len(in_channels)\n",
    "        self.num_outs = num_outs\n",
    "        self.relu_before_extra_convs = relu_before_extra_convs\n",
    "        self.no_norm_on_lateral = no_norm_on_lateral\n",
    "        self.fp16_enabled = False\n",
    "        self.upsample_cfg = upsample_cfg.copy()\n",
    "\n",
    "        if end_level == -1:\n",
    "            self.backbone_end_level = self.num_ins\n",
    "            assert num_outs >= self.num_ins - start_level\n",
    "        else:\n",
    "            # if end_level < inputs, no extra level is allowed\n",
    "            self.backbone_end_level = end_level\n",
    "            assert end_level <= len(in_channels)\n",
    "            assert num_outs == end_level - start_level\n",
    "        self.start_level = start_level\n",
    "        self.end_level = end_level\n",
    "        self.add_extra_convs = add_extra_convs\n",
    "        assert isinstance(add_extra_convs, (str, bool))\n",
    "        if isinstance(add_extra_convs, str):\n",
    "            # Extra_convs_source choices: 'on_input', 'on_lateral', 'on_output'\n",
    "            assert add_extra_convs in ('on_input', 'on_lateral', 'on_output')\n",
    "        elif add_extra_convs:  # True\n",
    "            if extra_convs_on_inputs:\n",
    "                # For compatibility with previous release\n",
    "                # TODO: deprecate `extra_convs_on_inputs`\n",
    "                self.add_extra_convs = 'on_input'\n",
    "            else:\n",
    "                self.add_extra_convs = 'on_output'\n",
    "\n",
    "        self.lateral_convs = nn.ModuleList()\n",
    "        self.fpn_convs = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.start_level, self.backbone_end_level):\n",
    "            l_conv = ConvModule(\n",
    "                in_channels[i],\n",
    "                out_channels,\n",
    "                kernel_size=1)\n",
    "            fpn_conv = ConvModule(\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1)\n",
    "\n",
    "            self.lateral_convs.append(l_conv)\n",
    "            self.fpn_convs.append(fpn_conv)\n",
    "\n",
    "        # add extra conv layers (e.g., RetinaNet)\n",
    "        extra_levels = num_outs - self.backbone_end_level + self.start_level\n",
    "        if self.add_extra_convs and extra_levels >= 1:\n",
    "            for i in range(extra_levels):\n",
    "                if i == 0 and self.add_extra_convs == 'on_input':\n",
    "                    in_channels = self.in_channels[self.backbone_end_level - 1]\n",
    "                else:\n",
    "                    in_channels = out_channels\n",
    "                extra_fpn_conv = ConvModule(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=3)\n",
    "                self.fpn_convs.append(extra_fpn_conv)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        assert len(inputs) == len(self.in_channels)\n",
    "\n",
    "        # build laterals\n",
    "        laterals = [\n",
    "            lateral_conv(inputs[i + self.start_level])\n",
    "            for i, lateral_conv in enumerate(self.lateral_convs)\n",
    "        ]\n",
    "\n",
    "        # build top-down path\n",
    "        used_backbone_levels = len(laterals)\n",
    "        for i in range(used_backbone_levels - 1, 0, -1):\n",
    "            # In some cases, fixing `scale factor` (e.g. 2) is preferred, but\n",
    "            #  it cannot co-exist with `size` in `F.interpolate`.\n",
    "            if 'scale_factor' in self.upsample_cfg:\n",
    "                laterals[i - 1] = laterals[i - 1] + resize(\n",
    "                    laterals[i], **self.upsample_cfg)\n",
    "            else:\n",
    "                prev_shape = laterals[i - 1].shape[2:]\n",
    "                laterals[i - 1] = laterals[i - 1] + resize(\n",
    "                    laterals[i], size=prev_shape, **self.upsample_cfg)\n",
    "\n",
    "        # build outputs\n",
    "        # part 1: from original levels\n",
    "        outs = [\n",
    "            self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels)\n",
    "        ]\n",
    "        # part 2: add extra levels\n",
    "        if self.num_outs > len(outs):\n",
    "            # use max pool to get more levels on top of outputs\n",
    "            # (e.g., Faster R-CNN, Mask R-CNN)\n",
    "            if not self.add_extra_convs:\n",
    "                for i in range(self.num_outs - used_backbone_levels):\n",
    "                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n",
    "            # add conv layers on top of original feature maps (RetinaNet)\n",
    "            else:\n",
    "                if self.add_extra_convs == 'on_input':\n",
    "                    extra_source = inputs[self.backbone_end_level - 1]\n",
    "                elif self.add_extra_convs == 'on_lateral':\n",
    "                    extra_source = laterals[-1]\n",
    "                elif self.add_extra_convs == 'on_output':\n",
    "                    extra_source = outs[-1]\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                outs.append(self.fpn_convs[used_backbone_levels](extra_source))\n",
    "                for i in range(used_backbone_levels + 1, self.num_outs):\n",
    "                    if self.relu_before_extra_convs:\n",
    "                        outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n",
    "                    else:\n",
    "                        outs.append(self.fpn_convs[i](outs[-1]))\n",
    "        return tuple(outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998528b2",
   "metadata": {},
   "source": [
    "### Mamba Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2dd68d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/anaconda3/envs/RSCaMa_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state:\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "\n",
    "# FIXME\n",
    "from transformers.models.mamba.modeling_mamba import logger, is_fast_path_available, mamba_inner_fn, causal_conv1d_fn, causal_conv1d_update, selective_state_update, selective_scan_fn\n",
    "from transformers.models.mamba.modeling_mamba import MambaRMSNorm, MambaPreTrainedModel, MambaCache, MambaOutput, MambaMixer\n",
    "from transformers import MambaConfig\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define the gating model\n",
    "\n",
    "class MambaMixer(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute ∆, A, B, C, and D the state space parameters and compute the `contextualized_states`.\n",
    "    A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n",
    "    ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n",
    "    and is why Mamba is called **selective** state spaces)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, layer_idx, head_num=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.ssm_state_size = config.state_size\n",
    "        self.conv_kernel_size = config.conv_kernel\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.time_step_rank = config.time_step_rank\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_num = head_num\n",
    "        self.use_conv_bias = config.use_conv_bias\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.intermediate_size,\n",
    "            out_channels=self.intermediate_size,\n",
    "            bias=config.use_conv_bias,\n",
    "            kernel_size=config.conv_kernel,\n",
    "            groups=self.intermediate_size,\n",
    "            padding=config.conv_kernel - 1,\n",
    "        )\n",
    "        self.conv1d_back = nn.Conv1d(\n",
    "            in_channels=self.intermediate_size,\n",
    "            out_channels=self.intermediate_size,\n",
    "            bias=config.use_conv_bias,\n",
    "            kernel_size=config.conv_kernel,\n",
    "            groups=self.intermediate_size,\n",
    "            padding=config.conv_kernel - 1,\n",
    "        )\n",
    "\n",
    "        self.activation = config.hidden_act\n",
    "        self.act = ACT2FN[config.hidden_act]\n",
    "\n",
    "        # projection of the input hidden states\n",
    "        self.in_proj = nn.Linear(self.hidden_size, self.intermediate_size * 2, bias=config.use_bias)\n",
    "        self.in_proj_dif = nn.Linear(self.hidden_size, self.intermediate_size * 2, bias=config.use_bias)\n",
    "        # selective projection used to make dt, B and C input dependant\n",
    "        self.x_proj = nn.Linear(self.intermediate_size, self.time_step_rank + self.ssm_state_size * 2, bias=False)\n",
    "        self.x_proj_back = nn.Linear(self.intermediate_size, self.time_step_rank + self.ssm_state_size * 2, bias=False)\n",
    "        self.x_proj_dif = nn.Linear(self.intermediate_size, self.time_step_rank + self.ssm_state_size * 2, bias=False)\n",
    "        self.x_proj_dif_back = nn.Linear(self.intermediate_size, self.time_step_rank + self.ssm_state_size * 2, bias=False)\n",
    "        # time step projection (discretization)\n",
    "        self.dt_proj = nn.Linear(self.time_step_rank, self.intermediate_size, bias=True)\n",
    "        self.dt_proj_back = nn.Linear(self.time_step_rank, self.intermediate_size, bias=True)\n",
    "        self.dt_proj_dif = nn.Linear(self.time_step_rank, self.intermediate_size, bias=True)\n",
    "        self.dt_proj_dif_back = nn.Linear(self.time_step_rank, self.intermediate_size, bias=True)\n",
    "\n",
    "        self.linear_hid2 = nn.Linear(self.intermediate_size,2*self.intermediate_size, bias=True)\n",
    "        self.linear_hid2_back = nn.Linear(self.intermediate_size, 2*self.intermediate_size, bias=True)\n",
    "\n",
    "        # S4D real initialization. These are not discretized!\n",
    "        # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n",
    "        A = torch.arange(1, self.ssm_state_size + 1, dtype=torch.float32)[None, :]\n",
    "        A = A.expand(self.intermediate_size, -1).contiguous()\n",
    "\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.A_log_back = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(self.intermediate_size))\n",
    "        self.D_back = nn.Parameter(torch.ones(self.intermediate_size))\n",
    "        self.out_proj = nn.Linear(1*self.intermediate_size, self.hidden_size, bias=config.use_bias)\n",
    "        self.out_LN = nn.LayerNorm(self.intermediate_size)\n",
    "        self.use_bias = config.use_bias\n",
    "\n",
    "        if not is_fast_path_available:\n",
    "            logger.warning_once(\n",
    "                \"The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n",
    "                \" is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and\"\n",
    "                \" https://github.com/Dao-AILab/causal-conv1d\"\n",
    "            )\n",
    "\n",
    "    def cuda_kernels_forward(self, hidden_states: torch.Tensor, hidden_states_dif: torch.Tensor, cache_params: Optional[MambaCache] = None, cache_params_2: Optional[MambaCache] = None):\n",
    "        # 1. Gated MLP's linear projection\n",
    "        batch_size, seq_len, _ = hidden_states.shape\n",
    "        flag_one = False\n",
    "        if hidden_states_dif is None:\n",
    "            flag_one = True\n",
    "            hidden_states_dif = hidden_states\n",
    "        hidden_states = torch.cat([hidden_states, hidden_states_dif], dim=0)\n",
    "        # input_hidden_states_dif = hidden_states_dif\n",
    "        projected_states = self.in_proj(hidden_states).transpose(1, 2)\n",
    "        projected_states_dif = self.in_proj_dif(hidden_states).transpose(1, 2)\n",
    "\n",
    "        # process\n",
    "        hidden_states, gate = projected_states.chunk(2, dim=1)\n",
    "        hidden_states_dif, gate_dif = projected_states_dif.chunk(2, dim=1)\n",
    "        # gate = gate[:batch_size]\n",
    "        gate = gate_dif[batch_size:] if not flag_one else gate[:batch_size]\n",
    "\n",
    "        gate_back = gate.flip(-1)\n",
    "\n",
    "        # 2. Convolution sequence transformation\n",
    "        conv_weights = self.conv1d.weight.view(self.conv1d.weight.size(0), self.conv1d.weight.size(2))\n",
    "        # conv_weights_back = self.conv1d_back.weight.view(self.conv1d_back.weight.size(0), self.conv1d_back.weight.size(2))\n",
    "        hidden_states_cat = causal_conv1d_fn(\n",
    "            hidden_states, conv_weights, self.conv1d.bias, activation=self.activation\n",
    "        )\n",
    "        # hidden_states_back = hidden_states.flip(-1)\n",
    "        # hidden_states_back_cat = causal_conv1d_fn(\n",
    "        #     hidden_states_back, conv_weights_back, self.conv1d_back.bias, activation=self.activation\n",
    "        # )\n",
    "        hidden_states_back_cat = hidden_states_cat.flip(-1)\n",
    "\n",
    "        # 3. State Space Model sequence transformation\n",
    "        # 3.a. input varying initialization of time_step, B and C\n",
    "        hidden_states = hidden_states_cat[:batch_size]  # [batch, seq_len, intermediate_size]\n",
    "\n",
    "        ## 反向排序：\n",
    "        hidden_states_back = hidden_states_back_cat[:batch_size]  # [batch, seq_len, intermediate_size]\n",
    "\n",
    "        ssm_parameters = self.x_proj(hidden_states.transpose(1, 2))\n",
    "        time_step, B, C = torch.split(\n",
    "            ssm_parameters, [self.time_step_rank, self.ssm_state_size, self.ssm_state_size], dim=-1\n",
    "        )\n",
    "        discrete_time_step = self.dt_proj.weight @ time_step.transpose(1, 2)\n",
    "        # 反向：\n",
    "        ssm_parameters_back = self.x_proj_back(hidden_states_back.transpose(1, 2))\n",
    "        time_step_back, B_back, C_back = torch.split(\n",
    "            ssm_parameters_back, [self.time_step_rank, self.ssm_state_size, self.ssm_state_size], dim=-1\n",
    "        )\n",
    "        discrete_time_step_back = self.dt_proj_back.weight @ time_step_back.transpose(1, 2)\n",
    "\n",
    "\n",
    "        A = -torch.exp(self.A_log.float())\n",
    "        A_back = -torch.exp(self.A_log_back.float())\n",
    "        # 3.c perform the recurrence y ← SSM(A, B, C)(x)\n",
    "        time_proj_bias = self.dt_proj.bias.float() if hasattr(self.dt_proj, \"bias\") else None\n",
    "        time_proj_bias_back = self.dt_proj_back.bias.float() if hasattr(self.dt_proj_back, \"bias\") else None\n",
    "\n",
    "        scan_outputs, ssm_state = selective_scan_fn(\n",
    "            hidden_states,\n",
    "            discrete_time_step,\n",
    "            A,\n",
    "            B.transpose(1, 2),\n",
    "            C.transpose(1, 2),\n",
    "            self.D.float(),\n",
    "            gate,#None,\n",
    "            time_proj_bias,\n",
    "            delta_softplus=True,\n",
    "            return_last_state=True,\n",
    "        )\n",
    "        scan_outputs_back, ssm_state_back = selective_scan_fn(\n",
    "            hidden_states.flip(-1),\n",
    "            discrete_time_step_back,\n",
    "            A_back,\n",
    "            B_back.transpose(1, 2),\n",
    "            C_back.transpose(1, 2),\n",
    "            self.D_back.float(),\n",
    "            gate_back, #None, #\n",
    "            time_proj_bias_back,\n",
    "            delta_softplus=True,\n",
    "            return_last_state=True,\n",
    "        )\n",
    "\n",
    "        # 4. Final linear projection\n",
    "        contextualized_states = self.out_proj((scan_outputs+scan_outputs_back.flip(-1)).transpose(1, 2))\n",
    "        return contextualized_states\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states, hidden_states_2, cache_params: Optional[MambaCache] = None, cache_params_2: Optional[MambaCache] = None):\n",
    "        if is_fast_path_available and \"cuda\" in self.x_proj.weight.device.type:\n",
    "            return self.cuda_kernels_forward(hidden_states, hidden_states_2, cache_params, cache_params_2)\n",
    "        else:\n",
    "            raise NotImplementedError(\"The fast path is not available\")\n",
    "\n",
    "\n",
    "class CaMambaBlock(nn.Module):\n",
    "    def __init__(self, config, layer_idx, head_num=1, length=49, craft=False):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.residual_in_fp32 = config.residual_in_fp32\n",
    "        self.norm = MambaRMSNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.head_num = head_num\n",
    "        self.length = length\n",
    "        self.craft = craft\n",
    "        self.config.intermediate_size = config.intermediate_size\n",
    "        self.mixer = MambaMixer(config, layer_idx=layer_idx)\n",
    "\n",
    "    def forward(self, hidden_states, hidden_states_2, cache_params: Optional[MambaCache] = None, cache_params_2: Optional[MambaCache] = None,):\n",
    "        residual = hidden_states\n",
    "        # if hidden_states_2==None:\n",
    "        #     residual = hidden_states[:,:,:768]\n",
    "        #     hidden_states = self.linear(hidden_states)\n",
    "        hidden_states = self.norm(hidden_states.to(dtype=self.norm.weight.dtype))\n",
    "        # hidden_states_2 = self.norm(hidden_states_2.to(dtype=self.norm.weight.dtype))\n",
    "        if self.residual_in_fp32:\n",
    "            residual = residual.to(torch.float32)\n",
    "\n",
    "        hidden_states = self.mixer(hidden_states, hidden_states_2, cache_params=cache_params, cache_params_2=cache_params_2)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "class CaMambaModel(MambaPreTrainedModel):\n",
    "    def __init__(self, config, head_num=1, length=49, craft=False):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([CaMambaBlock(config, layer_idx=idx) for idx in range(config.num_hidden_layers)])\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        self.norm_f = MambaRMSNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs_embeds: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds_2: Optional[torch.LongTensor] = None,\n",
    "        cache_params: Optional[MambaCache] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,  # `attention_mask` is passed by the tokenizer and we don't want it\n",
    "    ) -> Union[Tuple, MambaOutput]:\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else (self.config.use_cache if not self.training else False)\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # FIXME\n",
    "        cache_params = None\n",
    "        use_cache = False\n",
    "        cache_params_2 = cache_params\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "        hidden_states_2 = inputs_embeds_2\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        for mixer_block in self.layers:\n",
    "            assert len(self.layers)==1\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                hidden_states = self._gradient_checkpointing_func(mixer_block.__call__, hidden_states, cache_params)\n",
    "            else:\n",
    "                hidden_states = mixer_block(hidden_states, hidden_states_2, cache_params=cache_params, cache_params_2=cache_params_2)\n",
    "\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if use_cache:\n",
    "            cache_params.seqlen_offset += inputs_embeds.shape[1]\n",
    "            cache_params_2.seqlen_offset += inputs_embeds_2.shape[1]\n",
    "\n",
    "        hidden_states = self.norm_f(hidden_states)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, cache_params, all_hidden_states] if v is not None)\n",
    "\n",
    "        return MambaOutput(\n",
    "            last_hidden_state=hidden_states,\n",
    "            cache_params=cache_params if use_cache else None,\n",
    "            hidden_states=all_hidden_states,\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = MambaConfig(num_hidden_layers=1)\n",
    "    model = CaMambaModel(config, head_num=1, length=49, craft=True)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    input_embeds = torch.randn(4, 49, 768).to(device)\n",
    "    input_embeds_2 = torch.randn(4, 49, 768).to(device)\n",
    "    out1 = model(inputs_embeds=input_embeds, inputs_embeds_2=input_embeds_2).last_hidden_state\n",
    "    print('last_hidden_state:')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a095166",
   "metadata": {},
   "source": [
    "### Resblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa0ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class resblock(nn.Module):\n",
    "    \"\"\"\n",
    "    module: Residual Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inchannel, outchannel, stride=1, shortcut=None):\n",
    "        super(resblock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            # nn.Conv2d(inchannel, int(outchannel / 1), kernel_size=1),\n",
    "            # nn.LayerNorm(int(outchannel/2),dim=1),\n",
    "            nn.BatchNorm2d(int(outchannel / 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                int(outchannel / 1),\n",
    "                int(outchannel / 1),\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            # nn.LayerNorm(int(outchannel/2),dim=1),\n",
    "            nn.BatchNorm2d(int(outchannel / 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(int(outchannel / 1), outchannel, kernel_size=1),\n",
    "            # nn.LayerNorm(int(outchannel / 1),dim=1)\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "        )\n",
    "        self.right = shortcut\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        residual = x\n",
    "        out = out + residual\n",
    "        return self.act(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c6995",
   "metadata": {},
   "source": [
    "### Modified Attentive Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80407ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedAttentiveEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified visual transformer block that takes three inputs:\n",
    "    - img_A: before image\n",
    "    - img_B: after image\n",
    "    - change_seg: change segmentation mask\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, feature_size, heads, dropout=0.0):\n",
    "        super(ModifiedAttentiveEncoder, self).__init__()\n",
    "        h_feat, w_feat, channels = feature_size\n",
    "        self.h_feat = h_feat\n",
    "        self.w_feat = w_feat\n",
    "        self.n_layers = n_layers\n",
    "        self.channels = channels\n",
    "        \n",
    "        # position embedding\n",
    "        self.h_embedding = nn.Embedding(h_feat, int(channels / 2))\n",
    "        self.w_embedding = nn.Embedding(w_feat, int(channels / 2))\n",
    "        \n",
    "        # Mamba configs\n",
    "        config_1 = MambaConfig(num_hidden_layers=1, conv_kernel=3, hidden_size=channels)\n",
    "        config_2 = MambaConfig(num_hidden_layers=1, conv_kernel=3, hidden_size=channels)\n",
    "        config_3 = MambaConfig(num_hidden_layers=1, conv_kernel=3, hidden_size=channels)  # For change segmentation\n",
    "        \n",
    "        # Module lists\n",
    "        self.CaMalayer_list = nn.ModuleList([])\n",
    "        self.fuselayer_list = nn.ModuleList([])\n",
    "        self.fuselayer_list_2 = nn.ModuleList([])\n",
    "        self.linear_dif = nn.ModuleList([])\n",
    "        self.linear_img1 = nn.ModuleList([])\n",
    "        self.linear_img2 = nn.ModuleList([])\n",
    "        self.linear_seg = nn.ModuleList([])  # New linear layer for segmentation\n",
    "        self.Conv1_list = nn.ModuleList([])\n",
    "        self.LN_list = nn.ModuleList([])\n",
    "        self.seg_processor = nn.ModuleList([])  # Process segmentation mask\n",
    "        \n",
    "        embed_dim = channels\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            # Mamba blocks for images and segmentation\n",
    "            self.CaMalayer_list.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        CaMambaModel(config_1),  # For img_A\n",
    "                        CaMambaModel(config_1),  # For img_B\n",
    "                        CaMambaModel(config_3),  # For change_seg\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Fusion layers\n",
    "            self.fuselayer_list.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        CaMambaModel(config_2),  # For temporal fusion\n",
    "                        CaMambaModel(config_2),  # For segmentation fusion\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Attention-guided fusion\n",
    "            self.linear_seg.append(nn.Linear(channels, channels))\n",
    "            \n",
    "            # Final fusion conv\n",
    "            self.Conv1_list.append(nn.Conv2d(channels * 3, embed_dim, kernel_size=1))  # Updated to 3 inputs\n",
    "            self.LN_list.append(resblock(embed_dim, embed_dim))\n",
    "            \n",
    "        self.act = nn.Tanh()\n",
    "        self.layerscan = CaMambaModel(config_1)\n",
    "        self.LN_norm = nn.LayerNorm(channels)\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "        self.beta = nn.Parameter(torch.tensor(0.0), requires_grad=True)  # Weight for segmentation contribution\n",
    "        \n",
    "        # Fusion bi-temporal feat for captioning decoder\n",
    "        self.cos = torch.nn.CosineSimilarity(dim=1)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        \"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def add_pos_embedding(self, x):\n",
    "        if len(x.shape) == 3:  # NLD\n",
    "            b = x.shape[0]\n",
    "            c = x.shape[-1]\n",
    "            x = x.transpose(-1, 1).view(b, c, self.h_feat, self.w_feat)\n",
    "        batch, c, h, w = x.shape\n",
    "        pos_h = torch.arange(h).cuda()\n",
    "        pos_w = torch.arange(w).cuda()\n",
    "        embed_h = self.w_embedding(pos_h)\n",
    "        embed_w = self.h_embedding(pos_w)\n",
    "        pos_embedding = torch.cat(\n",
    "            [\n",
    "                embed_w.unsqueeze(0).repeat(h, 1, 1),\n",
    "                embed_h.unsqueeze(1).repeat(1, w, 1),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        pos_embedding = (\n",
    "            pos_embedding.permute(2, 0, 1).unsqueeze(0).repeat(batch, 1, 1, 1)\n",
    "        )\n",
    "        x = x + pos_embedding\n",
    "        # reshape back to NLD\n",
    "        x = x.view(b, c, -1).transpose(-1, 1)  # NLD (b,hw,c)\n",
    "        return x\n",
    "\n",
    "    def forward(self, img_A, img_B, change_seg):\n",
    "        h, w = self.h_feat, self.w_feat\n",
    "\n",
    "        # 1. Add positional embeddings to all inputs\n",
    "        img_A = self.add_pos_embedding(img_A)\n",
    "        img_B = self.add_pos_embedding(img_B)\n",
    "        change_seg = self.add_pos_embedding(change_seg)  # Add position embeddings to segmentation\n",
    "\n",
    "        # captioning setup\n",
    "        batch, c = img_A.shape[0], img_A.shape[-1]\n",
    "        img_sa1, img_sa2, seg_sa = img_A, img_B, change_seg\n",
    "\n",
    "        # Method: Mamba with segmentation integration\n",
    "        img_list = []\n",
    "        N, L, D = img_sa1.shape\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            # Difference features\n",
    "            dif = img_sa2 - img_sa1\n",
    "            \n",
    "            # SD-SSM for images with difference guidance\n",
    "            img_sa1 = self.CaMalayer_list[i][0](\n",
    "                inputs_embeds=img_sa1, inputs_embeds_2=dif\n",
    "            ).last_hidden_state\n",
    "            \n",
    "            img_sa2 = self.CaMalayer_list[i][1](\n",
    "                inputs_embeds=img_sa2, inputs_embeds_2=dif\n",
    "            ).last_hidden_state\n",
    "            \n",
    "            # Process segmentation with the guidance from both images\n",
    "            seg_guidance = dif * self.linear_seg[i](seg_sa)\n",
    "            seg_sa = self.CaMalayer_list[i][2](\n",
    "                inputs_embeds=seg_sa, inputs_embeds_2=seg_guidance\n",
    "            ).last_hidden_state\n",
    "\n",
    "            # TT-SSM: Temporal fusion with segmentation integration\n",
    "            scan_mode = \"TT-SSM\"\n",
    "            if scan_mode == \"TT-SSM\":\n",
    "                # Normalize features\n",
    "                img_sa1 = self.LN_norm(img_sa1)\n",
    "                img_sa2 = self.LN_norm(img_sa2)\n",
    "                seg_sa = self.LN_norm(seg_sa)\n",
    "                \n",
    "                # Save residuals\n",
    "                img_sa1_res = img_sa1\n",
    "                img_sa2_res = img_sa2\n",
    "                seg_sa_res = seg_sa\n",
    "                \n",
    "                # Temporal fusion (img_A and img_B)\n",
    "                img_fuse1 = img_sa1.permute(0, 2, 1).unsqueeze(-1)  # (N,D,L,1)\n",
    "                img_fuse2 = img_sa2.permute(0, 2, 1).unsqueeze(-1)  # (N,D,L,1)\n",
    "                img_fuse = torch.cat([img_fuse1, img_fuse2], dim=-1).reshape(\n",
    "                    N, D, -1\n",
    "                )  # (N,D,L*2)\n",
    "                \n",
    "                # Apply temporal fusion Mamba\n",
    "                img_fuse = self.fuselayer_list[i][0](\n",
    "                    inputs_embeds=img_fuse.permute(0, 2, 1)\n",
    "                ).last_hidden_state.permute(0, 2, 1)  # (N,D,L*2)\n",
    "                \n",
    "                # Reshape and extract fused features\n",
    "                img_fuse = img_fuse.reshape(N, D, L, -1)\n",
    "                img_sa1 = img_fuse[..., 0].permute(0, 2, 1)  # (N,L,D)\n",
    "                img_sa2 = img_fuse[..., 1].permute(0, 2, 1)  # (N,L,D)\n",
    "                \n",
    "                # Segmentation-image fusion\n",
    "                seg_img_fuse = torch.cat([\n",
    "                    seg_sa.permute(0, 2, 1).unsqueeze(-1),\n",
    "                    ((img_sa1 + img_sa2)/2).permute(0, 2, 1).unsqueeze(-1)\n",
    "                ], dim=-1).reshape(N, D, -1)\n",
    "                \n",
    "                # Apply segmentation fusion Mamba\n",
    "                seg_img_fuse = self.fuselayer_list[i][1](\n",
    "                    inputs_embeds=seg_img_fuse.permute(0, 2, 1)\n",
    "                ).last_hidden_state.permute(0, 2, 1)\n",
    "                \n",
    "                # Extract fused segmentation features\n",
    "                seg_img_fuse = seg_img_fuse.reshape(N, D, L, -1)\n",
    "                seg_sa = seg_img_fuse[..., 0].permute(0, 2, 1)  # (N,L,D)\n",
    "                \n",
    "                # Residual connections with learnable weights\n",
    "                img_sa1 = self.LN_norm(img_sa1) + img_sa1_res * self.alpha\n",
    "                img_sa2 = self.LN_norm(img_sa2) + img_sa2_res * self.alpha\n",
    "                seg_sa = self.LN_norm(seg_sa) + seg_sa_res * self.beta\n",
    "\n",
    "            # Final layer - combine all three modalities\n",
    "            if i == self.n_layers - 1:\n",
    "                # Reshape to spatial features\n",
    "                img1_cap = img_sa1.transpose(-1, 1).view(batch, c, h, w)\n",
    "                img2_cap = img_sa2.transpose(-1, 1).view(batch, c, h, w)\n",
    "                seg_cap = seg_sa.transpose(-1, 1).view(batch, c, h, w)\n",
    "                \n",
    "                # Concatenate all three modalities\n",
    "                feat_cap = torch.cat([img1_cap, img2_cap, seg_cap], dim=1)\n",
    "                \n",
    "                # Final fusion\n",
    "                feat_cap = self.LN_list[i](self.Conv1_list[i](feat_cap))\n",
    "                \n",
    "                # Reshape for output\n",
    "                img_fuse = feat_cap.view(batch, c, -1).transpose(-1, 1).unsqueeze(-1)\n",
    "                img_list.append(img_fuse)\n",
    "\n",
    "        # Output\n",
    "        feat_cap = img_list[-1][..., 0]\n",
    "        feat_cap = feat_cap.transpose(-1, 1)\n",
    "        return feat_cap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e72206",
   "metadata": {},
   "source": [
    "## Novel Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, network,device,width=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.minus_conv = nn.Sequential(ConvModule(\n",
    "                    in_channels=self.minus_channel[0],\n",
    "                    out_channels=256,\n",
    "                    kernel_size=1),\n",
    "                    ConvModule(\n",
    "                    in_channels=self.minus_channel[1],\n",
    "                    out_channels=256,\n",
    "                    kernel_size=1),\n",
    "                    ConvModule(\n",
    "                    in_channels=self.minus_channel[2],\n",
    "                    out_channels=256,\n",
    "                    kernel_size=1),\n",
    "                    ConvModule(\n",
    "                    in_channels=self.minus_channel[3],\n",
    "                    out_channels=256,\n",
    "                    kernel_size=1)\n",
    "                    )\n",
    "        \n",
    "        self.neck = FPN(in_channels=[512,1024,2048,4100],out_channels=256,num_outs=4)\n",
    "        self.network = network\n",
    "        \n",
    "        self.feature_extractor = CLIPResNetWithAttention([1,1,1,1],get_embeddings=True)\n",
    "        \n",
    "        clip_model_type = self.network.replace(\"CLIP-\", \"\")\n",
    "        self.clip_model, preprocess = clip.load(clip_model_type,device)  #\n",
    "        self.clip_model = self.clip_model.to(device=device,dtype=torch.float32)\n",
    "        self.attentive_encoder = ModifiedAttentiveEncoder(n_layers=3,\n",
    "                                        feature_size=[7, 7, 768],\n",
    "                                        heads=8, dropout=0.1)\n",
    "        self.text_decoder = DecoderTransformer(decoder_type='transformer_decoder',embed_dim=768,\n",
    "                                    vocab_size=len(word_vocab), max_lengths=42,\n",
    "                                    word_vocab=word_vocab, n_head=8,\n",
    "                                    n_layers=1, dropout=0.1,device=device)\n",
    "    def predict_by_feat(seg_logits,\n",
    "                        batch_img_metas):\n",
    "        \"\"\"Transform a batch of output seg_logits to the input shape.\n",
    "\n",
    "        Args:\n",
    "            seg_logits (Tensor): The output from decode head forward function.\n",
    "            batch_img_metas (list[dict]): Meta information of each image, e.g.,\n",
    "                image size, scaling factor, etc.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Outputs segmentation logits map.\n",
    "        \"\"\"\n",
    "\n",
    "        seg_logits = resize(\n",
    "            input=seg_logits,\n",
    "            size=batch_img_metas[0]['img_shape'],\n",
    "            mode='bilinear',\n",
    "            align_corners=False)\n",
    "        return seg_logits\n",
    "    \n",
    "    def forward(self,imageA, imageB):\n",
    "        \n",
    "        # Encoder\n",
    "        x_clipA,x_clipB = self.feature_extractor(imageA),self.feature_extractor(imageB) #CLIPResnetwithAttention\n",
    "        \n",
    "        img_A = imageA.to(dtype=torch.float32)\n",
    "        img_B = imageB.to(dtype=torch.float32)\n",
    "        clip_emb_A, img_feat_A = self.clip_model.encode_image(img_A) #CLIP Pretrained encoder\n",
    "        clip_emb_B, img_feat_B = self.clip_model.encode_image(img_B) #CLIP Pretrained encoder\n",
    "        \n",
    "        \n",
    "        \n",
    "        # FPN Functions\n",
    "        x_orig = [torch.cat([x_clipA[i], x_clipB[i]], dim=1) for i in range(len(x_clipA))]\n",
    "        x_minus = [self.minus_conv[i](torch.abs(x_clipA[i]-x_clipB[i])) for i in range(len(x_clipA))]\n",
    "        x_diff = [F.sigmoid(1-torch.cosine_similarity(x_clipA[i], x_clipB[i], dim=1)).unsqueeze(1) for i in range(len(x_clipA))]\n",
    "        \n",
    "        if self.with_neck:\n",
    "            x_orig = list(self.neck(x_orig)) # Feature Pyramid Network \n",
    "        \n",
    "        x = x_orig\n",
    "        \n",
    "        x = [torch.cat([x[i]*x_diff[i], x_minus[i], x[i]], dim=1) for i in range(len(x))]\n",
    "        \n",
    "        seg_logits = self.decode_head.forward(x,clip_emb_A,clip_emb_B) # SwinTextDecoder\n",
    "\n",
    "        data_samples = [{'img_shape': (256, 256)}]\n",
    "        output = self.predict_by_feat(seg_logits,data_samples) \n",
    "\n",
    "        # Decoding Text\n",
    "        out_put_embeddings,output_features = self.clip_model.encode_image(output)\n",
    "        \n",
    "        feat_cap = self.attentive_encoder(img_feat_A,img_feat_B,output_features)\n",
    "        \n",
    "        # Generating Text decodings\n",
    "        text_feat = \n",
    "        return output,output_features,img_feat_A,img_feat_B\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RSCaMa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
