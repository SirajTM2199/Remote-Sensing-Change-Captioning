{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f5701f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085f6d6",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dab035",
   "metadata": {},
   "source": [
    "1) Import word-vocab \n",
    "2) Get the new datasets\n",
    "3) Try training\n",
    "4) Try Testing \n",
    "5) Start paper "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea231d0b",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3678567b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/anaconda3/envs/Siraj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/guest/anaconda3/envs/Siraj/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from clip import clip\n",
    "import sys\n",
    "sys.path.append('/home/guest/Documents/Siraj TM/RSCaMa')\n",
    "from model.model_decoder import DecoderTransformer\n",
    "import torchvision.transforms.functional as TF\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.layers import drop_path, trunc_normal_\n",
    "from PIL import Image\n",
    "from data.LEVIR_CC.LEVIRCC_Modified import LEVIRCCDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4c77920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896df65",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7db2cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path) as f:\n",
    "        file = json.load(f)\n",
    "    f.close()\n",
    "    return file\n",
    "def save_json(file,path):\n",
    "    with open(path,'w') as f:\n",
    "        json.dump(file,f)\n",
    "    f.close()\n",
    "    print(\"Saved Successfully\")\n",
    "def rem_print(word):\n",
    "    t_word = word\n",
    "    for _ in range(100 - len(t_word)):\n",
    "        word = word + ' '\n",
    "    print(word,end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48491b9b",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b82d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = load_json('assets/vocab.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20550db8",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dd8ad1",
   "metadata": {},
   "source": [
    "## Architecture imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fbc4d",
   "metadata": {},
   "source": [
    "### Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117735c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.model import constant_init, kaiming_init\n",
    "from mmengine.registry import MODELS\n",
    "from mmengine.utils.dl_utils.parrots_wrapper import _BatchNorm, _InstanceNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a00576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = None\n",
    "        self.stride = stride\n",
    "\n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
    "            self.downsample = nn.Sequential(OrderedDict([\n",
    "                (\"-1\", nn.AvgPool2d(stride)),\n",
    "                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
    "                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.avgpool(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a834f00",
   "metadata": {},
   "source": [
    "### AttentionPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210e3f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPool2d(nn.Module):\n",
    "    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.spacial_dim = spacial_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n",
    "        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n",
    "\n",
    "        cls_pos = self.positional_embedding[0:1, :]\n",
    "        spatial_pos = F.interpolate(self.positional_embedding[1:,].reshape(1, self.spacial_dim, self.spacial_dim, self.embed_dim).permute(0, 3, 1, 2), size=(H, W), mode='bilinear')\n",
    "        spatial_pos = spatial_pos.reshape(self.embed_dim, H*W).permute(1, 0)\n",
    "        positional_embedding = torch.cat([cls_pos, spatial_pos], dim=0)\n",
    "\n",
    "        x = x + positional_embedding[:, None, :]\n",
    "        x, _ = F.multi_head_attention_forward(\n",
    "            query=x, key=x, value=x,\n",
    "            embed_dim_to_check=x.shape[-1],\n",
    "            num_heads=self.num_heads,\n",
    "            q_proj_weight=self.q_proj.weight,\n",
    "            k_proj_weight=self.k_proj.weight,\n",
    "            v_proj_weight=self.v_proj.weight,\n",
    "            in_proj_weight=None,\n",
    "            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n",
    "            bias_k=None,\n",
    "            bias_v=None,\n",
    "            add_zero_attn=False,\n",
    "            dropout_p=0,\n",
    "            out_proj_weight=self.c_proj.weight,\n",
    "            out_proj_bias=self.c_proj.bias,\n",
    "            use_separate_proj_weight=True,\n",
    "            training=self.training,\n",
    "            need_weights=False\n",
    "        )\n",
    "\n",
    "        x = x.permute(1, 2, 0)\n",
    "        global_feat = x[:, :, 0]\n",
    "        feature_map = x[:, :, 1:].reshape(B, -1, H, W)\n",
    "        return global_feat, feature_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a01507",
   "metadata": {},
   "source": [
    "### CLIPResNetWithAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc1ad09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPResNetWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
    "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "    - The final pooling layer is a QKV attention instead of an average pool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, output_dim=1024, input_resolution=224, width=64, pretrained=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.pretrained = pretrained\n",
    "        self.output_dim = output_dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        # the 3-layer stem\n",
    "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # residual layers\n",
    "        self._inplanes = width  # this is a *mutable* variable used during construction\n",
    "        self.layer1 = self._make_layer(width, layers[0])\n",
    "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
    "\n",
    "        embed_dim = width * 32  # the ResNet feature dimension\n",
    "        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, 32, output_dim)\n",
    "        # self.init_weights()\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        pretrained = pretrained or self.pretrained\n",
    "        if isinstance(pretrained, str):\n",
    "            checkpoint = torch.jit.load(pretrained, map_location=device).float().state_dict()\n",
    "\n",
    "            state_dict = {}\n",
    "\n",
    "            for k in checkpoint.keys():\n",
    "                if k.startswith('visual.'):\n",
    "                    new_k = k.replace('visual.', '')\n",
    "                    state_dict[new_k] = checkpoint[k]\n",
    "\n",
    "                    if 'positional_embedding' in new_k:\n",
    "                        if self.attnpool.positional_embedding.shape != state_dict[new_k].shape:\n",
    "                            print(f'Resize the pos_embed shape from {state_dict[new_k].shape} to {self.attnpool.positional_embedding.shape}')\n",
    "                            cls_pos = state_dict[new_k][0:1, :]\n",
    "                            H = W = self.input_resolution // 32\n",
    "                            old_h = int(math.sqrt(state_dict[new_k][1:,].shape[0]))\n",
    "                            spatial_pos = F.interpolate(state_dict[new_k][1:,].reshape(1, old_h, old_h, cls_pos.shape[1]).permute(0, 3, 1, 2), size=(H, W), mode='bilinear')\n",
    "                            spatial_pos = spatial_pos.reshape(cls_pos.shape[1], H*W).permute(1, 0)\n",
    "                            positional_embedding = torch.cat([cls_pos, spatial_pos], dim=0)\n",
    "                            state_dict[new_k] = positional_embedding\n",
    "                            assert self.attnpool.positional_embedding.shape == state_dict[new_k].shape\n",
    "\n",
    "            u, w = self.load_state_dict(state_dict, False)\n",
    "            print(u, w, 'are misaligned params in CLIPResNet')\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "\n",
    "        self._inplanes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self._inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        def stem(x):\n",
    "            for conv, bn in [(self.conv1, self.bn1), (self.conv2, self.bn2), (self.conv3, self.bn3)]:\n",
    "                x = self.relu(bn(conv(x)))\n",
    "            x = self.avgpool(x)\n",
    "            return x\n",
    "\n",
    "        x = x.type(self.conv1.weight.dtype)\n",
    "        x = stem(x)\n",
    "\n",
    "        outs = []\n",
    "        x = self.layer1(x)\n",
    "        outs.append(x)\n",
    "        x = self.layer2(x)\n",
    "        outs.append(x)\n",
    "        x = self.layer3(x)\n",
    "        outs.append(x)\n",
    "        x = self.layer4(x)\n",
    "        outs.append(x)\n",
    "\n",
    "        x_global, x_local = self.attnpool(x)\n",
    "        outs.append([x_global, x_local])\n",
    "\n",
    "        \n",
    "        return tuple(outs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637d54f2",
   "metadata": {},
   "source": [
    "### ConvModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a90c5",
   "metadata": {},
   "source": [
    "#### Hsigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5bcfd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from mmengine.registry import MODELS\n",
    "\n",
    "\n",
    "@MODELS.register_module()\n",
    "class HSigmoid(nn.Module):\n",
    "    \"\"\"Hard Sigmoid Module. Apply the hard sigmoid function:\n",
    "    Hsigmoid(x) = min(max((x + bias) / divisor, min_value), max_value)\n",
    "    Default: Hsigmoid(x) = min(max((x + 3) / 6, 0), 1)\n",
    "\n",
    "    Note:\n",
    "        In MMCV v1.4.4, we modified the default value of args to align with\n",
    "        PyTorch official.\n",
    "\n",
    "    Args:\n",
    "        bias (float): Bias of the input feature map. Default: 3.0.\n",
    "        divisor (float): Divisor of the input feature map. Default: 6.0.\n",
    "        min_value (float): Lower bound value. Default: 0.0.\n",
    "        max_value (float): Upper bound value. Default: 1.0.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 bias: float = 3.0,\n",
    "                 divisor: float = 6.0,\n",
    "                 min_value: float = 0.0,\n",
    "                 max_value: float = 1.0):\n",
    "        super().__init__()\n",
    "        warnings.warn(\n",
    "            'In MMCV v1.4.4, we modified the default value of args to align '\n",
    "            'with PyTorch official. Previous Implementation: '\n",
    "            'Hsigmoid(x) = min(max((x + 1) / 2, 0), 1). '\n",
    "            'Current Implementation: '\n",
    "            'Hsigmoid(x) = min(max((x + 3) / 6, 0), 1).')\n",
    "        self.bias = bias\n",
    "        self.divisor = divisor\n",
    "        assert self.divisor != 0\n",
    "        self.min_value = min_value\n",
    "        self.max_value = max_value\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = (x + self.bias) / self.divisor\n",
    "\n",
    "        return x.clamp_(self.min_value, self.max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed4011f",
   "metadata": {},
   "source": [
    "#### build_activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b28b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from mmengine.registry import MODELS\n",
    "from mmengine.utils import digit_version\n",
    "from mmengine.utils.dl_utils import TORCH_VERSION\n",
    "\n",
    "for module in [\n",
    "        nn.ReLU, nn.LeakyReLU, nn.PReLU, nn.RReLU, nn.ReLU6, nn.ELU,\n",
    "        nn.Sigmoid, nn.Tanh\n",
    "]:\n",
    "    MODELS.register_module(module=module)\n",
    "\n",
    "if digit_version(torch.__version__) >= digit_version('1.7.0'):\n",
    "    #MODELS.register_module(module=nn.SiLU, name='SiLU')\n",
    "    pass\n",
    "else:\n",
    "\n",
    "    class SiLU(nn.Module):\n",
    "        \"\"\"Sigmoid Weighted Liner Unit.\"\"\"\n",
    "\n",
    "        def __init__(self, inplace=False):\n",
    "            super().__init__()\n",
    "            self.inplace = inplace\n",
    "\n",
    "        def forward(self, inputs) -> torch.Tensor:\n",
    "            if self.inplace:\n",
    "                return inputs.mul_(torch.sigmoid(inputs))\n",
    "            else:\n",
    "                return inputs * torch.sigmoid(inputs)\n",
    "\n",
    "    MODELS.register_module(module=SiLU, name='SiLU')\n",
    "\n",
    "\n",
    "'''@MODELS.register_module(name='Clip')\n",
    "@MODELS.register_module()'''\n",
    "class Clamp(nn.Module):\n",
    "    \"\"\"Clamp activation layer.\n",
    "\n",
    "    This activation function is to clamp the feature map value within\n",
    "    :math:`[min, max]`. More details can be found in ``torch.clamp()``.\n",
    "\n",
    "    Args:\n",
    "        min (Number | optional): Lower-bound of the range to be clamped to.\n",
    "            Default to -1.\n",
    "        max (Number | optional): Upper-bound of the range to be clamped to.\n",
    "            Default to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min: float = -1., max: float = 1.):\n",
    "        super().__init__()\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"Forward function.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Clamped tensor.\n",
    "        \"\"\"\n",
    "        return torch.clamp(x, min=self.min, max=self.max)\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    r\"\"\"Applies the Gaussian Error Linear Units function:\n",
    "\n",
    "    .. math::\n",
    "        \\text{GELU}(x) = x * \\Phi(x)\n",
    "    where :math:`\\Phi(x)` is the Cumulative Distribution Function for\n",
    "    Gaussian Distribution.\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *)` where `*` means, any number of additional\n",
    "          dimensions\n",
    "        - Output: :math:`(N, *)`, same shape as the input\n",
    "\n",
    "    .. image:: scripts/activation_images/GELU.png\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.GELU()\n",
    "        >>> input = torch.randn(2)\n",
    "        >>> output = m(input)\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.gelu(input)\n",
    "\n",
    "\n",
    "if (TORCH_VERSION == 'parrots'\n",
    "        or digit_version(TORCH_VERSION) < digit_version('1.4')):\n",
    "    #MODELS.register_module(module=GELU)\n",
    "    pass\n",
    "else:\n",
    "    #MODELS.register_module(module=nn.GELU)\n",
    "    pass\n",
    "\n",
    "def build_activation_layer(cfg: Dict) -> nn.Module:\n",
    "    \"\"\"Build activation layer.\n",
    "\n",
    "    Args:\n",
    "        cfg (dict): The activation layer config, which should contain:\n",
    "\n",
    "            - type (str): Layer type.\n",
    "            - layer args: Args needed to instantiate an activation layer.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Created activation layer.\n",
    "    \"\"\"\n",
    "    return MODELS.build(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c246ff",
   "metadata": {},
   "source": [
    "#### build_conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09476a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import inspect\n",
    "from typing import Dict, Optional\n",
    "\n",
    "#from mmengine.registry import MODELS\n",
    "from torch import nn\n",
    "\n",
    "MODELS.register_module('Conv1d', module=nn.Conv1d)\n",
    "MODELS.register_module('Conv2d', module=nn.Conv2d)\n",
    "MODELS.register_module('Conv3d', module=nn.Conv3d)\n",
    "MODELS.register_module('Conv', module=nn.Conv2d)\n",
    "\n",
    "\n",
    "def build_conv_layer(cfg: Optional[Dict], *args, **kwargs) -> nn.Module:\n",
    "    \"\"\"Build convolution layer.\n",
    "\n",
    "    Args:\n",
    "        cfg (None or dict): The conv layer config, which should contain:\n",
    "            - type (str): Layer type.\n",
    "            - layer args: Args needed to instantiate an conv layer.\n",
    "        args (argument list): Arguments passed to the `__init__`\n",
    "            method of the corresponding conv layer.\n",
    "        kwargs (keyword arguments): Keyword arguments passed to the `__init__`\n",
    "            method of the corresponding conv layer.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Created conv layer.\n",
    "    \"\"\"\n",
    "    if cfg is None:\n",
    "        cfg_ = dict(type='Conv2d')\n",
    "    else:\n",
    "        if not isinstance(cfg, dict):\n",
    "            raise TypeError('cfg must be a dict')\n",
    "        if 'type' not in cfg:\n",
    "            raise KeyError('the cfg dict must contain the key \"type\"')\n",
    "        cfg_ = cfg.copy()\n",
    "\n",
    "    layer_type = cfg_.pop('type')\n",
    "    if inspect.isclass(layer_type):\n",
    "        return layer_type(*args, **kwargs, **cfg_)  # type: ignore\n",
    "    # Switch registry to the target scope. If `conv_layer` cannot be found\n",
    "    # in the registry, fallback to search `conv_layer` in the\n",
    "    # mmengine.MODELS.\n",
    "    with MODELS.switch_scope_and_registry(None) as registry:\n",
    "        conv_layer = registry.get(layer_type)\n",
    "    if conv_layer is None:\n",
    "        raise KeyError(f'Cannot find {conv_layer} in registry under scope '\n",
    "                       f'name {registry.scope}')\n",
    "    layer = conv_layer(*args, **kwargs, **cfg_)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f3cc6",
   "metadata": {},
   "source": [
    "#### build_norm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24431e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import inspect\n",
    "from typing import Dict, Tuple, Union\n",
    "\n",
    "import torch.nn as nn\n",
    "#from mmengine.registry import MODELS\n",
    "from mmengine.utils import is_tuple_of\n",
    "from mmengine.utils.dl_utils.parrots_wrapper import (SyncBatchNorm, _BatchNorm,\n",
    "                                                     _InstanceNorm)\n",
    "\n",
    "#MODELS.register_module('BN', module=nn.BatchNorm2d)\n",
    "#MODELS.register_module('BN1d', module=nn.BatchNorm1d)\n",
    "#MODELS.register_module('BN2d', module=nn.BatchNorm2d)\n",
    "#MODELS.register_module('BN3d', module=nn.BatchNorm3d)\n",
    "#MODELS.register_module('SyncBN', module=SyncBatchNorm)\n",
    "#MODELS.register_module('GN', module=nn.GroupNorm)\n",
    "#MODELS.register_module('LN', module=nn.LayerNorm)\n",
    "#MODELS.register_module('IN', module=nn.InstanceNorm2d)\n",
    "#MODELS.register_module('IN1d', module=nn.InstanceNorm1d)\n",
    "#MODELS.register_module('IN2d', module=nn.InstanceNorm2d)\n",
    "#MODELS.register_module('IN3d', module=nn.InstanceNorm3d)\n",
    "\n",
    "\n",
    "def infer_abbr(class_type):\n",
    "    \"\"\"Infer abbreviation from the class name.\n",
    "\n",
    "    When we build a norm layer with `build_norm_layer()`, we want to preserve\n",
    "    the norm type in variable names, e.g, self.bn1, self.gn. This method will\n",
    "    infer the abbreviation to map class types to abbreviations.\n",
    "\n",
    "    Rule 1: If the class has the property \"_abbr_\", return the property.\n",
    "    Rule 2: If the parent class is _BatchNorm, GroupNorm, LayerNorm or\n",
    "    InstanceNorm, the abbreviation of this layer will be \"bn\", \"gn\", \"ln\" and\n",
    "    \"in\" respectively.\n",
    "    Rule 3: If the class name contains \"batch\", \"group\", \"layer\" or \"instance\",\n",
    "    the abbreviation of this layer will be \"bn\", \"gn\", \"ln\" and \"in\"\n",
    "    respectively.\n",
    "    Rule 4: Otherwise, the abbreviation falls back to \"norm\".\n",
    "\n",
    "    Args:\n",
    "        class_type (type): The norm layer type.\n",
    "\n",
    "    Returns:\n",
    "        str: The inferred abbreviation.\n",
    "    \"\"\"\n",
    "    if not inspect.isclass(class_type):\n",
    "        raise TypeError(\n",
    "            f'class_type must be a type, but got {type(class_type)}')\n",
    "    if hasattr(class_type, '_abbr_'):\n",
    "        return class_type._abbr_\n",
    "    if issubclass(class_type, _InstanceNorm):  # IN is a subclass of BN\n",
    "        return 'in'\n",
    "    elif issubclass(class_type, _BatchNorm):\n",
    "        return 'bn'\n",
    "    elif issubclass(class_type, nn.GroupNorm):\n",
    "        return 'gn'\n",
    "    elif issubclass(class_type, nn.LayerNorm):\n",
    "        return 'ln'\n",
    "    else:\n",
    "        class_name = class_type.__name__.lower()\n",
    "        if 'batch' in class_name:\n",
    "            return 'bn'\n",
    "        elif 'group' in class_name:\n",
    "            return 'gn'\n",
    "        elif 'layer' in class_name:\n",
    "            return 'ln'\n",
    "        elif 'instance' in class_name:\n",
    "            return 'in'\n",
    "        else:\n",
    "            return 'norm_layer'\n",
    "\n",
    "\n",
    "def build_norm_layer(cfg: Dict,\n",
    "                     num_features: int,\n",
    "                     postfix: Union[int, str] = '') -> Tuple[str, nn.Module]:\n",
    "    \"\"\"Build normalization layer.\n",
    "\n",
    "    Args:\n",
    "        cfg (dict): The norm layer config, which should contain:\n",
    "\n",
    "            - type (str): Layer type.\n",
    "            - layer args: Args needed to instantiate a norm layer.\n",
    "            - requires_grad (bool, optional): Whether stop gradient updates.\n",
    "        num_features (int): Number of input channels.\n",
    "        postfix (int | str): The postfix to be appended into norm abbreviation\n",
    "            to create named layer.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, nn.Module]: The first element is the layer name consisting\n",
    "        of abbreviation and postfix, e.g., bn1, gn. The second element is the\n",
    "        created norm layer.\n",
    "    \"\"\"\n",
    "    if not isinstance(cfg, dict):\n",
    "        raise TypeError('cfg must be a dict')\n",
    "    if 'type' not in cfg:\n",
    "        raise KeyError('the cfg dict must contain the key \"type\"')\n",
    "    cfg_ = cfg.copy()\n",
    "\n",
    "    layer_type = cfg_.pop('type')\n",
    "\n",
    "    if inspect.isclass(layer_type):\n",
    "        norm_layer = layer_type\n",
    "    else:\n",
    "        # Switch registry to the target scope. If `norm_layer` cannot be found\n",
    "        # in the registry, fallback to search `norm_layer` in the\n",
    "        # mmengine.MODELS.\n",
    "        with MODELS.switch_scope_and_registry(None) as registry:\n",
    "            norm_layer = registry.get(layer_type)\n",
    "        if norm_layer is None:\n",
    "            raise KeyError(f'Cannot find {norm_layer} in registry under '\n",
    "                           f'scope name {registry.scope}')\n",
    "    abbr = infer_abbr(norm_layer)\n",
    "\n",
    "    assert isinstance(postfix, (int, str))\n",
    "    name = abbr + str(postfix)\n",
    "\n",
    "    requires_grad = cfg_.pop('requires_grad', True)\n",
    "    cfg_.setdefault('eps', 1e-5)\n",
    "    if norm_layer is not nn.GroupNorm:\n",
    "        layer = norm_layer(num_features, **cfg_)\n",
    "        if layer_type == 'SyncBN' and hasattr(layer, '_specify_ddp_gpu_num'):\n",
    "            layer._specify_ddp_gpu_num(1)\n",
    "    else:\n",
    "        assert 'num_groups' in cfg_\n",
    "        layer = norm_layer(num_channels=num_features, **cfg_)\n",
    "\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = requires_grad\n",
    "\n",
    "    return name, layer\n",
    "\n",
    "\n",
    "def is_norm(layer: nn.Module,\n",
    "            exclude: Union[type, tuple, None] = None) -> bool:\n",
    "    \"\"\"Check if a layer is a normalization layer.\n",
    "\n",
    "    Args:\n",
    "        layer (nn.Module): The layer to be checked.\n",
    "        exclude (type | tuple[type]): Types to be excluded.\n",
    "\n",
    "    Returns:\n",
    "        bool: Whether the layer is a norm layer.\n",
    "    \"\"\"\n",
    "    if exclude is not None:\n",
    "        if not isinstance(exclude, tuple):\n",
    "            exclude = (exclude, )\n",
    "        if not is_tuple_of(exclude, type):\n",
    "            raise TypeError(\n",
    "                f'\"exclude\" must be either None or type or a tuple of types, '\n",
    "                f'but got {type(exclude)}: {exclude}')\n",
    "\n",
    "    if exclude and isinstance(layer, exclude):\n",
    "        return False\n",
    "\n",
    "    all_norm_bases = (_BatchNorm, _InstanceNorm, nn.GroupNorm, nn.LayerNorm)\n",
    "    return isinstance(layer, all_norm_bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d097ed2",
   "metadata": {},
   "source": [
    "#### build_padding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13f2a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import inspect\n",
    "from typing import Dict\n",
    "\n",
    "import torch.nn as nn\n",
    "from mmengine.registry import MODELS\n",
    "\n",
    "#MODELS.register_module('zero', module=nn.ZeroPad2d)\n",
    "#MODELS.register_module('reflect', module=nn.ReflectionPad2d)\n",
    "#MODELS.register_module('replicate', module=nn.ReplicationPad2d)\n",
    "\n",
    "\n",
    "def build_padding_layer(cfg: Dict, *args, **kwargs) -> nn.Module:\n",
    "    \"\"\"Build padding layer.\n",
    "\n",
    "    Args:\n",
    "        cfg (dict): The padding layer config, which should contain:\n",
    "            - type (str): Layer type.\n",
    "            - layer args: Args needed to instantiate a padding layer.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Created padding layer.\n",
    "    \"\"\"\n",
    "    if not isinstance(cfg, dict):\n",
    "        raise TypeError('cfg must be a dict')\n",
    "    if 'type' not in cfg:\n",
    "        raise KeyError('the cfg dict must contain the key \"type\"')\n",
    "\n",
    "    cfg_ = cfg.copy()\n",
    "    padding_type = cfg_.pop('type')\n",
    "    if inspect.isclass(padding_type):\n",
    "        return padding_type(*args, **kwargs, **cfg_)\n",
    "    # Switch registry to the target scope. If `padding_layer` cannot be found\n",
    "    # in the registry, fallback to search `padding_layer` in the\n",
    "    # mmengine.MODELS.\n",
    "    with MODELS.switch_scope_and_registry(None) as registry:\n",
    "        padding_layer = registry.get(padding_type)\n",
    "    if padding_layer is None:\n",
    "        raise KeyError(f'Cannot find {padding_layer} in registry under scope '\n",
    "                       f'name {registry.scope}')\n",
    "    layer = padding_layer(*args, **kwargs, **cfg_)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb9ed1",
   "metadata": {},
   "source": [
    "#### ConvModule code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04b134cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import warnings\n",
    "from functools import partial\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mmengine.model import constant_init, kaiming_init\n",
    "#from mmengine.registry import MODELS\n",
    "from mmengine.utils.dl_utils.parrots_wrapper import _BatchNorm, _InstanceNorm\n",
    "\n",
    "'''from .activation import build_activation_layer\n",
    "from .conv import build_conv_layer\n",
    "from .norm import build_norm_layer\n",
    "from .padding import build_padding_layer'''\n",
    "\n",
    "\n",
    "def efficient_conv_bn_eval_forward(bn: _BatchNorm,\n",
    "                                   conv: nn.modules.conv._ConvNd,\n",
    "                                   x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Implementation based on https://arxiv.org/abs/2305.11624\n",
    "    \"Tune-Mode ConvBN Blocks For Efficient Transfer Learning\"\n",
    "    It leverages the associative law between convolution and affine transform,\n",
    "    i.e., normalize (weight conv feature) = (normalize weight) conv feature.\n",
    "    It works for Eval mode of ConvBN blocks during validation, and can be used\n",
    "    for training as well. It reduces memory and computation cost.\n",
    "\n",
    "    Args:\n",
    "        bn (_BatchNorm): a BatchNorm module.\n",
    "        conv (nn._ConvNd): a conv module\n",
    "        x (torch.Tensor): Input feature map.\n",
    "    \"\"\"\n",
    "    # These lines of code are designed to deal with various cases\n",
    "    # like bn without affine transform, and conv without bias\n",
    "    weight_on_the_fly = conv.weight\n",
    "    if conv.bias is not None:\n",
    "        bias_on_the_fly = conv.bias\n",
    "    else:\n",
    "        bias_on_the_fly = torch.zeros_like(bn.running_var)\n",
    "\n",
    "    if bn.weight is not None:\n",
    "        bn_weight = bn.weight\n",
    "    else:\n",
    "        bn_weight = torch.ones_like(bn.running_var)\n",
    "\n",
    "    if bn.bias is not None:\n",
    "        bn_bias = bn.bias\n",
    "    else:\n",
    "        bn_bias = torch.zeros_like(bn.running_var)\n",
    "\n",
    "    # shape of [C_out, 1, 1, 1] in Conv2d\n",
    "    weight_coeff = torch.rsqrt(bn.running_var +\n",
    "                               bn.eps).reshape([-1] + [1] *\n",
    "                                               (len(conv.weight.shape) - 1))\n",
    "    # shape of [C_out, 1, 1, 1] in Conv2d\n",
    "    coefff_on_the_fly = bn_weight.view_as(weight_coeff) * weight_coeff\n",
    "\n",
    "    # shape of [C_out, C_in, k, k] in Conv2d\n",
    "    weight_on_the_fly = weight_on_the_fly * coefff_on_the_fly\n",
    "    # shape of [C_out] in Conv2d\n",
    "    bias_on_the_fly = bn_bias + coefff_on_the_fly.flatten() *\\\n",
    "        (bias_on_the_fly - bn.running_mean)\n",
    "\n",
    "    return conv._conv_forward(x, weight_on_the_fly, bias_on_the_fly)\n",
    "\n",
    "\n",
    "#@MODELS.register_module()\n",
    "class ConvModule(nn.Module):\n",
    "    \"\"\"A conv block that bundles conv/norm/activation layers.\n",
    "\n",
    "    This block simplifies the usage of convolution layers, which are commonly\n",
    "    used with a norm layer (e.g., BatchNorm) and activation layer (e.g., ReLU).\n",
    "    It is based upon three build methods: `build_conv_layer()`,\n",
    "    `build_norm_layer()` and `build_activation_layer()`.\n",
    "\n",
    "    Besides, we add some additional features in this module.\n",
    "    1. Automatically set `bias` of the conv layer.\n",
    "    2. Spectral norm is supported.\n",
    "    3. More padding modes are supported. Before PyTorch 1.5, nn.Conv2d only\n",
    "    supports zero and circular padding, and we add \"reflect\" padding mode.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input feature map.\n",
    "            Same as that in ``nn._ConvNd``.\n",
    "        out_channels (int): Number of channels produced by the convolution.\n",
    "            Same as that in ``nn._ConvNd``.\n",
    "        kernel_size (int | tuple[int]): Size of the convolving kernel.\n",
    "            Same as that in ``nn._ConvNd``.\n",
    "        stride (int | tuple[int]): Stride of the convolution.\n",
    "            Same as that in ``nn._ConvNd``.\n",
    "        padding (int | tuple[int]): Zero-padding added to both sides of\n",
    "            the input. Same as that in ``nn._ConvNd``.\n",
    "        dilation (int | tuple[int]): Spacing between kernel elements.\n",
    "            Same as that in ``nn._ConvNd``.\n",
    "        groups (int): Number of blocked connections from input channels to\n",
    "            output channels. Same as that in ``nn._ConvNd``.\n",
    "        bias (bool | str): If specified as `auto`, it will be decided by the\n",
    "            norm_cfg. Bias will be set as True if `norm_cfg` is None, otherwise\n",
    "            False. Default: \"auto\".\n",
    "        conv_cfg (dict): Config dict for convolution layer. Default: None,\n",
    "            which means using conv2d.\n",
    "        norm_cfg (dict): Config dict for normalization layer. Default: None.\n",
    "        act_cfg (dict): Config dict for activation layer.\n",
    "            Default: dict(type='ReLU').\n",
    "        inplace (bool): Whether to use inplace mode for activation.\n",
    "            Default: True.\n",
    "        with_spectral_norm (bool): Whether use spectral norm in conv module.\n",
    "            Default: False.\n",
    "        padding_mode (str): If the `padding_mode` has not been supported by\n",
    "            current `Conv2d` in PyTorch, we will use our own padding layer\n",
    "            instead. Currently, we support ['zeros', 'circular'] with official\n",
    "            implementation and ['reflect'] with our own implementation.\n",
    "            Default: 'zeros'.\n",
    "        order (tuple[str]): The order of conv/norm/activation layers. It is a\n",
    "            sequence of \"conv\", \"norm\" and \"act\". Common examples are\n",
    "            (\"conv\", \"norm\", \"act\") and (\"act\", \"conv\", \"norm\").\n",
    "            Default: ('conv', 'norm', 'act').\n",
    "        efficient_conv_bn_eval (bool): Whether use efficient conv when the\n",
    "            consecutive bn is in eval mode (either training or testing), as\n",
    "            proposed in https://arxiv.org/abs/2305.11624 . Default: `False`.\n",
    "    \"\"\"\n",
    "\n",
    "    _abbr_ = 'conv_block'\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: Union[int, Tuple[int, int]],\n",
    "                 stride: Union[int, Tuple[int, int]] = 1,\n",
    "                 padding: Union[int, Tuple[int, int]] = 0,\n",
    "                 dilation: Union[int, Tuple[int, int]] = 1,\n",
    "                 groups: int = 1,\n",
    "                 bias: Union[bool, str] = 'auto',\n",
    "                 conv_cfg: Optional[Dict] = None,\n",
    "                 norm_cfg: Optional[Dict] = None,\n",
    "                 act_cfg: Optional[Dict] = dict(type='ReLU'),\n",
    "                 inplace: bool = True,\n",
    "                 with_spectral_norm: bool = False,\n",
    "                 padding_mode: str = 'zeros',\n",
    "                 order: tuple = ('conv', 'norm', 'act'),\n",
    "                 efficient_conv_bn_eval: bool = False):\n",
    "        super().__init__()\n",
    "        assert conv_cfg is None or isinstance(conv_cfg, dict)\n",
    "        assert norm_cfg is None or isinstance(norm_cfg, dict)\n",
    "        assert act_cfg is None or isinstance(act_cfg, dict)\n",
    "        official_padding_mode = ['zeros', 'circular']\n",
    "        self.conv_cfg = conv_cfg\n",
    "        self.norm_cfg = norm_cfg\n",
    "        self.act_cfg = act_cfg\n",
    "        self.inplace = inplace\n",
    "        self.with_spectral_norm = with_spectral_norm\n",
    "        self.with_explicit_padding = padding_mode not in official_padding_mode\n",
    "        self.order = order\n",
    "        assert isinstance(self.order, tuple) and len(self.order) == 3\n",
    "        assert set(order) == {'conv', 'norm', 'act'}\n",
    "\n",
    "        self.with_norm = norm_cfg is not None\n",
    "        self.with_activation = act_cfg is not None\n",
    "        # if the conv layer is before a norm layer, bias is unnecessary.\n",
    "        if bias == 'auto':\n",
    "            bias = not self.with_norm\n",
    "        self.with_bias = bias\n",
    "\n",
    "        if self.with_explicit_padding:\n",
    "            pad_cfg = dict(type=padding_mode)\n",
    "            self.padding_layer = build_padding_layer(pad_cfg, padding)\n",
    "\n",
    "        # reset padding to 0 for conv module\n",
    "        conv_padding = 0 if self.with_explicit_padding else padding\n",
    "        # build convolution layer\n",
    "        self.conv = build_conv_layer(\n",
    "            conv_cfg,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=conv_padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=bias)\n",
    "        # export the attributes of self.conv to a higher level for convenience\n",
    "        self.in_channels = self.conv.in_channels\n",
    "        self.out_channels = self.conv.out_channels\n",
    "        self.kernel_size = self.conv.kernel_size\n",
    "        self.stride = self.conv.stride\n",
    "        self.padding = padding\n",
    "        self.dilation = self.conv.dilation\n",
    "        self.transposed = self.conv.transposed\n",
    "        self.output_padding = self.conv.output_padding\n",
    "        self.groups = self.conv.groups\n",
    "\n",
    "        if self.with_spectral_norm:\n",
    "            self.conv = nn.utils.spectral_norm(self.conv)\n",
    "\n",
    "        # build normalization layers\n",
    "        if self.with_norm:\n",
    "            # norm layer is after conv layer\n",
    "            if order.index('norm') > order.index('conv'):\n",
    "                norm_channels = out_channels\n",
    "            else:\n",
    "                norm_channels = in_channels\n",
    "            self.norm_name, norm = build_norm_layer(\n",
    "                norm_cfg, norm_channels)  # type: ignore\n",
    "            self.add_module(self.norm_name, norm)\n",
    "            if self.with_bias:\n",
    "                if isinstance(norm, (_BatchNorm, _InstanceNorm)):\n",
    "                    warnings.warn(\n",
    "                        'Unnecessary conv bias before batch/instance norm')\n",
    "        else:\n",
    "            self.norm_name = None  # type: ignore\n",
    "\n",
    "        self.turn_on_efficient_conv_bn_eval(efficient_conv_bn_eval)\n",
    "\n",
    "        # build activation layer\n",
    "        if self.with_activation:\n",
    "            act_cfg_ = act_cfg.copy()  # type: ignore\n",
    "            # nn.Tanh has no 'inplace' argument\n",
    "            if act_cfg_['type'] not in [\n",
    "                    'Tanh', 'PReLU', 'Sigmoid', 'HSigmoid', 'Swish', 'GELU'\n",
    "            ]:\n",
    "                act_cfg_.setdefault('inplace', inplace)\n",
    "            self.activate = build_activation_layer(act_cfg_)\n",
    "\n",
    "        # Use msra init by default\n",
    "        self.init_weights()\n",
    "\n",
    "    @property\n",
    "    def norm(self):\n",
    "        if self.norm_name:\n",
    "            return getattr(self, self.norm_name)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def init_weights(self):\n",
    "        # 1. It is mainly for customized conv layers with their own\n",
    "        #    initialization manners by calling their own ``init_weights()``,\n",
    "        #    and we do not want ConvModule to override the initialization.\n",
    "        # 2. For customized conv layers without their own initialization\n",
    "        #    manners (that is, they don't have their own ``init_weights()``)\n",
    "        #    and PyTorch's conv layers, they will be initialized by\n",
    "        #    this method with default ``kaiming_init``.\n",
    "        # Note: For PyTorch's conv layers, they will be overwritten by our\n",
    "        #    initialization implementation using default ``kaiming_init``.\n",
    "        if not hasattr(self.conv, 'init_weights'):\n",
    "            if self.with_activation and self.act_cfg['type'] == 'LeakyReLU':\n",
    "                nonlinearity = 'leaky_relu'\n",
    "                a = self.act_cfg.get('negative_slope', 0.01)\n",
    "            else:\n",
    "                nonlinearity = 'relu'\n",
    "                a = 0\n",
    "            kaiming_init(self.conv, a=a, nonlinearity=nonlinearity)\n",
    "        if self.with_norm:\n",
    "            constant_init(self.norm, 1, bias=0)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                activate: bool = True,\n",
    "                norm: bool = True) -> torch.Tensor:\n",
    "        layer_index = 0\n",
    "        while layer_index < len(self.order):\n",
    "            layer = self.order[layer_index]\n",
    "            if layer == 'conv':\n",
    "                if self.with_explicit_padding:\n",
    "                    x = self.padding_layer(x)\n",
    "                # if the next operation is norm and we have a norm layer in\n",
    "                # eval mode and we have enabled `efficient_conv_bn_eval` for\n",
    "                # the conv operator, then activate the optimized forward and\n",
    "                # skip the next norm operator since it has been fused\n",
    "                if layer_index + 1 < len(self.order) and \\\n",
    "                        self.order[layer_index + 1] == 'norm' and norm and \\\n",
    "                        self.with_norm and not self.norm.training and \\\n",
    "                        self.efficient_conv_bn_eval_forward is not None:\n",
    "                    self.conv.forward = partial(\n",
    "                        self.efficient_conv_bn_eval_forward, self.norm,\n",
    "                        self.conv)\n",
    "                    layer_index += 1\n",
    "                    x = self.conv(x)\n",
    "                    del self.conv.forward\n",
    "                else:\n",
    "                    x = self.conv(x)\n",
    "            elif layer == 'norm' and norm and self.with_norm:\n",
    "                x = self.norm(x)\n",
    "            elif layer == 'act' and activate and self.with_activation:\n",
    "                x = self.activate(x)\n",
    "            layer_index += 1\n",
    "        return x\n",
    "\n",
    "    def turn_on_efficient_conv_bn_eval(self, efficient_conv_bn_eval=True):\n",
    "        # efficient_conv_bn_eval works for conv + bn\n",
    "        # with `track_running_stats` option\n",
    "        if efficient_conv_bn_eval and self.norm \\\n",
    "                            and isinstance(self.norm, _BatchNorm) \\\n",
    "                            and self.norm.track_running_stats:\n",
    "            self.efficient_conv_bn_eval_forward = efficient_conv_bn_eval_forward  # noqa: E501\n",
    "        else:\n",
    "            self.efficient_conv_bn_eval_forward = None  # type: ignore\n",
    "\n",
    "    @staticmethod\n",
    "    def create_from_conv_bn(conv: torch.nn.modules.conv._ConvNd,\n",
    "                            bn: torch.nn.modules.batchnorm._BatchNorm,\n",
    "                            efficient_conv_bn_eval=True) -> 'ConvModule':\n",
    "        \"\"\"Create a ConvModule from a conv and a bn module.\"\"\"\n",
    "        self = ConvModule.__new__(ConvModule)\n",
    "        super(ConvModule, self).__init__()\n",
    "\n",
    "        self.conv_cfg = None\n",
    "        self.norm_cfg = None\n",
    "        self.act_cfg = None\n",
    "        self.inplace = False\n",
    "        self.with_spectral_norm = False\n",
    "        self.with_explicit_padding = False\n",
    "        self.order = ('conv', 'norm', 'act')\n",
    "\n",
    "        self.with_norm = True\n",
    "        self.with_activation = False\n",
    "        self.with_bias = conv.bias is not None\n",
    "\n",
    "        # build convolution layer\n",
    "        self.conv = conv\n",
    "        # export the attributes of self.conv to a higher level for convenience\n",
    "        self.in_channels = self.conv.in_channels\n",
    "        self.out_channels = self.conv.out_channels\n",
    "        self.kernel_size = self.conv.kernel_size\n",
    "        self.stride = self.conv.stride\n",
    "        self.padding = self.conv.padding\n",
    "        self.dilation = self.conv.dilation\n",
    "        self.transposed = self.conv.transposed\n",
    "        self.output_padding = self.conv.output_padding\n",
    "        self.groups = self.conv.groups\n",
    "\n",
    "        # build normalization layers\n",
    "        self.norm_name, norm = 'bn', bn\n",
    "        self.add_module(self.norm_name, norm)\n",
    "\n",
    "        self.turn_on_efficient_conv_bn_eval(efficient_conv_bn_eval)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0619a",
   "metadata": {},
   "source": [
    "### Feature Pyramid Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04a65206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(nn.Module):\n",
    "    \"\"\"Feature Pyramid Network.\n",
    "\n",
    "    This neck is the implementation of `Feature Pyramid Networks for Object\n",
    "    Detection <https://arxiv.org/abs/1612.03144>`_.\n",
    "\n",
    "    Args:\n",
    "        in_channels (list[int]): Number of input channels per scale.\n",
    "        out_channels (int): Number of output channels (used at each scale).\n",
    "        num_outs (int): Number of output scales.\n",
    "        start_level (int): Index of the start input backbone level used to\n",
    "            build the feature pyramid. Default: 0.\n",
    "        end_level (int): Index of the end input backbone level (exclusive) to\n",
    "            build the feature pyramid. Default: -1, which means the last level.\n",
    "        add_extra_convs (bool | str): If bool, it decides whether to add conv\n",
    "            layers on top of the original feature maps. Default to False.\n",
    "            If True, its actual mode is specified by `extra_convs_on_inputs`.\n",
    "            If str, it specifies the source feature map of the extra convs.\n",
    "            Only the following options are allowed\n",
    "\n",
    "            - 'on_input': Last feat map of neck inputs (i.e. backbone feature).\n",
    "            - 'on_lateral': Last feature map after lateral convs.\n",
    "            - 'on_output': The last output feature map after fpn convs.\n",
    "        extra_convs_on_inputs (bool, deprecated): Whether to apply extra convs\n",
    "            on the original feature from the backbone. If True,\n",
    "            it is equivalent to `add_extra_convs='on_input'`. If False, it is\n",
    "            equivalent to set `add_extra_convs='on_output'`. Default to True.\n",
    "        relu_before_extra_convs (bool): Whether to apply relu before the extra\n",
    "            conv. Default: False.\n",
    "        no_norm_on_lateral (bool): Whether to apply norm on lateral.\n",
    "            Default: False.\n",
    "        conv_cfg (dict): Config dict for convolution layer. Default: None.\n",
    "        norm_cfg (dict): Config dict for normalization layer. Default: None.\n",
    "        act_cfg (dict): Config dict for activation layer in ConvModule.\n",
    "            Default: None.\n",
    "        upsample_cfg (dict): Config dict for interpolate layer.\n",
    "            Default: dict(mode='nearest').\n",
    "        init_cfg (dict or list[dict], optional): Initialization config dict.\n",
    "\n",
    "    Example:\n",
    "        >>> import torch\n",
    "        >>> in_channels = [2, 3, 5, 7]\n",
    "        >>> scales = [340, 170, 84, 43]\n",
    "        >>> inputs = [torch.rand(1, c, s, s)\n",
    "        ...           for c, s in zip(in_channels, scales)]\n",
    "        >>> self = FPN(in_channels, 11, len(in_channels)).eval()\n",
    "        >>> outputs = self.forward(inputs)\n",
    "        >>> for i in range(len(outputs)):\n",
    "        ...     print(f'outputs[{i}].shape = {outputs[i].shape}')\n",
    "        outputs[0].shape = torch.Size([1, 11, 340, 340])\n",
    "        outputs[1].shape = torch.Size([1, 11, 170, 170])\n",
    "        outputs[2].shape = torch.Size([1, 11, 84, 84])\n",
    "        outputs[3].shape = torch.Size([1, 11, 43, 43])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 num_outs,\n",
    "                 start_level=0,\n",
    "                 end_level=-1,\n",
    "                 add_extra_convs=False,\n",
    "                 extra_convs_on_inputs=False,\n",
    "                 relu_before_extra_convs=False,\n",
    "                 no_norm_on_lateral=False,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=None,\n",
    "                 act_cfg=None,\n",
    "                 upsample_cfg=dict(mode='nearest')):\n",
    "        super().__init__()\n",
    "        assert isinstance(in_channels, list)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_ins = len(in_channels)\n",
    "        self.num_outs = num_outs\n",
    "        self.relu_before_extra_convs = relu_before_extra_convs\n",
    "        self.no_norm_on_lateral = no_norm_on_lateral\n",
    "        self.fp16_enabled = False\n",
    "        self.upsample_cfg = upsample_cfg.copy()\n",
    "\n",
    "        if end_level == -1:\n",
    "            self.backbone_end_level = self.num_ins\n",
    "            assert num_outs >= self.num_ins - start_level\n",
    "        else:\n",
    "            # if end_level < inputs, no extra level is allowed\n",
    "            self.backbone_end_level = end_level\n",
    "            assert end_level <= len(in_channels)\n",
    "            assert num_outs == end_level - start_level\n",
    "        self.start_level = start_level\n",
    "        self.end_level = end_level\n",
    "        self.add_extra_convs = add_extra_convs\n",
    "        assert isinstance(add_extra_convs, (str, bool))\n",
    "        if isinstance(add_extra_convs, str):\n",
    "            # Extra_convs_source choices: 'on_input', 'on_lateral', 'on_output'\n",
    "            assert add_extra_convs in ('on_input', 'on_lateral', 'on_output')\n",
    "        elif add_extra_convs:  # True\n",
    "            if extra_convs_on_inputs:\n",
    "                # For compatibility with previous release\n",
    "                # TODO: deprecate `extra_convs_on_inputs`\n",
    "                self.add_extra_convs = 'on_input'\n",
    "            else:\n",
    "                self.add_extra_convs = 'on_output'\n",
    "\n",
    "        self.lateral_convs = nn.ModuleList()\n",
    "        self.fpn_convs = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.start_level, self.backbone_end_level):\n",
    "            l_conv = ConvModule(\n",
    "                in_channels[i],\n",
    "                out_channels,\n",
    "                1,\n",
    "                conv_cfg=conv_cfg,\n",
    "                norm_cfg=norm_cfg if not self.no_norm_on_lateral else None,\n",
    "                act_cfg=act_cfg,\n",
    "                inplace=False)\n",
    "            fpn_conv = ConvModule(\n",
    "                out_channels,\n",
    "                out_channels,\n",
    "                3,\n",
    "                padding=1,\n",
    "                conv_cfg=conv_cfg,\n",
    "                norm_cfg=norm_cfg,\n",
    "                act_cfg=act_cfg,\n",
    "                inplace=False)\n",
    "\n",
    "            self.lateral_convs.append(l_conv)\n",
    "            self.fpn_convs.append(fpn_conv)\n",
    "\n",
    "        # add extra conv layers (e.g., RetinaNet)\n",
    "        extra_levels = num_outs - self.backbone_end_level + self.start_level\n",
    "        if self.add_extra_convs and extra_levels >= 1:\n",
    "            for i in range(extra_levels):\n",
    "                if i == 0 and self.add_extra_convs == 'on_input':\n",
    "                    in_channels = self.in_channels[self.backbone_end_level - 1]\n",
    "                else:\n",
    "                    in_channels = out_channels\n",
    "                extra_fpn_conv = ConvModule(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    conv_cfg=conv_cfg,\n",
    "                    norm_cfg=norm_cfg,\n",
    "                    act_cfg=act_cfg,\n",
    "                    inplace=False)\n",
    "                self.fpn_convs.append(extra_fpn_conv)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        assert len(inputs) == len(self.in_channels)\n",
    "\n",
    "        # build laterals\n",
    "        laterals = [\n",
    "            lateral_conv(inputs[i + self.start_level])\n",
    "            for i, lateral_conv in enumerate(self.lateral_convs)\n",
    "        ]\n",
    "\n",
    "        # build top-down path\n",
    "        used_backbone_levels = len(laterals)\n",
    "        for i in range(used_backbone_levels - 1, 0, -1):\n",
    "            # In some cases, fixing `scale factor` (e.g. 2) is preferred, but\n",
    "            #  it cannot co-exist with `size` in `F.interpolate`.\n",
    "            if 'scale_factor' in self.upsample_cfg:\n",
    "                laterals[i - 1] = laterals[i - 1] + resize(\n",
    "                    laterals[i], **self.upsample_cfg)\n",
    "            else:\n",
    "                prev_shape = laterals[i - 1].shape[2:]\n",
    "                laterals[i - 1] = laterals[i - 1] + resize(\n",
    "                    laterals[i], size=prev_shape, **self.upsample_cfg)\n",
    "\n",
    "        # build outputs\n",
    "        # part 1: from original levels\n",
    "        outs = [\n",
    "            self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels)\n",
    "        ]\n",
    "        # part 2: add extra levels\n",
    "        if self.num_outs > len(outs):\n",
    "            # use max pool to get more levels on top of outputs\n",
    "            # (e.g., Faster R-CNN, Mask R-CNN)\n",
    "            if not self.add_extra_convs:\n",
    "                for i in range(self.num_outs - used_backbone_levels):\n",
    "                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n",
    "            # add conv layers on top of original feature maps (RetinaNet)\n",
    "            else:\n",
    "                if self.add_extra_convs == 'on_input':\n",
    "                    extra_source = inputs[self.backbone_end_level - 1]\n",
    "                elif self.add_extra_convs == 'on_lateral':\n",
    "                    extra_source = laterals[-1]\n",
    "                elif self.add_extra_convs == 'on_output':\n",
    "                    extra_source = outs[-1]\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                outs.append(self.fpn_convs[used_backbone_levels](extra_source))\n",
    "                for i in range(used_backbone_levels + 1, self.num_outs):\n",
    "                    if self.relu_before_extra_convs:\n",
    "                        outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n",
    "                    else:\n",
    "                        outs.append(self.fpn_convs[i](outs[-1]))\n",
    "        return tuple(outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998528b2",
   "metadata": {},
   "source": [
    "### Mamba Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2dd68d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state:\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "\n",
    "# FIXME\n",
    "from transformers.models.mamba.modeling_mamba import logger, is_fast_path_available, mamba_inner_fn, causal_conv1d_fn, causal_conv1d_update, selective_state_update, selective_scan_fn\n",
    "from transformers.models.mamba.modeling_mamba import MambaRMSNorm, MambaPreTrainedModel, MambaCache, MambaOutput, MambaMixer\n",
    "from transformers import MambaConfig\n",
    "from transformers.activations import ACT2FN\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define the gating model\n",
    "\n",
    "class MambaMixer(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute ∆, A, B, C, and D the state space parameters and compute the `contextualized_states`.\n",
    "    A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n",
    "    ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n",
    "    and is why Mamba is called **selective** state spaces)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, layer_idx, head_num=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.ssm_state_size = config.state_size\n",
    "        self.conv_kernel_size = config.conv_kernel\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.time_step_rank = config.time_step_rank\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_num = head_num\n",
    "        self.use_conv_bias = config.use_conv_bias\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.intermediate_size,\n",
    "            out_channels=self.intermediate_size,\n",
    "            bias=config.use_conv_bias,\n",
    "            kernel_size=config.conv_kernel,\n",
    "            groups=self.intermediate_size,\n",
    "            padding=config.conv_kernel - 1,\n",
    "        )\n",
    "        self.conv1d_back = nn.Conv1d(\n",
    "            in_channels=self.intermediate_size,\n",
    "            out_channels=self.intermediate_size,\n",
    "            bias=config.use_conv_bias,\n",
    "            kernel_size=config.conv_kernel,\n",
    "            groups=self.intermediate_size,\n",
    "            padding=config.conv_kernel - 1,\n",
    "        )\n",
    "\n",
    "        self.activation = config.hidden_act\n",
    "        self.act = ACT2FN[config.hidden_act]\n",
    "\n",
    "        # projection of the input hidden states\n",
    "        self.in_proj = nn.Linear(self.hidden_size, self.intermediate_size * 2, bias=config.use_bias)\n",
    "        self.in_proj_dif = nn.Linear(self.hidden_size, self.intermediate_size * 2, bias=config.use_bias)\n",
    "        # selective projection used to make dt, B and C input dependant\n",
    "        self.x_proj = nn.Linear(self.intermediate_size, self.time_step_rank + self.ssm_state_size * 2, bias=False)\n",
    "        self.x_proj_back = nn.Linear(self.intermediate_size, self.time_step_rank + self.ssm_state_size * 2, bias=False)\n",
    "        self.x_proj_dif = nn.Linear(self.intermediate_size, self.time_step_rank + self.ssm_state_size * 2, bias=False)\n",
    "        self.x_proj_dif_back = nn.Linear(self.intermediate_size, self.time_step_rank + self.ssm_state_size * 2, bias=False)\n",
    "        # time step projection (discretization)\n",
    "        self.dt_proj = nn.Linear(self.time_step_rank, self.intermediate_size, bias=True)\n",
    "        self.dt_proj_back = nn.Linear(self.time_step_rank, self.intermediate_size, bias=True)\n",
    "        self.dt_proj_dif = nn.Linear(self.time_step_rank, self.intermediate_size, bias=True)\n",
    "        self.dt_proj_dif_back = nn.Linear(self.time_step_rank, self.intermediate_size, bias=True)\n",
    "\n",
    "        self.linear_hid2 = nn.Linear(self.intermediate_size,2*self.intermediate_size, bias=True)\n",
    "        self.linear_hid2_back = nn.Linear(self.intermediate_size, 2*self.intermediate_size, bias=True)\n",
    "\n",
    "        # S4D real initialization. These are not discretized!\n",
    "        # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n",
    "        A = torch.arange(1, self.ssm_state_size + 1, dtype=torch.float32)[None, :]\n",
    "        A = A.expand(self.intermediate_size, -1).contiguous()\n",
    "\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.A_log_back = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(self.intermediate_size))\n",
    "        self.D_back = nn.Parameter(torch.ones(self.intermediate_size))\n",
    "        self.out_proj = nn.Linear(1*self.intermediate_size, self.hidden_size, bias=config.use_bias)\n",
    "        self.out_LN = nn.LayerNorm(self.intermediate_size)\n",
    "        self.use_bias = config.use_bias\n",
    "\n",
    "        if not is_fast_path_available:\n",
    "            logger.warning_once(\n",
    "                \"The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n",
    "                \" is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and\"\n",
    "                \" https://github.com/Dao-AILab/causal-conv1d\"\n",
    "            )\n",
    "\n",
    "    def cuda_kernels_forward(self, hidden_states: torch.Tensor, hidden_states_dif: torch.Tensor, cache_params: Optional[MambaCache] = None, cache_params_2: Optional[MambaCache] = None):\n",
    "        # 1. Gated MLP's linear projection\n",
    "        batch_size, seq_len, _ = hidden_states.shape\n",
    "        flag_one = False\n",
    "        if hidden_states_dif is None:\n",
    "            flag_one = True\n",
    "            hidden_states_dif = hidden_states\n",
    "        hidden_states = torch.cat([hidden_states, hidden_states_dif], dim=0)\n",
    "        # input_hidden_states_dif = hidden_states_dif\n",
    "        projected_states = self.in_proj(hidden_states).transpose(1, 2)\n",
    "        projected_states_dif = self.in_proj_dif(hidden_states).transpose(1, 2)\n",
    "\n",
    "        # process\n",
    "        hidden_states, gate = projected_states.chunk(2, dim=1)\n",
    "        hidden_states_dif, gate_dif = projected_states_dif.chunk(2, dim=1)\n",
    "        # gate = gate[:batch_size]\n",
    "        gate = gate_dif[batch_size:] if not flag_one else gate[:batch_size]\n",
    "\n",
    "        gate_back = gate.flip(-1)\n",
    "\n",
    "        # 2. Convolution sequence transformation\n",
    "        conv_weights = self.conv1d.weight.view(self.conv1d.weight.size(0), self.conv1d.weight.size(2))\n",
    "        # conv_weights_back = self.conv1d_back.weight.view(self.conv1d_back.weight.size(0), self.conv1d_back.weight.size(2))\n",
    "        hidden_states_cat = causal_conv1d_fn(\n",
    "            hidden_states, conv_weights, self.conv1d.bias, activation=self.activation\n",
    "        )\n",
    "        # hidden_states_back = hidden_states.flip(-1)\n",
    "        # hidden_states_back_cat = causal_conv1d_fn(\n",
    "        #     hidden_states_back, conv_weights_back, self.conv1d_back.bias, activation=self.activation\n",
    "        # )\n",
    "        hidden_states_back_cat = hidden_states_cat.flip(-1)\n",
    "\n",
    "        # 3. State Space Model sequence transformation\n",
    "        # 3.a. input varying initialization of time_step, B and C\n",
    "        hidden_states = hidden_states_cat[:batch_size]  # [batch, seq_len, intermediate_size]\n",
    "\n",
    "        ## 反向排序：\n",
    "        hidden_states_back = hidden_states_back_cat[:batch_size]  # [batch, seq_len, intermediate_size]\n",
    "\n",
    "        ssm_parameters = self.x_proj(hidden_states.transpose(1, 2))\n",
    "        time_step, B, C = torch.split(\n",
    "            ssm_parameters, [self.time_step_rank, self.ssm_state_size, self.ssm_state_size], dim=-1\n",
    "        )\n",
    "        discrete_time_step = self.dt_proj.weight @ time_step.transpose(1, 2)\n",
    "        # 反向：\n",
    "        ssm_parameters_back = self.x_proj_back(hidden_states_back.transpose(1, 2))\n",
    "        time_step_back, B_back, C_back = torch.split(\n",
    "            ssm_parameters_back, [self.time_step_rank, self.ssm_state_size, self.ssm_state_size], dim=-1\n",
    "        )\n",
    "        discrete_time_step_back = self.dt_proj_back.weight @ time_step_back.transpose(1, 2)\n",
    "\n",
    "\n",
    "        A = -torch.exp(self.A_log.float())\n",
    "        A_back = -torch.exp(self.A_log_back.float())\n",
    "        # 3.c perform the recurrence y ← SSM(A, B, C)(x)\n",
    "        time_proj_bias = self.dt_proj.bias.float() if hasattr(self.dt_proj, \"bias\") else None\n",
    "        time_proj_bias_back = self.dt_proj_back.bias.float() if hasattr(self.dt_proj_back, \"bias\") else None\n",
    "\n",
    "        scan_outputs, ssm_state = selective_scan_fn(\n",
    "            hidden_states,\n",
    "            discrete_time_step,\n",
    "            A,\n",
    "            B.transpose(1, 2),\n",
    "            C.transpose(1, 2),\n",
    "            self.D.float(),\n",
    "            gate,#None,\n",
    "            time_proj_bias,\n",
    "            delta_softplus=True,\n",
    "            return_last_state=True,\n",
    "        )\n",
    "        scan_outputs_back, ssm_state_back = selective_scan_fn(\n",
    "            hidden_states.flip(-1),\n",
    "            discrete_time_step_back,\n",
    "            A_back,\n",
    "            B_back.transpose(1, 2),\n",
    "            C_back.transpose(1, 2),\n",
    "            self.D_back.float(),\n",
    "            gate_back, #None, #\n",
    "            time_proj_bias_back,\n",
    "            delta_softplus=True,\n",
    "            return_last_state=True,\n",
    "        )\n",
    "\n",
    "        # 4. Final linear projection\n",
    "        contextualized_states = self.out_proj((scan_outputs+scan_outputs_back.flip(-1)).transpose(1, 2))\n",
    "        return contextualized_states\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states, hidden_states_2, cache_params: Optional[MambaCache] = None, cache_params_2: Optional[MambaCache] = None):\n",
    "        if is_fast_path_available and \"cuda\" in self.x_proj.weight.device.type:\n",
    "            return self.cuda_kernels_forward(hidden_states, hidden_states_2, cache_params, cache_params_2)\n",
    "        else:\n",
    "            raise NotImplementedError(\"The fast path is not available\")\n",
    "\n",
    "\n",
    "class CaMambaBlock(nn.Module):\n",
    "    def __init__(self, config, layer_idx, head_num=1, length=49, craft=False):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.residual_in_fp32 = config.residual_in_fp32\n",
    "        self.norm = MambaRMSNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.head_num = head_num\n",
    "        self.length = length\n",
    "        self.craft = craft\n",
    "        self.config.intermediate_size = config.intermediate_size\n",
    "        self.mixer = MambaMixer(config, layer_idx=layer_idx)\n",
    "\n",
    "    def forward(self, hidden_states, hidden_states_2, cache_params: Optional[MambaCache] = None, cache_params_2: Optional[MambaCache] = None,):\n",
    "        residual = hidden_states\n",
    "        # if hidden_states_2==None:\n",
    "        #     residual = hidden_states[:,:,:768]\n",
    "        #     hidden_states = self.linear(hidden_states)\n",
    "        hidden_states = self.norm(hidden_states.to(dtype=self.norm.weight.dtype))\n",
    "        # hidden_states_2 = self.norm(hidden_states_2.to(dtype=self.norm.weight.dtype))\n",
    "        if self.residual_in_fp32:\n",
    "            residual = residual.to(torch.float32)\n",
    "\n",
    "        hidden_states = self.mixer(hidden_states, hidden_states_2, cache_params=cache_params, cache_params_2=cache_params_2)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "class CaMambaModel(MambaPreTrainedModel):\n",
    "    def __init__(self, config, head_num=1, length=49, craft=False):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([CaMambaBlock(config, layer_idx=idx) for idx in range(config.num_hidden_layers)])\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        self.norm_f = MambaRMSNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs_embeds: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds_2: Optional[torch.LongTensor] = None,\n",
    "        cache_params: Optional[MambaCache] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,  # `attention_mask` is passed by the tokenizer and we don't want it\n",
    "    ) -> Union[Tuple, MambaOutput]:\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else (self.config.use_cache if not self.training else False)\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # FIXME\n",
    "        cache_params = None\n",
    "        use_cache = False\n",
    "        cache_params_2 = cache_params\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "        hidden_states_2 = inputs_embeds_2\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        for mixer_block in self.layers:\n",
    "            assert len(self.layers)==1\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                hidden_states = self._gradient_checkpointing_func(mixer_block.__call__, hidden_states, cache_params)\n",
    "            else:\n",
    "                hidden_states = mixer_block(hidden_states, hidden_states_2, cache_params=cache_params, cache_params_2=cache_params_2)\n",
    "\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if use_cache:\n",
    "            cache_params.seqlen_offset += inputs_embeds.shape[1]\n",
    "            cache_params_2.seqlen_offset += inputs_embeds_2.shape[1]\n",
    "\n",
    "        hidden_states = self.norm_f(hidden_states)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, cache_params, all_hidden_states] if v is not None)\n",
    "\n",
    "        return MambaOutput(\n",
    "            last_hidden_state=hidden_states,\n",
    "            cache_params=cache_params if use_cache else None,\n",
    "            hidden_states=all_hidden_states,\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = MambaConfig(num_hidden_layers=1)\n",
    "    model = CaMambaModel(config, head_num=1, length=49, craft=True)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    input_embeds = torch.randn(4, 49, 768).to(device)\n",
    "    input_embeds_2 = torch.randn(4, 49, 768).to(device)\n",
    "    out1 = model(inputs_embeds=input_embeds, inputs_embeds_2=input_embeds_2).last_hidden_state\n",
    "    print('last_hidden_state:')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a095166",
   "metadata": {},
   "source": [
    "### Resblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daa0ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class resblock(nn.Module):\n",
    "    \"\"\"\n",
    "    module: Residual Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inchannel, outchannel, stride=1, shortcut=None):\n",
    "        super(resblock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            # nn.Conv2d(inchannel, int(outchannel / 1), kernel_size=1),\n",
    "            # nn.LayerNorm(int(outchannel/2),dim=1),\n",
    "            nn.BatchNorm2d(int(outchannel / 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                int(outchannel / 1),\n",
    "                int(outchannel / 1),\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            # nn.LayerNorm(int(outchannel/2),dim=1),\n",
    "            nn.BatchNorm2d(int(outchannel / 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(int(outchannel / 1), outchannel, kernel_size=1),\n",
    "            # nn.LayerNorm(int(outchannel / 1),dim=1)\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "        )\n",
    "        self.right = shortcut\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        residual = x\n",
    "        out = out + residual\n",
    "        return self.act(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c6995",
   "metadata": {},
   "source": [
    "### Modified Attentive Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80407ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedAttentiveEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified visual transformer block that takes three inputs:\n",
    "    - img_A: before image\n",
    "    - img_B: after image\n",
    "    - change_seg: change segmentation mask\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, feature_size, heads, dropout=0.0):\n",
    "        super(ModifiedAttentiveEncoder, self).__init__()\n",
    "        h_feat, w_feat, channels = feature_size\n",
    "        self.h_feat = h_feat\n",
    "        self.w_feat = w_feat\n",
    "        self.n_layers = n_layers\n",
    "        self.channels = channels\n",
    "        \n",
    "        # position embedding\n",
    "        self.h_embedding = nn.Embedding(h_feat, int(channels / 2))\n",
    "        self.w_embedding = nn.Embedding(w_feat, int(channels / 2))\n",
    "        \n",
    "        # Mamba configs\n",
    "        config_1 = MambaConfig(num_hidden_layers=1, conv_kernel=3, hidden_size=channels)\n",
    "        config_2 = MambaConfig(num_hidden_layers=1, conv_kernel=3, hidden_size=channels)\n",
    "        config_3 = MambaConfig(num_hidden_layers=1, conv_kernel=3, hidden_size=channels)  # For change segmentation\n",
    "        \n",
    "        # Module lists\n",
    "        self.CaMalayer_list = nn.ModuleList([])\n",
    "        self.fuselayer_list = nn.ModuleList([])\n",
    "        self.fuselayer_list_2 = nn.ModuleList([])\n",
    "        self.linear_dif = nn.ModuleList([])\n",
    "        self.linear_img1 = nn.ModuleList([])\n",
    "        self.linear_img2 = nn.ModuleList([])\n",
    "        self.linear_seg = nn.ModuleList([])  # New linear layer for segmentation\n",
    "        self.Conv1_list = nn.ModuleList([])\n",
    "        self.LN_list = nn.ModuleList([])\n",
    "        self.seg_processor = nn.ModuleList([])  # Process segmentation mask\n",
    "        \n",
    "        embed_dim = channels\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            # Mamba blocks for images and segmentation\n",
    "            self.CaMalayer_list.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        CaMambaModel(config_1),  # For img_A\n",
    "                        CaMambaModel(config_1),  # For img_B\n",
    "                        CaMambaModel(config_3),  # For change_seg\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Fusion layers\n",
    "            self.fuselayer_list.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        CaMambaModel(config_2),  # For temporal fusion\n",
    "                        CaMambaModel(config_2),  # For segmentation fusion\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Attention-guided fusion\n",
    "            self.linear_seg.append(nn.Linear(channels, channels))\n",
    "            \n",
    "            # Final fusion conv\n",
    "            self.Conv1_list.append(nn.Conv2d(channels * 3, embed_dim, kernel_size=1))  # Updated to 3 inputs\n",
    "            self.LN_list.append(resblock(embed_dim, embed_dim))\n",
    "            \n",
    "        self.act = nn.Tanh()\n",
    "        self.layerscan = CaMambaModel(config_1)\n",
    "        self.LN_norm = nn.LayerNorm(channels)\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "        self.beta = nn.Parameter(torch.tensor(0.0), requires_grad=True)  # Weight for segmentation contribution\n",
    "        \n",
    "        # Fusion bi-temporal feat for captioning decoder\n",
    "        self.cos = torch.nn.CosineSimilarity(dim=1)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        \"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def add_pos_embedding(self, x):\n",
    "        if len(x.shape) == 3:  # NLD\n",
    "            b = x.shape[0]\n",
    "            c = x.shape[-1]\n",
    "            x = x.transpose(-1, 1).view(b, c, self.h_feat, self.w_feat)\n",
    "        batch, c, h, w = x.shape\n",
    "        pos_h = torch.arange(h).cuda()\n",
    "        pos_w = torch.arange(w).cuda()\n",
    "        embed_h = self.w_embedding(pos_h)\n",
    "        embed_w = self.h_embedding(pos_w)\n",
    "        pos_embedding = torch.cat(\n",
    "            [\n",
    "                embed_w.unsqueeze(0).repeat(h, 1, 1),\n",
    "                embed_h.unsqueeze(1).repeat(1, w, 1),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        pos_embedding = (\n",
    "            pos_embedding.permute(2, 0, 1).unsqueeze(0).repeat(batch, 1, 1, 1)\n",
    "        )\n",
    "        x = x + pos_embedding\n",
    "        # reshape back to NLD\n",
    "        x = x.view(b, c, -1).transpose(-1, 1)  # NLD (b,hw,c)\n",
    "        return x\n",
    "\n",
    "    def forward(self, img_A, img_B, change_seg):\n",
    "        h, w = self.h_feat, self.w_feat\n",
    "\n",
    "        # 1. Add positional embeddings to all inputs\n",
    "        img_A = self.add_pos_embedding(img_A)\n",
    "        img_B = self.add_pos_embedding(img_B)\n",
    "        change_seg = self.add_pos_embedding(change_seg)  # Add position embeddings to segmentation\n",
    "\n",
    "        # captioning setup\n",
    "        batch, c = img_A.shape[0], img_A.shape[-1]\n",
    "        img_sa1, img_sa2, seg_sa = img_A, img_B, change_seg\n",
    "\n",
    "        # Method: Mamba with segmentation integration\n",
    "        img_list = []\n",
    "        N, L, D = img_sa1.shape\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            # Difference features\n",
    "            dif = img_sa2 - img_sa1\n",
    "            \n",
    "            # SD-SSM for images with difference guidance\n",
    "            img_sa1 = self.CaMalayer_list[i][0](\n",
    "                inputs_embeds=img_sa1, inputs_embeds_2=dif\n",
    "            ).last_hidden_state\n",
    "            \n",
    "            img_sa2 = self.CaMalayer_list[i][1](\n",
    "                inputs_embeds=img_sa2, inputs_embeds_2=dif\n",
    "            ).last_hidden_state\n",
    "            \n",
    "            # Process segmentation with the guidance from both images\n",
    "            seg_guidance = dif * self.linear_seg[i](seg_sa)\n",
    "            seg_sa = self.CaMalayer_list[i][2](\n",
    "                inputs_embeds=seg_sa, inputs_embeds_2=seg_guidance\n",
    "            ).last_hidden_state\n",
    "\n",
    "            # TT-SSM: Temporal fusion with segmentation integration\n",
    "            scan_mode = \"TT-SSM\"\n",
    "            if scan_mode == \"TT-SSM\":\n",
    "                # Normalize features\n",
    "                img_sa1 = self.LN_norm(img_sa1)\n",
    "                img_sa2 = self.LN_norm(img_sa2)\n",
    "                seg_sa = self.LN_norm(seg_sa)\n",
    "                \n",
    "                # Save residuals\n",
    "                img_sa1_res = img_sa1\n",
    "                img_sa2_res = img_sa2\n",
    "                seg_sa_res = seg_sa\n",
    "                \n",
    "                # Temporal fusion (img_A and img_B)\n",
    "                img_fuse1 = img_sa1.permute(0, 2, 1).unsqueeze(-1)  # (N,D,L,1)\n",
    "                img_fuse2 = img_sa2.permute(0, 2, 1).unsqueeze(-1)  # (N,D,L,1)\n",
    "                img_fuse = torch.cat([img_fuse1, img_fuse2], dim=-1).reshape(\n",
    "                    N, D, -1\n",
    "                )  # (N,D,L*2)\n",
    "                \n",
    "                # Apply temporal fusion Mamba\n",
    "                img_fuse = self.fuselayer_list[i][0](\n",
    "                    inputs_embeds=img_fuse.permute(0, 2, 1)\n",
    "                ).last_hidden_state.permute(0, 2, 1)  # (N,D,L*2)\n",
    "                \n",
    "                # Reshape and extract fused features\n",
    "                img_fuse = img_fuse.reshape(N, D, L, -1)\n",
    "                img_sa1 = img_fuse[..., 0].permute(0, 2, 1)  # (N,L,D)\n",
    "                img_sa2 = img_fuse[..., 1].permute(0, 2, 1)  # (N,L,D)\n",
    "                \n",
    "                # Segmentation-image fusion\n",
    "                seg_img_fuse = torch.cat([\n",
    "                    seg_sa.permute(0, 2, 1).unsqueeze(-1),\n",
    "                    ((img_sa1 + img_sa2)/2).permute(0, 2, 1).unsqueeze(-1)\n",
    "                ], dim=-1).reshape(N, D, -1)\n",
    "                \n",
    "                # Apply segmentation fusion Mamba\n",
    "                seg_img_fuse = self.fuselayer_list[i][1](\n",
    "                    inputs_embeds=seg_img_fuse.permute(0, 2, 1)\n",
    "                ).last_hidden_state.permute(0, 2, 1)\n",
    "                \n",
    "                # Extract fused segmentation features\n",
    "                seg_img_fuse = seg_img_fuse.reshape(N, D, L, -1)\n",
    "                seg_sa = seg_img_fuse[..., 0].permute(0, 2, 1)  # (N,L,D)\n",
    "                \n",
    "                # Residual connections with learnable weights\n",
    "                img_sa1 = self.LN_norm(img_sa1) + img_sa1_res * self.alpha\n",
    "                img_sa2 = self.LN_norm(img_sa2) + img_sa2_res * self.alpha\n",
    "                seg_sa = self.LN_norm(seg_sa) + seg_sa_res * self.beta\n",
    "\n",
    "            # Final layer - combine all three modalities\n",
    "            if i == self.n_layers - 1:\n",
    "                # Reshape to spatial features\n",
    "                img1_cap = img_sa1.transpose(-1, 1).view(batch, c, h, w)\n",
    "                img2_cap = img_sa2.transpose(-1, 1).view(batch, c, h, w)\n",
    "                seg_cap = seg_sa.transpose(-1, 1).view(batch, c, h, w)\n",
    "                \n",
    "                # Concatenate all three modalities\n",
    "                feat_cap = torch.cat([img1_cap, img2_cap, seg_cap], dim=1)\n",
    "                \n",
    "                # Final fusion\n",
    "                feat_cap = self.LN_list[i](self.Conv1_list[i](feat_cap))\n",
    "                \n",
    "                # Reshape for output\n",
    "                img_fuse = feat_cap.view(batch, c, -1).transpose(-1, 1).unsqueeze(-1)\n",
    "                img_list.append(img_fuse)\n",
    "\n",
    "        # Output\n",
    "        feat_cap = img_list[-1][..., 0]\n",
    "        feat_cap = feat_cap.transpose(-1, 1)\n",
    "        return feat_cap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ade4e1",
   "metadata": {},
   "source": [
    "### Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7530ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(input,\n",
    "           size=None,\n",
    "           scale_factor=None,\n",
    "           mode='nearest',\n",
    "           align_corners=None,\n",
    "           warning=True):\n",
    "    if warning:\n",
    "        if size is not None and align_corners:\n",
    "            input_h, input_w = tuple(int(x) for x in input.shape[2:])\n",
    "            output_h, output_w = tuple(int(x) for x in size)\n",
    "            if output_h > input_h or output_w > output_h:\n",
    "                if ((output_h > 1 and output_w > 1 and input_h > 1\n",
    "                     and input_w > 1) and (output_h - 1) % (input_h - 1)\n",
    "                        and (output_w - 1) % (input_w - 1)):\n",
    "                    warnings.warn(\n",
    "                        f'When align_corners={align_corners}, '\n",
    "                        'the output would more aligned if '\n",
    "                        f'input size {(input_h, input_w)} is `x+1` and '\n",
    "                        f'out size {(output_h, output_w)} is `nx+1`')\n",
    "    return F.interpolate(input, size, scale_factor, mode, align_corners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b4acd",
   "metadata": {},
   "source": [
    "### SwinDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41f5c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = 32\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" Multilayer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LowRankBilinearAttention(nn.Module):\n",
    "    def __init__(self, d_model=128, d_k=64, d_v=256):\n",
    "        super(LowRankBilinearAttention, self).__init__()\n",
    "        self.WQ = nn.Linear(d_model, d_k)\n",
    "        self.WK = nn.Linear(d_model, d_k)\n",
    "        self.WV = nn.Linear(d_model, d_v)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        V = K.clone()\n",
    "        q = self.WQ(Q)\n",
    "        k = self.WK(K)\n",
    "        v = self.WV(V)\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n",
    "        attn_probs = self.softmax(attn_scores)\n",
    "        output = torch.matmul(attn_probs, v)\n",
    "        return output\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\" Swin Transformer Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        self.H = None\n",
    "        self.W = None\n",
    "        # self.low_rank_metric = nn.Parameter(torch.rand(256, 128))\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.proj = nn.Linear(256, 128)\n",
    "        self.proj_drop = nn.Dropout(drop)\n",
    "        self.low_rank_bilinear = LowRankBilinearAttention()\n",
    "\n",
    "    # def low_rank_bilinear(self, x, text):\n",
    "    #     x_ = self.proj_x(x)\n",
    "    #     text = self.proj_x(text)\n",
    "    #     x_hat = torch.matmul(x_, self.low_rank_metric)\n",
    "    #     text_hat = torch.matmul(text, self.low_rank_metric)\n",
    "    #     attention = torch.matmul(x_hat, text_hat.permute(0, 2, 1))\n",
    "    #     attention = self.attn_drop(self.softmax(attention))\n",
    "    #     out = torch.matmul(attention, text)\n",
    "    #     out = self.proj(out)\n",
    "    #     out = self.proj_drop(out)\n",
    "    #     return x+out\n",
    "\n",
    "    def forward(self, x, text, mask_matrix):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "            mask_matrix: Attention mask for cyclic shift.\n",
    "        \"\"\"\n",
    "        x_text = self.low_rank_bilinear(x, text)\n",
    "        x = self.proj_drop(self.proj(x_text))+x\n",
    "        B, L, C = x.shape\n",
    "        H, W = self.H, self.W\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # pad feature maps to multiples of window size\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchRecover(nn.Module):\n",
    "    \"\"\" Patch Merging Layer\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(dim, dim // 2, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.GroupNorm(num_channels=dim // 2, num_groups=groups),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.permute(0, 1, 2)  # B ,C, L\n",
    "        x = x.reshape(B, C, H, W)\n",
    "        x = self.up(x)  # B, C//2, H, W\n",
    "\n",
    "        x = x.reshape(B, C // 2, -1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # x = Variable(torch.randn(B, H * 2, W * 2, C // 2))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    \"\"\" Patch Merging Layer\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # padding\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "    Args:\n",
    "        dim (int): Number of feature channels\n",
    "        depth (int): Depths of this stage.\n",
    "        num_heads (int): Number of attention head.\n",
    "        window_size (int): Local window size. Default: 7.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 depth,\n",
    "                 num_heads,\n",
    "                 window_size=7,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 downsample=None,\n",
    "                 use_checkpoint=False,\n",
    "                 up=True):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.up = up\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop,\n",
    "                attn_drop=attn_drop,\n",
    "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, text, H, W):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate attention mask for SW-MSA\n",
    "        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
    "        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
    "        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            blk.H, blk.W = H, W\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, attn_mask)\n",
    "            else:\n",
    "                x = blk(x, text, attn_mask)\n",
    "        if self.downsample is not None:\n",
    "            x_down = self.downsample(x, H, W)\n",
    "            if self.up:\n",
    "                Wh, Ww = (H + 1) // 2, (W + 1) // 2\n",
    "            else:\n",
    "                Wh, Ww = H * 2, W * 2\n",
    "            return x, H, W, x_down, Wh, Ww\n",
    "        else:\n",
    "            return x, H, W, x, H, W\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        # padding\n",
    "        _, _, H, W = x.size()\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "        x = self.proj(x)  # B C Wh Ww\n",
    "        if self.norm is not None:\n",
    "            Wh, Ww = x.size(2), x.size(3)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(embed_dim)\n",
    "        self.maxPool = nn.MaxPool2d(kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        # padding\n",
    "        _, _, H, W = x.size()\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "        x = self.proj(x)  # B C Wh Ww\n",
    "        x = self.bn(x)\n",
    "        x = self.maxPool(x)\n",
    "        if self.norm is not None:\n",
    "            Wh, Ww = x.size(2), x.size(3)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    Up Convolution Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.GroupNorm(num_channels=out_ch, num_groups=groups),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.up = up_conv(in_channels, out_channels)\n",
    "        # self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv_relu = nn.Sequential(\n",
    "            nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\n",
    "            # coorAtt(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # x2 = self.att_block(x1, x2)\n",
    "        x1 = torch.cat((x2, x1), dim=1)\n",
    "        x1 = self.conv_relu(x1)\n",
    "        return x1\n",
    "\n",
    "\n",
    "class SwinUp(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(SwinUp, self).__init__()\n",
    "        self.up = nn.Linear(dim, dim * 2)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.up(x)\n",
    "        x = x.reshape(B, H, W, 2 * C)\n",
    "\n",
    "        x0 = x[:, :, :, 0:C // 2]\n",
    "        x1 = x[:, :, :, C // 2:C]\n",
    "        x2 = x[:, :, :, C:C + C // 2]\n",
    "        x3 = x[:, :, :, C + C // 2:C * 2]\n",
    "\n",
    "        x0 = torch.cat((x0, x1), dim=1)\n",
    "        x3 = torch.cat((x2, x3), dim=1)\n",
    "        x = torch.cat((x0, x3), dim=2)\n",
    "\n",
    "        # x = Variable(torch.randn(B, H * 2, W * 2, C // 2))\n",
    "\n",
    "        x = x.reshape(B, -1, C // 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 patch_size=4,\n",
    "                 depths=2,\n",
    "                 num_heads=6,\n",
    "                 window_size=7,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.2,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 patch_norm=True,\n",
    "                 use_checkpoint=False):\n",
    "        super(SwinDecoder, self).__init__()\n",
    "\n",
    "        self.patch_norm = patch_norm\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depths)]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layer = BasicLayer(\n",
    "            dim=embed_dim // 2,\n",
    "            depth=depths,\n",
    "            num_heads=num_heads,\n",
    "            window_size=window_size,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            drop=drop_rate,\n",
    "            attn_drop=attn_drop_rate,\n",
    "            drop_path=dpr,\n",
    "            norm_layer=norm_layer,\n",
    "            downsample=None,\n",
    "            use_checkpoint=use_checkpoint)\n",
    "\n",
    "        self.up = up_conv(embed_dim, embed_dim // 2)\n",
    "        self.conv_relu = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim // 2, embed_dim // 2, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, text):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "\n",
    "        # print(x.shape)\n",
    "        # for i in range(len(e_o)):\n",
    "        #    layer = self.layers[i]\n",
    "        #    x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n",
    "        # return x\n",
    "\n",
    "        identity = x\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.up(x)  # B , C//2, 2H, 2W\n",
    "        x = x.reshape(B, C // 2, H * W * 4)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # x2 = torch.rand(10, 2, 256).to(x.device)\n",
    "        # xx = torch.cat([x, x2], dim=1)\n",
    "\n",
    "        x_out, H, W, x, Wh, Ww = self.layer(x, text, H * 2, W * 2)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x.reshape(B, C // 2, H, W)\n",
    "        # B, C//4 2H, 2W\n",
    "        x = self.conv_relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Swin_Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, depths, num_heads):\n",
    "        super(Swin_Decoder, self).__init__()\n",
    "        self.up = SwinDecoder(in_channels, depths=depths, num_heads=num_heads)\n",
    "        # self.up1 = nn.Upsample(scale_factor=2)\n",
    "        # self.up2 = nn.Conv2d(in_channels, in_channels//2, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv_relu = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 2, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, text):\n",
    "        x1 = self.up(x1, text)\n",
    "        # x1 = self.up2(x1)\n",
    "        # x2 = self.att_block(x1, x2)\n",
    "        x2 = self.conv2(x2)\n",
    "        x1 = torch.cat((x2, x1), dim=1)\n",
    "        out = self.conv_relu(x1)\n",
    "        return out\n",
    "#dropout = None\n",
    "'''\n",
    "conv_seg = nn.Conv2d(256, 3, kernel_size=1)\n",
    "    def cls_seg(feat):\n",
    "        output = conv_seg(feat)\n",
    "        return output\n",
    "'''\n",
    "\n",
    "#@MODELS.register_module()\n",
    "class CLIPSwinTextDecode(nn.Module):\n",
    "    def __init__(self, channels,output_channels, text_dim=1024, **kwargs):\n",
    "        super().__init__()\n",
    "        self.layer1 = Swin_Decoder(channels, 2, 2)\n",
    "        self.layer2 = Swin_Decoder(channels, 2, 2)\n",
    "        self.layer3 = Swin_Decoder(channels, 2, 2)\n",
    "        self.text_dim = text_dim\n",
    "        self.linearA = nn.Sequential(nn.Linear(self.text_dim, 128),\n",
    "                                     nn.LayerNorm(128))\n",
    "        self.linearB = nn.Sequential(nn.Linear(self.text_dim, 128),\n",
    "                                     nn.LayerNorm(128))\n",
    "        #self.cls_seg = cls_seg()\n",
    "        self.output_channels = output_channels\n",
    "        self.conv_seg = nn.Conv2d(channels, out_channels=output_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, inputs, textA, textB):\n",
    "        e1, e2, e3, e4 = inputs\n",
    "        text = torch.cat([self.linearA(textA), self.linearB(textB)], dim=1)\n",
    "        d1 = self.layer1(e4, e3, text)\n",
    "        d2 = self.layer2(d1, e2, text)\n",
    "        d3 = self.layer3(d2, e1, text)\n",
    "        #print(d3.shape)\n",
    "        output = self.conv_seg(d3)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47316292",
   "metadata": {},
   "source": [
    "### Newsiwndecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a32326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mmseg.registry import MODELS\n",
    "#from .decode_head import BaseDecodeHead\n",
    "\n",
    "groups = 32\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" Multilayer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LowRankBilinearAttention(nn.Module):\n",
    "    def __init__(self, d_model=128, d_k=64, d_v=256):\n",
    "        super(LowRankBilinearAttention, self).__init__()\n",
    "        self.WQ = nn.Linear(d_model, d_k)\n",
    "        self.WK = nn.Linear(d_model, d_k)\n",
    "        self.WV = nn.Linear(d_model, d_v)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        V = K.clone()\n",
    "        q = self.WQ(Q)\n",
    "        k = self.WK(K)\n",
    "        v = self.WV(V)\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (q.size(-1) ** 0.5)\n",
    "        attn_probs = self.softmax(attn_scores)\n",
    "        output = torch.matmul(attn_probs, v)\n",
    "        return output\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\" Swin Transformer Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        self.H = None\n",
    "        self.W = None\n",
    "        # self.low_rank_metric = nn.Parameter(torch.rand(256, 128))\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.proj = nn.Linear(256, 128)\n",
    "        self.proj_drop = nn.Dropout(drop)\n",
    "        self.low_rank_bilinear = LowRankBilinearAttention()\n",
    "\n",
    "    # def low_rank_bilinear(self, x, image):\n",
    "    #     x_ = self.proj_x(x)\n",
    "    #     image = self.proj_x(image)\n",
    "    #     x_hat = torch.matmul(x_, self.low_rank_metric)\n",
    "    #     image_hat = torch.matmul(image, self.low_rank_metric)\n",
    "    #     attention = torch.matmul(x_hat, image_hat.permute(0, 2, 1))\n",
    "    #     attention = self.attn_drop(self.softmax(attention))\n",
    "    #     out = torch.matmul(attention, image)\n",
    "    #     out = self.proj(out)\n",
    "    #     out = self.proj_drop(out)\n",
    "    #     return x+out\n",
    "\n",
    "    def forward(self, x, image, mask_matrix):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "            mask_matrix: Attention mask for cyclic shift.\n",
    "        \"\"\"\n",
    "        x_image = self.low_rank_bilinear(x, image)\n",
    "        x = self.proj_drop(self.proj(x_image))+x\n",
    "        B, L, C = x.shape\n",
    "        H, W = self.H, self.W\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # pad feature maps to multiples of window size\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchRecover(nn.Module):\n",
    "    \"\"\" Patch Merging Layer\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(dim, dim // 2, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.GroupNorm(num_channels=dim // 2, num_groups=groups),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.permute(0, 1, 2)  # B ,C, L\n",
    "        x = x.reshape(B, C, H, W)\n",
    "        x = self.up(x)  # B, C//2, H, W\n",
    "\n",
    "        x = x.reshape(B, C // 2, -1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # x = Variable(torch.randn(B, H * 2, W * 2, C // 2))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    \"\"\" Patch Merging Layer\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # padding\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "    Args:\n",
    "        dim (int): Number of feature channels\n",
    "        depth (int): Depths of this stage.\n",
    "        num_heads (int): Number of attention head.\n",
    "        window_size (int): Local window size. Default: 7.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 depth,\n",
    "                 num_heads,\n",
    "                 window_size=7,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 downsample=None,\n",
    "                 use_checkpoint=False,\n",
    "                 up=True):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.up = up\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop,\n",
    "                attn_drop=attn_drop,\n",
    "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, image, H, W):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate attention mask for SW-MSA\n",
    "        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
    "        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
    "        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            blk.H, blk.W = H, W\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, attn_mask)\n",
    "            else:\n",
    "                x = blk(x, image, attn_mask)\n",
    "        if self.downsample is not None:\n",
    "            x_down = self.downsample(x, H, W)\n",
    "            if self.up:\n",
    "                Wh, Ww = (H + 1) // 2, (W + 1) // 2\n",
    "            else:\n",
    "                Wh, Ww = H * 2, W * 2\n",
    "            return x, H, W, x_down, Wh, Ww\n",
    "        else:\n",
    "            return x, H, W, x, H, W\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        # padding\n",
    "        _, _, H, W = x.size()\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "        x = self.proj(x)  # B C Wh Ww\n",
    "        if self.norm is not None:\n",
    "            Wh, Ww = x.size(2), x.size(3)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(embed_dim)\n",
    "        self.maxPool = nn.MaxPool2d(kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        # padding\n",
    "        _, _, H, W = x.size()\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "        x = self.proj(x)  # B C Wh Ww\n",
    "        x = self.bn(x)\n",
    "        x = self.maxPool(x)\n",
    "        if self.norm is not None:\n",
    "            Wh, Ww = x.size(2), x.size(3)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    Up Convolution Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.GroupNorm(num_channels=out_ch, num_groups=groups),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.up = up_conv(in_channels, out_channels)\n",
    "        # self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv_relu = nn.Sequential(\n",
    "            nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\n",
    "            # coorAtt(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # x2 = self.att_block(x1, x2)\n",
    "        x1 = torch.cat((x2, x1), dim=1)\n",
    "        x1 = self.conv_relu(x1)\n",
    "        return x1\n",
    "class SwinUp(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(SwinUp, self).__init__()\n",
    "        self.up = nn.Linear(dim, dim * 2)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.up(x)\n",
    "        x = x.reshape(B, H, W, 2 * C)\n",
    "\n",
    "        x0 = x[:, :, :, 0:C // 2]\n",
    "        x1 = x[:, :, :, C // 2:C]\n",
    "        x2 = x[:, :, :, C:C + C // 2]\n",
    "        x3 = x[:, :, :, C + C // 2:C * 2]\n",
    "\n",
    "        x0 = torch.cat((x0, x1), dim=1)\n",
    "        x3 = torch.cat((x2, x3), dim=1)\n",
    "        x = torch.cat((x0, x3), dim=2)\n",
    "\n",
    "        # x = Variable(torch.randn(B, H * 2, W * 2, C // 2))\n",
    "\n",
    "        x = x.reshape(B, -1, C // 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 patch_size=4,\n",
    "                 depths=2,\n",
    "                 num_heads=6,\n",
    "                 window_size=7,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.2,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 patch_norm=True,\n",
    "                 use_checkpoint=False):\n",
    "        super(SwinDecoder, self).__init__()\n",
    "\n",
    "        self.patch_norm = patch_norm\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depths)]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layer = BasicLayer(\n",
    "            dim=embed_dim // 2,\n",
    "            depth=depths,\n",
    "            num_heads=num_heads,\n",
    "            window_size=window_size,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            drop=drop_rate,\n",
    "            attn_drop=attn_drop_rate,\n",
    "            drop_path=dpr,\n",
    "            norm_layer=norm_layer,\n",
    "            downsample=None,\n",
    "            use_checkpoint=use_checkpoint)\n",
    "\n",
    "        self.up = up_conv(embed_dim, embed_dim // 2)\n",
    "        self.conv_relu = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim // 2, embed_dim // 2, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, text):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "\n",
    "        # print(x.shape)\n",
    "        # for i in range(len(e_o)):\n",
    "        #    layer = self.layers[i]\n",
    "        #    x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n",
    "        # return x\n",
    "\n",
    "        identity = x\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.up(x)  # B , C//2, 2H, 2W\n",
    "        x = x.reshape(B, C // 2, H * W * 4)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # x2 = torch.rand(10, 2, 256).to(x.device)\n",
    "        # xx = torch.cat([x, x2], dim=1)\n",
    "\n",
    "        x_out, H, W, x, Wh, Ww = self.layer(x, text, H * 2, W * 2)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x.reshape(B, C // 2, H, W)\n",
    "        # B, C//4 2H, 2W\n",
    "        x = self.conv_relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Swin_Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, depths, num_heads):\n",
    "        super(Swin_Decoder, self).__init__()\n",
    "        self.up = SwinDecoder(in_channels, depths=depths, num_heads=num_heads)\n",
    "        # self.up1 = nn.Upsample(scale_factor=2)\n",
    "        # self.up2 = nn.Conv2d(in_channels, in_channels//2, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv_relu = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 2, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, text):\n",
    "        x1 = self.up(x1, text)\n",
    "        # x1 = self.up2(x1)\n",
    "        # x2 = self.att_block(x1, x2)\n",
    "        x2 = self.conv2(x2)\n",
    "        x1 = torch.cat((x2, x1), dim=1)\n",
    "        out = self.conv_relu(x1)\n",
    "        return out\n",
    "#dropout = None\n",
    "'''\n",
    "conv_seg = nn.Conv2d(256, 3, kernel_size=1)\n",
    "    def cls_seg(feat):\n",
    "        output = conv_seg(feat)\n",
    "        return output\n",
    "'''\n",
    "\n",
    "#@MODELS.register_module()\n",
    "class CLIPSwinImageDecode(nn.Module):\n",
    "    def __init__(self, channels, output_channels, image_dim=512, **kwargs):\n",
    "        super().__init__()\n",
    "        self.layer1 = Swin_Decoder(channels, 2, 2)\n",
    "        self.layer2 = Swin_Decoder(channels, 2, 2)\n",
    "        self.layer3 = Swin_Decoder(channels, 2, 2)\n",
    "        self.image_dim = image_dim\n",
    "        \n",
    "        # Modified linear layers to handle image embeddings of shape (1, 512)\n",
    "        self.linearA = nn.Sequential(nn.Linear(self.image_dim, 128),\n",
    "                                     nn.LayerNorm(128))\n",
    "        self.linearB = nn.Sequential(nn.Linear(self.image_dim, 128),\n",
    "                                     nn.LayerNorm(128))\n",
    "        \n",
    "        self.output_channels = output_channels\n",
    "        self.conv_seg = nn.Conv2d(channels, out_channels=output_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, inputs, imageA, imageB):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: tuple of 4 images (e1, e2, e3, e4)\n",
    "            imageA: CLIP image embedding of shape (1, 512)\n",
    "            imageB: CLIP image embedding of shape (1, 512)\n",
    "        \"\"\"\n",
    "        e1, e2, e3, e4 = inputs\n",
    "        \n",
    "        # Process image embeddings through linear layers\n",
    "        # imageA and imageB are (1, 512), linear layers expect (batch, features)\n",
    "        processed_imageA = self.linearA(imageA)  # (1, 128)\n",
    "        processed_imageB = self.linearB(imageB)  # (1, 128)\n",
    "        \n",
    "        # Concatenate processed image embeddings\n",
    "        image_features = torch.cat([processed_imageA, processed_imageB], dim=1)  # (1, 256)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        d1 = self.layer1(e4, e3, image_features)\n",
    "        d2 = self.layer2(d1, e2, image_features)\n",
    "        d3 = self.layer3(d2, e1, image_features)\n",
    "        \n",
    "        # Final segmentation output\n",
    "        output = self.conv_seg(d3)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f3c8c",
   "metadata": {},
   "source": [
    "### Make Divisible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b61ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_divisible(value, divisor, min_value=None, min_ratio=0.9):\n",
    "    \"\"\"Make divisible function.\n",
    "\n",
    "    This function rounds the channel number to the nearest value that can be\n",
    "    divisible by the divisor. It is taken from the original tf repo. It ensures\n",
    "    that all layers have a channel number that is divisible by divisor. It can\n",
    "    be seen here: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py  # noqa\n",
    "\n",
    "    Args:\n",
    "        value (int): The original channel number.\n",
    "        divisor (int): The divisor to fully divide the channel number.\n",
    "        min_value (int): The minimum value of the output channel.\n",
    "            Default: None, means that the minimum value equal to the divisor.\n",
    "        min_ratio (float): The minimum ratio of the rounded channel number to\n",
    "            the original channel number. Default: 0.9.\n",
    "\n",
    "    Returns:\n",
    "        int: The modified output channel number.\n",
    "    \"\"\"\n",
    "\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_value = max(min_value, int(value + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than (1-min_ratio).\n",
    "    if new_value < min_ratio * value:\n",
    "        new_value += divisor\n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f922b71d",
   "metadata": {},
   "source": [
    "### SELayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b044cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from .make_divisible import make_divisible\n",
    "\n",
    "\n",
    "class SELayer_v2(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation Module.\n",
    "\n",
    "    Args:\n",
    "        channels (int): The input (and output) channels of the SE layer.\n",
    "        ratio (int): Squeeze ratio in SELayer, the intermediate channel will be\n",
    "            ``int(channels/ratio)``. Default: 16.\n",
    "        conv_cfg (None or dict): Config dict for convolution layer.\n",
    "            Default: None, which means using conv2d.\n",
    "        act_cfg (dict or Sequence[dict]): Config dict for activation layer.\n",
    "            If act_cfg is a dict, two activation layers will be configured\n",
    "            by this dict. If act_cfg is a sequence of dicts, the first\n",
    "            activation layer will be configured by the first dict and the\n",
    "            second activation layer will be configured by the second dict.\n",
    "            Default: (dict(type='ReLU'), dict(type='HSigmoid', bias=3.0,\n",
    "            divisor=6.0)).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 channels,\n",
    "                 out_channels=None,\n",
    "                 ratio=16,\n",
    "                 conv_cfg=None,\n",
    "                 act_cfg=(dict(type='ReLU'),\n",
    "                          dict(type='HSigmoid', bias=3.0, divisor=6.0))):\n",
    "        super(SELayer_v2, self).__init__()\n",
    "        if isinstance(act_cfg, dict):\n",
    "            act_cfg = (act_cfg, act_cfg)\n",
    "        assert len(act_cfg) == 2\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.out_channels = out_channels\n",
    "        self.conv1 = ConvModule(\n",
    "            in_channels=channels,\n",
    "            out_channels=make_divisible(channels // ratio, 8),\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            conv_cfg=conv_cfg,\n",
    "            act_cfg=act_cfg[0])\n",
    "        self.conv2 = ConvModule(\n",
    "            in_channels=make_divisible(channels // ratio, 8),\n",
    "            out_channels=channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            conv_cfg=conv_cfg,\n",
    "            act_cfg=act_cfg[1])\n",
    "        self.conv_last = ConvModule(\n",
    "            in_channels=channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            conv_cfg=conv_cfg,\n",
    "            act_cfg=act_cfg[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.global_avgpool(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        if self.out_channels is not None:\n",
    "            out = x * out\n",
    "            out = self.conv_last(out)\n",
    "            return out\n",
    "        else:\n",
    "            return x * out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e72206",
   "metadata": {},
   "source": [
    "## Novel Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, network,device='cpu',width=64,minus_channel = [256, 512, 1024, 2048],output_channels=3):\n",
    "        super(Model, self).__init__()\n",
    "        self.minus_channel = minus_channel\n",
    "        self.minus_conv = nn.Sequential(ConvModule(\n",
    "                    in_channels=self.minus_channel[0],\n",
    "                    out_channels=256,\n",
    "                    kernel_size=1),\n",
    "                    ConvModule(\n",
    "                    in_channels=self.minus_channel[1],\n",
    "                    out_channels=256,\n",
    "                    kernel_size=1),\n",
    "                    ConvModule(\n",
    "                    in_channels=self.minus_channel[2],\n",
    "                    out_channels=256,\n",
    "                    kernel_size=1),\n",
    "                    ConvModule(\n",
    "                    in_channels=self.minus_channel[3],\n",
    "                    out_channels=256,\n",
    "                    kernel_size=1)\n",
    "                    )\n",
    "        \n",
    "        self.neck = FPN(in_channels=[512,1024,2048,4096],out_channels=256,num_outs=4)\n",
    "        self.network = network\n",
    "        \n",
    "        self.feature_extractor = CLIPResNetWithAttention([1,1,1,1],get_embeddings=True)\n",
    "        \n",
    "        clip_model_type = self.network.replace(\"CLIP-\", \"\")\n",
    "        self.clip_model, self.preprocess = clip.load(clip_model_type,device=device)  #\n",
    "        self.clip_model = self.clip_model.to(device=device,dtype=torch.float32)\n",
    "        self.attentive_encoder = ModifiedAttentiveEncoder(n_layers=3,\n",
    "                                        feature_size=[7, 7, 768],\n",
    "                                        heads=8, dropout=0.1).cuda()\n",
    "\n",
    "        \n",
    "        self.score_concat_index = 3\n",
    "        self.with_neck = True\n",
    "        self.decode_head = CLIPSwinImageDecode(channels=256,output_channels=output_channels,image_dim=512)\n",
    "        self.channel_att = nn.Sequential(SELayer_v2(768, 256), SELayer_v2(768, 256), SELayer_v2(768, 256), SELayer_v2(768, 256))\n",
    "        \n",
    "        self.text_decoder = DecoderTransformer(decoder_type='transformer_decoder',embed_dim=768,\n",
    "                                    vocab_size=len(word_vocab), max_lengths=42,\n",
    "                                    word_vocab=word_vocab, n_head=8,\n",
    "                                    n_layers=1, dropout=0.1,device=device)\n",
    "        \n",
    "    def predict_by_feat(self,seg_logits,\n",
    "                        batch_img_metas):\n",
    "        \"\"\"Transform a batch of output seg_logits to the input shape.\n",
    "\n",
    "        Args:\n",
    "            seg_logits (Tensor): The output from decode head forward function.\n",
    "            batch_img_metas (list[dict]): Meta information of each image, e.g.,\n",
    "                image size, scaling factor, etc.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Outputs segmentation logits map.\n",
    "        \"\"\"\n",
    "\n",
    "        seg_logits = resize(\n",
    "            input=seg_logits,\n",
    "            size=batch_img_metas[0]['img_shape'],\n",
    "            mode='bilinear',\n",
    "            align_corners=False)\n",
    "        return seg_logits\n",
    "    def manual_preprocess(self,images, size=224):\n",
    "        \"\"\"\n",
    "        Manual preprocessing function that replicates:\n",
    "        Resize(bicubic) -> CenterCrop -> ToTensor -> Normalize\n",
    "        (without RGB conversion)\n",
    "        \n",
    "        Args:\n",
    "            images: Single image (PIL Image, tensor, numpy array) or batch of images\n",
    "                - For batch: list of images or 4D tensor (B, C, H, W)\n",
    "            size: Target size for resize and crop (default: 224)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Preprocessed image tensor(s)\n",
    "                        - Single image: (C, H, W)\n",
    "                        - Batch: (B, C, H, W)\n",
    "        \"\"\"\n",
    "        \n",
    "        def process_single_image(img):\n",
    "            # Convert to PIL Image if it's a tensor or numpy array\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                if img.dim() == 3:  # Single image tensor (C, H, W)\n",
    "                    img = TF.to_pil_image(img)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected tensor dimensions: {img.dim()}\")\n",
    "            elif isinstance(img, np.ndarray):\n",
    "                img = Image.fromarray(img)\n",
    "            \n",
    "            # 1. Resize with bicubic interpolation\n",
    "            w, h = img.size\n",
    "            if w < h:\n",
    "                new_w = size\n",
    "                new_h = int(size * h / w)\n",
    "            else:\n",
    "                new_h = size\n",
    "                new_w = int(size * w / h)\n",
    "            \n",
    "            img = img.resize((new_w, new_h), Image.BICUBIC)\n",
    "            \n",
    "            # 2. Center Crop\n",
    "            img = TF.center_crop(img, (size, size))\n",
    "            \n",
    "            # 3. Convert to Tensor\n",
    "            tensor = TF.to_tensor(img)\n",
    "            \n",
    "            # 4. Normalize\n",
    "            mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3, 1, 1)\n",
    "            std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3, 1, 1)\n",
    "            tensor = (tensor - mean) / std\n",
    "            \n",
    "            return tensor\n",
    "        \n",
    "        # Handle different input types\n",
    "        if isinstance(images, (list, tuple)):\n",
    "            # Batch of images as list/tuple\n",
    "            processed_batch = []\n",
    "            for img in images:\n",
    "                processed_batch.append(process_single_image(img))\n",
    "            return torch.stack(processed_batch)\n",
    "        \n",
    "        elif isinstance(images, torch.Tensor) and images.dim() == 4:\n",
    "            # Batch tensor (B, C, H, W)\n",
    "            batch_size = images.shape[0]\n",
    "            processed_batch = []\n",
    "            for i in range(batch_size):\n",
    "                single_img = images[i]\n",
    "                processed_batch.append(process_single_image(single_img))\n",
    "            return torch.stack(processed_batch)\n",
    "        \n",
    "        elif isinstance(images, np.ndarray) and images.ndim == 4:\n",
    "            # Batch numpy array (B, H, W, C) or (B, C, H, W)\n",
    "            batch_size = images.shape[0]\n",
    "            processed_batch = []\n",
    "            for i in range(batch_size):\n",
    "                single_img = images[i]\n",
    "                processed_batch.append(process_single_image(single_img))\n",
    "            return torch.stack(processed_batch)\n",
    "        \n",
    "        else:\n",
    "            # Single image\n",
    "            return process_single_image(images)\n",
    "    \n",
    "    def forward(self,imageA, imageB,mode='Train'):\n",
    "        \n",
    "        # Encoder\n",
    "        x_A,x_B = self.feature_extractor(imageA),self.feature_extractor(imageB) #CLIPResnetwithAttention\n",
    "        x_clipA = list(x_A[0:4])\n",
    "        x_clipB = list(x_B[0:4])\n",
    "        \n",
    "        img_A = imageA.to(dtype=torch.float32)\n",
    "        img_B = imageB.to(dtype=torch.float32)\n",
    "        clip_emb_A, img_feat_A = self.clip_model.encode_image(self.manual_preprocess(img_A).to(device)) #CLIP Pretrained encoder\n",
    "        clip_emb_B, img_feat_B = self.clip_model.encode_image(self.manual_preprocess(img_B).to(device)) #CLIP Pretrained encoder\n",
    "        \n",
    "        # FPN Functions\n",
    "        x_orig = [torch.cat([x_clipA[i], x_clipB[i]], dim=1) for i in range(len(x_clipA))]\n",
    "        x_minus = [self.minus_conv[i](torch.abs(x_clipA[i]-x_clipB[i])) for i in range(len(x_clipA))]\n",
    "        x_diff = [F.sigmoid(1-torch.cosine_similarity(x_clipA[i], x_clipB[i], dim=1)).unsqueeze(1) for i in range(len(x_clipA))]\n",
    "        \n",
    "        if self.with_neck:\n",
    "            x_orig = list(self.neck(x_orig)) # Feature Pyramid Network \n",
    "        \n",
    "        x = x_orig\n",
    "        \n",
    "        x = [torch.cat([x[i]*x_diff[i], x_minus[i], x[i]], dim=1) for i in range(len(x))]\n",
    "\n",
    "        x = [self.channel_att[i](x[i]) for i in range(len(x))] # Channel attention\n",
    "        \n",
    "        clip_emb_A,clip_emb_B = clip_emb_A.unsqueeze(0).permute(1,0,2), clip_emb_B.unsqueeze(0).permute(1,0,2)\n",
    "        seg_logits = self.decode_head.forward(x,clip_emb_A,clip_emb_B) # SwinTextDecoder\n",
    "\n",
    "        data_samples = [{'img_shape': (256, 256)}]\n",
    "        output = self.predict_by_feat(seg_logits,data_samples) \n",
    "\n",
    "        # Decoding Text\n",
    "        out_put_embeddings,output_features = self.clip_model.encode_image(self.manual_preprocess(output).to(device))\n",
    "        featcap = self.attentive_encoder(img_feat_A,img_feat_B,output_features)\n",
    "        \n",
    "        if mode == 'Train':\n",
    "            scores, caps_sorted, decode_lengths, sort_ind = self.text_decoder.forward(featcap)\n",
    "            return output,scores, caps_sorted, decode_lengths, sort_ind\n",
    "        else:\n",
    "            captions = []\n",
    "            #return featcap\n",
    "            if len(featcap) == 1:\n",
    "                return self.text_decoder.sample(featcap.unsqueeze(0),k=1)\n",
    "            else:\n",
    "                for i in range(len(featcap)):\n",
    "                    #return individual_featcap\n",
    "                    captions.append(self.text_decoder.sample(featcap[i].unsqueeze(0),k=1))\n",
    "\n",
    "            return output,captions\n",
    "        # Generating Text decodings\n",
    "        #text_feat = self.text_decoder(feat_cap)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e75d582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_n_layers= 1\n",
      "decoder_type= transformer_decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2230994/3398444730.py:35: UserWarning: In MMCV v1.4.4, we modified the default value of args to align with PyTorch official. Previous Implementation: Hsigmoid(x) = min(max((x + 1) / 2, 0), 1). Current Implementation: Hsigmoid(x) = min(max((x + 3) / 6, 0), 1).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Model(device=device,network='CLIP-ViT-B/32').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10fc3620",
   "metadata": {},
   "outputs": [],
   "source": [
    "ima = torch.rand(2,3,256,256).to(device)\n",
    "imb = torch.rand(2,3,256,256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e00917c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "k[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81da4d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "k= model.forward(ima,imb,mode='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ffce5c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got int"
     ]
    }
   ],
   "source": [
    "torch.cat(k[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00eac610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 49, 768])\n",
      "torch.Size([1, 49, 768])\n",
      "torch.Size([1, 49, 768])\n"
     ]
    }
   ],
   "source": [
    "for im in k:\n",
    "    print(im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "342c60b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 768, 49])\n"
     ]
    }
   ],
   "source": [
    "for im in k:\n",
    "    print(im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "89c80967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2160000/3398444730.py:35: UserWarning: In MMCV v1.4.4, we modified the default value of args to align with PyTorch official. Previous Implementation: Hsigmoid(x) = min(max((x + 1) / 2, 0), 1). Current Implementation: Hsigmoid(x) = min(max((x + 3) / 6, 0), 1).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "channel_att = nn.Sequential(SELayer_v2(768, 256), SELayer_v2(768, 256), SELayer_v2(768, 256), SELayer_v2(768, 256)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4f267",
   "metadata": {},
   "source": [
    "# Training Loop of Modified Levir CD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20908ef7",
   "metadata": {},
   "source": [
    "1) Make a dataloader which gives tokens and segmentation masks - modify the existing dataloader to display segmentation masks as well  \n",
    "2) Make a training loop and start running it  \n",
    "3) Try to incorporate lightining to it to make it faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e7a722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24e7ad7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m      2\u001b[0m                 LEVIRCCDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLIP-ViT-B/32\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/Levir-CC-dataset/images\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/guest/Documents/Siraj TM/RSCaMa/data/LEVIR_CC/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/guest/Documents/Siraj TM/RSCaMa/data/LEVIR_CC/tokens/\u001b[39m\u001b[38;5;124m'\u001b[39m, word_vocab, \u001b[38;5;241m42\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      3\u001b[0m                 batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = data.DataLoader(\n",
    "                LEVIRCCDataset('CLIP-ViT-B/32','data/Levir-CC-dataset/images', '/home/guest/Documents/Siraj TM/RSCaMa/data/LEVIR_CC/', 'train', '/home/guest/Documents/Siraj TM/RSCaMa/data/LEVIR_CC/tokens/', word_vocab, 42, 1),\n",
    "                batch_size=64, shuffle=True, num_workers=24, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b60791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = data.DataLoader(\n",
    "                LEVIRCCDataset('CLIP-ViT-B/32','data/Levir-CC-dataset/images', '/home/guest/Documents/Siraj TM/RSCaMa/data/LEVIR_CC/', 'val', '/home/guest/Documents/Siraj TM/RSCaMa/data/LEVIR_CC/tokens/', word_vocab , 42, 1),\n",
    "                batch_size=64, shuffle=False, num_workers=24, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ba6489b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m,batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_loader\u001b[49m):\n\u001b[1;32m      7\u001b[0m         start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      8\u001b[0m         accum_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m64\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print_freq = 100\n",
    "EPOCHS = 25\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for id,batch_data in enumerate(train_loader):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        accum_steps = 64//64\n",
    "        \n",
    "        #Getting Data and moving to GPU if possible\n",
    "        imgA = batch_data['imgA']\n",
    "        imgB = batch_data['imgB']\n",
    "        token = batch_data['token']\n",
    "        token_len = batch_data['token_len']\n",
    "        break\n",
    "        imgA = imgA.cuda(device)\n",
    "        imgB = imgB.cuda(device)\n",
    "        token = token.cuda(device)\n",
    "        token_len = token_len.cuda(device)\n",
    "        \n",
    "        #Feat1 and Feat2\n",
    "        feat1, feat2 = encoder(imgA, imgB)\n",
    "        feat = encoder_trans(feat1, feat2)\n",
    "        break\n",
    "        scores, caps_sorted, decode_lengths, sort_ind = decoder(feat, token, token_len)\n",
    "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "        targets = caps_sorted[:, 1:]\n",
    "        \n",
    "        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n",
    "        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
    "        \n",
    "        loss = criterion_cap(scores, targets.to(torch.int64))\n",
    "        \n",
    "        loss = loss / accum_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if (id + 1) % accum_steps == 0 or (id + 1) == len(train_loader):\n",
    "            decoder_optimizer.step()\n",
    "            encoder_trans_optimizer.step()\n",
    "            if encoder_optimizer is not None:\n",
    "                encoder_optimizer.step()\n",
    "\n",
    "            # Adjust learning rate\n",
    "            decoder_lr_scheduler.step()\n",
    "            encoder_trans_lr_scheduler.step()\n",
    "            if encoder_lr_scheduler is not None:\n",
    "                encoder_lr_scheduler.step()\n",
    "\n",
    "            decoder_optimizer.zero_grad()\n",
    "            encoder_trans_optimizer.zero_grad()\n",
    "            if encoder_optimizer is not None:\n",
    "                encoder_optimizer.zero_grad()\n",
    "                \n",
    "        hist[index_i, 0] = time.time() - start_time #batch_time\n",
    "        hist[index_i, 1] = loss.item()  # train_loss\n",
    "        hist[index_i, 2] = accuracy_v0(scores, targets, 5) #top5\n",
    "        \n",
    "        index_i += 1\n",
    "        \n",
    "        if index_i % (print_freq) == 0:\n",
    "            print(f'Training Epoch: {epoch} | Index:{index_i} | Loss: {loss}\\n')\n",
    "        rem_print(f'Training Epoch: {epoch} | Index:{index_i} | Loss: {loss}')\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Siraj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
