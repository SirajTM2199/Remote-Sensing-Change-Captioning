{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97c741c7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fd4a47",
   "metadata": {},
   "source": [
    "Crucial packages are imported here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c042d",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e05d1a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/anaconda3/envs/Siraj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from clip import clip\n",
    "import sys\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce5fba",
   "metadata": {},
   "source": [
    " The device to use is set here, by default it is 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e76bfe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85c89e",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab18c82",
   "metadata": {},
   "source": [
    "These are functions to automate common tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d80aa48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):  # Loads json files\n",
    "    with open(path) as f:\n",
    "        file = json.load(f)\n",
    "    f.close()\n",
    "    return file\n",
    "def save_json(file,path): # Saves json files\n",
    "    with open(path,'w') as f:\n",
    "        json.dump(file,f)\n",
    "    f.close()\n",
    "    print(\"Saved Successfully\")\n",
    "def rem_print(word):  #Allows for printed statements to be overwritten\n",
    "    t_word = word\n",
    "    for _ in range(250 - len(t_word)):\n",
    "        word = word + ' '\n",
    "    print(word,end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca010b99",
   "metadata": {},
   "source": [
    "# Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d60249",
   "metadata": {},
   "source": [
    "This section deals with tasks involving captioning only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46805190",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "489067d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/anaconda3/envs/Siraj/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import time\n",
    "sys.path.append('/home/guest/Documents/Siraj TM/RSCaMa')\n",
    "from utils_tool.utils import *\n",
    "sys.path.append('/home/guest/Documents/Siraj TM/RSCaMa')\n",
    "from model.model_decoder import DecoderTransformer\n",
    "from model.model_encoder_attMamba import EnhancedEncoder\n",
    "from model.model_encoder_attMamba import CrossAttentiveEncoder\n",
    "import torchvision.transforms.functional as TF\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.layers import drop_path, trunc_normal_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db222d",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "775801de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_preprocess(images, size=224):\n",
    "    \"\"\"\n",
    "    Manual preprocessing function that replicates:\n",
    "    Resize(bicubic) -> CenterCrop -> ToTensor -> Normalize\n",
    "    (without RGB conversion)\n",
    "    \n",
    "    Args:\n",
    "        images: Single image (PIL Image, tensor, numpy array) or batch of images\n",
    "            - For batch: list of images or 4D tensor (B, C, H, W)\n",
    "        size: Target size for resize and crop (default: 224)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed image tensor(s)\n",
    "                    - Single image: (C, H, W)\n",
    "                    - Batch: (B, C, H, W)\n",
    "    \"\"\"\n",
    "    \n",
    "    def process_single_image(img):\n",
    "        # Convert to PIL Image if it's a tensor or numpy array\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            if img.dim() == 3:  # Single image tensor (C, H, W)\n",
    "                img = TF.to_pil_image(img)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected tensor dimensions: {img.dim()}\")\n",
    "        elif isinstance(img, np.ndarray):\n",
    "            img = Image.fromarray(img)\n",
    "        \n",
    "        # 1. Resize with bicubic interpolation\n",
    "        w, h = img.size\n",
    "        if w < h:\n",
    "            new_w = size\n",
    "            new_h = int(size * h / w)\n",
    "        else:\n",
    "            new_h = size\n",
    "            new_w = int(size * w / h)\n",
    "        \n",
    "        img = img.resize((new_w, new_h), Image.BICUBIC)\n",
    "        \n",
    "        # 2. Center Crop\n",
    "        img = TF.center_crop(img, (size, size))\n",
    "        \n",
    "        # 3. Convert to Tensor\n",
    "        tensor = TF.to_tensor(img)\n",
    "        \n",
    "        # 4. Normalize\n",
    "        mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3, 1, 1)\n",
    "        tensor = (tensor - mean) / std\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "    # Handle different input types\n",
    "    if isinstance(images, (list, tuple)):\n",
    "        # Batch of images as list/tuple\n",
    "        processed_batch = []\n",
    "        for img in images:\n",
    "            processed_batch.append(process_single_image(img))\n",
    "        return torch.stack(processed_batch)\n",
    "    \n",
    "    elif isinstance(images, torch.Tensor) and images.dim() == 4:\n",
    "        # Batch tensor (B, C, H, W)\n",
    "        batch_size = images.shape[0]\n",
    "        processed_batch = []\n",
    "        for i in range(batch_size):\n",
    "            single_img = images[i]\n",
    "            processed_batch.append(process_single_image(single_img))\n",
    "        return torch.stack(processed_batch)\n",
    "    \n",
    "    elif isinstance(images, np.ndarray) and images.ndim == 4:\n",
    "        # Batch numpy array (B, H, W, C) or (B, C, H, W)\n",
    "        batch_size = images.shape[0]\n",
    "        processed_batch = []\n",
    "        for i in range(batch_size):\n",
    "            single_img = images[i]\n",
    "            processed_batch.append(process_single_image(single_img))\n",
    "        return torch.stack(processed_batch)\n",
    "    \n",
    "    else:\n",
    "        # Single image\n",
    "        return process_single_image(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef27b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imread\n",
    "def get_image(path,preprocess):\n",
    "    \"\"\"\n",
    "    Load an image from the given path and preprocess it for CLIP.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the image file.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed image tensor.\n",
    "    \"\"\"\n",
    "    imagesA = []\n",
    "    imagesB = []\n",
    "    imagesegs = []\n",
    "    \n",
    "    if isinstance(path, str):\n",
    "        path = [path]\n",
    "    elif isinstance(path, list):\n",
    "        path = path\n",
    "    if len(path) == 1:\n",
    "        path = path[0]\n",
    "    \n",
    "        split = path.split('_')[0]\n",
    "        imgA = f'data/LEVIR-MCI-dataset/images/{split}/A/{path}'\n",
    "        imgB = f'data/LEVIR-MCI-dataset/images/{split}/B/{path}'\n",
    "        seg_label = f'data/LEVIR-MCI-dataset/images/{split}/label/{path}'\n",
    "        \n",
    "\n",
    "        \n",
    "        imageA = Image.open(imgA).convert('RGB')\n",
    "        imageB = Image.open(imgB).convert('RGB')\n",
    "        imageseg = np.asarray(Image.open(seg_label))\n",
    "    \n",
    "        seg_output = manual_preprocess((torch.tensor(np.array(imageseg)) / 128 > 0).float().permute(2, 0, 1)) > 0\n",
    "        return preprocess(imageA),preprocess(imageB), seg_output[:1,:,:] # Add batch dimension\n",
    "    else:\n",
    "        for i in range(len(path)):\n",
    "            split = path[i].split('_')[0]\n",
    "            imgA = f'data/LEVIR-MCI-dataset/images/{split}/A/{path[i]}'\n",
    "            imgB = f'data/LEVIR-MCI-dataset/images/{split}/B/{path[i]}'\n",
    "            seg_label = f'data/LEVIR-MCI-dataset/images/{split}/label/{path[i]}'\n",
    "            \n",
    "            imageA = Image.open(imgA).convert('RGB')\n",
    "            imageB = Image.open(imgB).convert('RGB')\n",
    "            imageseg = np.asarray(Image.open(seg_label))\n",
    "            \n",
    "            imagesA.append(preprocess(imageA).unsqueeze(0))\n",
    "            imagesB.append(preprocess(imageB).unsqueeze(0))\n",
    "            imagesegs.append(imageseg)  # Convert to binary mask (0 or 1)\n",
    "        \n",
    "        \n",
    "        seg_output = (manual_preprocess((torch.tensor(imagesegs) / 128 > 0).float().permute(0, 3, 1, 2)) > 0).float()\n",
    "        \n",
    "        return torch.cat(imagesA, dim=0),torch.cat(imagesB, dim=0 ),  seg_output[:,:1,:,:]#(manual_preprocess((torch.tensor( np.array(imagesegs))/128 > 0).float().unsqueeze(0).permute(0, 3, 1, 2)) > 0).float()  # Convert to binary mask (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ead9d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dc377b",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df19e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from preprocess_data import encode\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "#import cv2 as cv\n",
    "from imageio import imread\n",
    "from random import *\n",
    "class LEVIRMCIDataset_Modified(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_folder, list_path, split, preprocess=False,token_folder = None, vocab_file = None, max_length = 41, allow_unk = 0, max_iters=None):\n",
    "        \"\"\"\n",
    "        :param data_folder: folder where image files are stored\n",
    "        :param list_path: folder where the file name-lists of Train/val/test.txt sets are stored\n",
    "        :param split: split, one of 'TRAIN', 'VAL', or 'TEST'\n",
    "        :param token_folder: folder where token files are stored\n",
    "        :param vocab_file: the name of vocab file\n",
    "        :param max_length: the maximum length of each caption sentence\n",
    "        :param max_iters: the maximum iteration when loading the data\n",
    "        :param allow_unk: whether to allow the tokens have unknow word or not\n",
    "        \"\"\"\n",
    "        self.mean = [0.39073*255,  0.38623*255, 0.32989*255]\n",
    "        self.std = [0.15329*255,  0.14628*255, 0.13648*255]\n",
    "        self.list_path = list_path\n",
    "        self.split = split\n",
    "        self.max_length = max_length\n",
    "        self.preprocess = preprocess\n",
    "        \n",
    "        assert self.split in {'train', 'val', 'test'}\n",
    "        self.img_ids = [i_id.strip() for i_id in open(os.path.join(list_path + split + '.txt'))]\n",
    "        if vocab_file is not None:\n",
    "            with open(os.path.join(list_path + vocab_file + '.json'), 'r') as f:\n",
    "                self.word_vocab = json.load(f)\n",
    "            self.allow_unk = allow_unk\n",
    "        if not max_iters==None:\n",
    "            n_repeat = int(np.ceil(max_iters / len(self.img_ids)))\n",
    "            self.img_ids = self.img_ids * n_repeat + self.img_ids[:max_iters-n_repeat*len(self.img_ids)]\n",
    "        self.files = []\n",
    "        \n",
    "        \n",
    "        if split =='train':\n",
    "            for name in self.img_ids:\n",
    "                img_fileA = os.path.join(data_folder + '/' + split +'/A/' + name.split('-')[0])\n",
    "                img_fileB = img_fileA.replace('A', 'B')\n",
    "                #print(self.preprocess)\n",
    "                #print(self.max_length)\n",
    "                if self.preprocess:\n",
    "                    imgA,imgB,seg_label = get_image(name,preprocess=preprocess)\n",
    "                    #print(\"Successfully loaded image with preprocess\")\n",
    "                else:\n",
    "                    imgA = imread(img_fileA)\n",
    "                    imgB = imread(img_fileB)\n",
    "                    seg_label = imread(img_fileA.replace('A', 'label'))\n",
    "                    #print(\"Failure\")\n",
    "\n",
    "                if '-' in name:\n",
    "                    token_id = name.split('-')[-1]\n",
    "                else:\n",
    "                    token_id = None\n",
    "                if token_folder is not None:\n",
    "                    token_file = os.path.join(token_folder + name.split('.')[0] + '.txt')\n",
    "                else:\n",
    "                    token_file = None\n",
    "                self.files.append({\n",
    "                    \"imgA\": imgA,\n",
    "                    \"imgB\": imgB,\n",
    "                    \"seg_label\": seg_label,\n",
    "                    \"token\": token_file,\n",
    "                    \"token_id\": token_id,\n",
    "                    \"name\": name.split('-')[0]\n",
    "                })\n",
    "        elif split =='val':\n",
    "            for name in self.img_ids:\n",
    "                img_fileA = os.path.join(data_folder + '/' + split +'/A/' + name)\n",
    "                img_fileB = img_fileA.replace('A', 'B')\n",
    "                \n",
    "                if self.preprocess:\n",
    "                    imgA,imgB,seg_label = get_image(name,preprocess=preprocess)\n",
    "                else:\n",
    "                    imgA = imread(img_fileA)\n",
    "                    imgB = imread(img_fileB)\n",
    "                    seg_label = imread(img_fileA.replace('A', 'label'))\n",
    "                    \n",
    "                token_id = None\n",
    "                if token_folder is not None:\n",
    "                    token_file = os.path.join(token_folder + name.split('.')[0] + '.txt')\n",
    "                else:\n",
    "                    token_file = None\n",
    "                self.files.append({\n",
    "                    \"imgA\": imgA,\n",
    "                    \"imgB\": imgB,\n",
    "                    \"seg_label\": seg_label,\n",
    "                    \"token\": token_file,\n",
    "                    \"token_id\": token_id,\n",
    "                    \"name\": name\n",
    "                })\n",
    "        elif split =='test':\n",
    "            for name in self.img_ids:\n",
    "                img_fileA = os.path.join(data_folder + '/' + split +'/A/' + name)\n",
    "                img_fileB = img_fileA.replace('A', 'B')\n",
    "\n",
    "                if self.preprocess:\n",
    "                    imgA,imgB,seg_label = get_image(name,preprocess=preprocess)\n",
    "                else:\n",
    "                    imgA = imread(img_fileA)\n",
    "                    imgB = imread(img_fileB)\n",
    "                    seg_label = imread(img_fileA.replace('A', 'label'))\n",
    "\n",
    "                token_id = None\n",
    "                if token_folder is not None:\n",
    "                    token_file = os.path.join(token_folder + name.split('.')[0] + '.txt')\n",
    "                else:\n",
    "                    token_file = None\n",
    "                self.files.append({\n",
    "                    \"imgA\": imgA,\n",
    "                    \"imgB\": imgB,\n",
    "                    \"seg_label\": seg_label,\n",
    "                    \"token\": token_file,\n",
    "                    \"token_id\": token_id,\n",
    "                    \"name\": name\n",
    "                })\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        datafiles = self.files[index]\n",
    "        name = datafiles[\"name\"]\n",
    "\n",
    "        imgA = datafiles[\"imgA\"]\n",
    "        imgB = datafiles[\"imgB\"]\n",
    "        seg_label = datafiles[\"seg_label\"]\n",
    "        \n",
    "        if not self.preprocess:\n",
    "            imgA = np.asarray(imgA, np.float32)\n",
    "            imgB = np.asarray(imgB, np.float32)\n",
    "            imgA = imgA.transpose(2, 0, 1)\n",
    "            imgB = imgB.transpose(2, 0, 1)\n",
    "            seg_label = seg_label.transpose(2, 0, 1)[0]\n",
    "            seg_label[seg_label==255] = 2\n",
    "            seg_label[seg_label==128] = 1\n",
    "\n",
    "\n",
    "            for i in range(len(self.mean)):\n",
    "                imgA[i,:,:] -= self.mean[i]\n",
    "                imgA[i,:,:] /= self.std[i]\n",
    "                imgB[i,:,:] -= self.mean[i]\n",
    "                imgB[i,:,:] /= self.std[i]      \n",
    "                \n",
    "        if datafiles[\"token\"] is not None:\n",
    "            caption = open(datafiles[\"token\"])\n",
    "            caption = caption.read()\n",
    "            caption_list = json.loads(caption)\n",
    "\n",
    "            #token = np.zeros((1, self.max_length), dtype=int)\n",
    "            #j = randint(0, len(caption_list) - 1)\n",
    "            #tokens_encode = encode(caption_list[j], self.word_vocab,\n",
    "            #            allow_unk=self.allow_unk == 1)\n",
    "            #token[0, :len(tokens_encode)] = tokens_encode\n",
    "            #token_len = len(tokens_encode)\n",
    "\n",
    "            token_all = np.zeros((len(caption_list),self.max_length),dtype=int)\n",
    "            token_all_len = np.zeros((len(caption_list),1),dtype=int)\n",
    "            for j, tokens in enumerate(caption_list):\n",
    "                nochange_cap = ['<START>', 'the', 'scene', 'is', 'the', 'same', 'as', 'before', '<END>']\n",
    "                if self.split == 'train' and nochange_cap in caption_list:\n",
    "                    tokens = nochange_cap\n",
    "                tokens_encode = encode(tokens, self.word_vocab,\n",
    "                                    allow_unk=self.allow_unk == 1)\n",
    "                token_all[j,:len(tokens_encode)] = tokens_encode\n",
    "                token_all_len[j] = len(tokens_encode)\n",
    "            if datafiles[\"token_id\"] is not None:\n",
    "                id = int(datafiles[\"token_id\"])\n",
    "                token = token_all[id]\n",
    "                token_len = token_all_len[id].item()\n",
    "            else:\n",
    "                j = randint(0, len(caption_list) - 1)\n",
    "                token = token_all[j]\n",
    "                token_len = token_all_len[j].item()\n",
    "        else:\n",
    "            token_all = np.zeros(1, dtype=int)\n",
    "            token = np.zeros(1, dtype=int)\n",
    "            token_len = np.zeros(1, dtype=int)\n",
    "            token_all_len = np.zeros(1, dtype=int)\n",
    "        #print(imgA.shape)\n",
    "        return imgA, imgB, seg_label, token_all.copy(), token_all_len.copy(), token.copy(), np.array(token_len), name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d58cc09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics_over_epochs(metrics_dict):\n",
    "    \"\"\"\n",
    "    Plots metric values over epochs.\n",
    "\n",
    "    Args:\n",
    "        metrics_dict (dict): Dictionary with epochs as keys and each value is a dict with keys:\n",
    "            'Bleu_1', 'Bleu_2', 'Bleu_3', 'Bleu_4', 'Rouge', 'Cider', 'test_time'\n",
    "    \"\"\"\n",
    "    # Sort epochs\n",
    "    epochs = sorted(metrics_dict.keys())\n",
    "    metrics = ['Bleu_1', 'Bleu_2', 'Bleu_3', 'Bleu_4' ,'test_time']\n",
    "    values = {m: [metrics_dict[e][m] for e in epochs] for m in metrics}\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for m in metrics:\n",
    "        if m != 'test_time':\n",
    "            plt.plot(epochs, np.array(values[m]) * 100, marker='o', label=m)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score (%)')\n",
    "    plt.title('Evaluation Metrics over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Optionally, plot test_time separately\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(epochs, values['test_time'], marker='o', color='gray', label='test_time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Test Time (s)')\n",
    "    plt.title('Test Time over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b46fe6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_validation(encoder,encoder_trans,decoder,dataloader):\n",
    "    test_start_time = time.time()\n",
    "    references = list()  # references (true captions) for calculating BLEU-4 score\n",
    "    hypotheses = list()  # hypotheses (predictions)\n",
    "\n",
    "    print(\"Validation.....\\n\")\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for idx,batch_data in enumerate(dataloader):\n",
    "            imgA, imgB, seg_label, token_all, token_all_len, token, token_len, name = batch_data\n",
    "            \n",
    "            #Getting Data and moving to GPU if possible\n",
    "            imgA = imgA.cuda(device)\n",
    "            imgB = imgB.cuda(device)\n",
    "            #imgSM = imgSM.cuda(device)\n",
    "            token = token.cuda(device)\n",
    "            token_len = token_len.cuda(device)\n",
    "            \n",
    "            #Texts = get_text_inputs(name,split='test')\n",
    "            \n",
    "            \n",
    "            \n",
    "            #imgSM = ChangeCLIP.forward(xA=imgA,xB=imgB,Texts=Texts)\n",
    "            \n",
    "            #imgA, imgB, seg_label = Image_store_test[idx]\n",
    "\n",
    "            feat1,feat2 = encoder(\n",
    "                imgA.to(device),\n",
    "                imgB.to(device),\n",
    "                (seg_label > 0).int().to(device)\n",
    "                )\n",
    "\n",
    "            #eat1,feat2,feat3 = encoder(imgA.to(device),imgB.to(device),seg_label.to(device))\n",
    "            feat = encoder_trans(feat1,feat2)\n",
    "\n",
    "\n",
    "            seq = decoder.sample(feat,k=1)\n",
    "            \n",
    "            except_tokens = {word_vocab['<START>'], word_vocab['<END>'], word_vocab['<NULL>']}\n",
    "            img_token = token_all.tolist()\n",
    "            img_tokens = list(map(lambda c: [w for w in c if w not in except_tokens],\n",
    "                        img_token[0]))  # remove <start> and pads\n",
    "            references.append(img_tokens)\n",
    "            \n",
    "            pred_seq = [w for w in seq if w not in except_tokens]\n",
    "            hypotheses.append(pred_seq)\n",
    "            \n",
    "            pred_caption = \"\"\n",
    "            ref_caption = \"\"\n",
    "            for i in pred_seq:\n",
    "                pred_caption += (list(word_vocab.keys())[i]) + \" \"\n",
    "            ref_caption = \"\"\n",
    "            for i in img_tokens[0]:\n",
    "                ref_caption += (list(word_vocab.keys())[i]) + \" \"\n",
    "            ref_captions = \"\"\n",
    "            for i in img_tokens:\n",
    "                for j in i:\n",
    "                    ref_captions += (list(word_vocab.keys())[j]) + \" \"\n",
    "                ref_captions += \".    \"\n",
    "                    \n",
    "    test_time = time.time() - test_start_time\n",
    "    scorers = [\n",
    "        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"])\n",
    "    ]\n",
    "\n",
    "    hypo = [[' '.join(hypo)] for hypo in [[str(x) for x in hypo] for hypo in hypotheses]]\n",
    "    ref = [[' '.join(reft) for reft in reftmp] for reftmp in\n",
    "            [[[str(x) for x in reft] for reft in reftmp] for reftmp in references]]\n",
    "    score = []\n",
    "    method = []\n",
    "\n",
    "\n",
    "    for scorer, method_i in scorers:\n",
    "        score_i, scores_i = scorer.compute_score(ref, hypo)\n",
    "        score.extend(score_i) if isinstance(score_i, list) else score.append(score_i)\n",
    "        method.extend(method_i) if isinstance(method_i, list) else method.append(method_i)\n",
    "        #print(\"{} {}\".format(method_i, score_i))\n",
    "    score_dict = dict(zip(method, score))\n",
    "\n",
    "    #get_eval_score#score_dict = get_eval_score(references, hypotheses)\n",
    "    Bleu_1 = score_dict['Bleu_1']\n",
    "    Bleu_2 = score_dict['Bleu_2']\n",
    "    Bleu_3 = score_dict['Bleu_3']\n",
    "    Bleu_4 = score_dict['Bleu_4']\n",
    "    #Meteor = score_dict['METEOR']\n",
    "\n",
    "    #print(f\"{VERSION}_{epoch} Results\")\n",
    "    #print(f'Testing:\\n Time: {test_time}s\\n BLEU-1: {Bleu_1*100}  %\\n BLEU-2: {Bleu_2*100}  %\\n BLEU-3: {Bleu_3*100}  %\\n BLEU-4: {Bleu_4*100}  %\\n Rouge: {Rouge*100}  %\\n Cider: {Cider}\\t')\n",
    "    \n",
    "    return {\n",
    "        'Bleu_1': Bleu_1,\n",
    "        'Bleu_2': Bleu_2,\n",
    "        'Bleu_3': Bleu_3,\n",
    "        'Bleu_4': Bleu_4,\n",
    "        'test_time': test_time,}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "377abba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_history_plot(hist, save_path='data/Pre-Trained Models/Finetuning/training_history.4.2.1.json'):\n",
    "    import matplotlib.pyplot as plt\n",
    "    hist = np.array(hist)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(hist[:, 0], hist[:, 1], label='Loss')\n",
    "    plt.plot(hist[:, 0], hist[:, 2], label='Top-5 Accuracy')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Training History')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path.replace('.json', '.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b766a",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30c85f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_Path = 'data/LEVIR-MCI-dataset/images'\n",
    "token_path = 'Change-Agent/Multi_change/data/LEVIR_MCI/tokens/'\n",
    "\n",
    "network='CLIP-ViT-B/32'\n",
    "clip_model_type = network.replace(\"CLIP-\", \"\")\n",
    "clip_model, preprocess = clip.load(clip_model_type,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6816e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(\n",
    "                LEVIRMCIDataset_Modified(data_folder=Dataset_Path, list_path='Change-Agent/Multi_change/data/LEVIR_MCI/',preprocess=preprocess, split='train', token_folder=token_path, vocab_file='vocab', max_length=42, allow_unk=1),\n",
    "                batch_size=8, shuffle=True, num_workers=36, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1805cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = data.DataLoader(\n",
    "                LEVIRMCIDataset_Modified(data_folder=Dataset_Path, list_path='Change-Agent/Multi_change/data/LEVIR_MCI/',preprocess=preprocess, split='val', token_folder=token_path, vocab_file='vocab', max_length=42, allow_unk=1),\n",
    "                batch_size=1, shuffle=True, num_workers=36, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08ef6a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = load_json('assets/vocab_mci.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae21e9",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3bd20",
   "metadata": {},
   "source": [
    "VERSION denotes the specifiactions of the model's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a240a23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_n_layers= 1\n",
      "decoder_type= transformer_decoder\n"
     ]
    }
   ],
   "source": [
    "VERSION = '4.3.1.16'\n",
    "layers, atten_layers, decoder_layers ,heads= VERSION.split('.')\n",
    "\n",
    "decoder = DecoderTransformer(decoder_type='transformer_decoder',embed_dim=768,\n",
    "                                    vocab_size=len(word_vocab), max_lengths=42,\n",
    "                                    word_vocab=word_vocab, n_head=8,\n",
    "                                    n_layers=int(decoder_layers), dropout=0.1,device=device).to(device) # The decoder used in RSCaMa\n",
    "\n",
    "\n",
    "encoder_trans = CrossAttentiveEncoder(n_layers=int(layers),\n",
    "                                        feature_size=[7, 7, 768],\n",
    "                                        heads=int(heads), dropout=0.1,atten_layers=int(atten_layers),device=device).to(device) #Encoder Transformer of RSCaMa with cross attention\n",
    "\n",
    "encoder = EnhancedEncoder('CLIP-ViT-B/32').to(device) # Encoder with image enhancement features\n",
    "encoder.fine_tune(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa5f71",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6be813",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbc84f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "num_epochs = 50\n",
    "\n",
    "encoder_optimizer = torch.optim.Adam(params=encoder.parameters(),\n",
    "                                            lr=1e-4)\n",
    "encoder_trans_optimizer = torch.optim.Adam(\n",
    "    params=filter(lambda p: p.requires_grad, encoder_trans.parameters()),\n",
    "    lr=1e-4)\n",
    "decoder_optimizer = torch.optim.Adam(\n",
    "    params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
    "    lr=1e-4)\n",
    "\n",
    "# Move to GPU, if available\n",
    "encoder_trans.cuda(device)\n",
    "decoder.cuda(device)\n",
    "\n",
    "encoder_lr_scheduler = torch.optim.lr_scheduler.StepLR(encoder_optimizer, step_size=5,\n",
    "                                                            gamma=1.0)\n",
    "encoder_trans_lr_scheduler = torch.optim.lr_scheduler.StepLR(encoder_trans_optimizer, step_size=5,\n",
    "                                                                    gamma=1.0)\n",
    "decoder_lr_scheduler = torch.optim.lr_scheduler.StepLR(decoder_optimizer, step_size=5,\n",
    "                                                            gamma=1.0)\n",
    "hist = np.zeros((num_epochs*2 * len(train_loader), 5))\n",
    "\n",
    "l_resizeA = torch.nn.Upsample(size = (256, 256), mode ='bilinear', align_corners = True)\n",
    "l_resizeB = torch.nn.Upsample(size = (256, 256), mode ='bilinear', align_corners = True)\n",
    "index_i = 0\n",
    "\n",
    "criterion_cap = torch.nn.CrossEntropyLoss().cuda(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c796e2d",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb0db1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5 | Index:200 | Loss: 2.205578088760376 | Top-5 Accuracy: 70.51282051282053                                                                                                                                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 56\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#del imgA\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#del imgB\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#imgA, imgB, seg_label = Image_store[id]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m feat1,feat2 \u001b[38;5;241m=\u001b[39m encoder(\n\u001b[1;32m     51\u001b[0m     imgA\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     52\u001b[0m     imgB\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     53\u001b[0m     (seg_label \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     54\u001b[0m     )\n\u001b[0;32m---> 56\u001b[0m featcap \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_trans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeat2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m scores, caps_sorted, decode_lengths, sort_ind \u001b[38;5;241m=\u001b[39m decoder(featcap, token, token_len)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Siraj TM/RSCaMa/model/model_encoder_attMamba.py:529\u001b[0m, in \u001b[0;36mCrossAttentiveEncoder.forward\u001b[0;34m(self, img_A, img_B)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;66;03m# SD-SSM:\u001b[39;00m\n\u001b[1;32m    528\u001b[0m     dif \u001b[38;5;241m=\u001b[39m img_sa2 \u001b[38;5;241m-\u001b[39m img_sa1\n\u001b[0;32m--> 529\u001b[0m     img_sa1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCaMalayer_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_sa1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdif\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    530\u001b[0m     img_sa2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCaMalayer_list[i][\u001b[38;5;241m1\u001b[39m](inputs_embeds\u001b[38;5;241m=\u001b[39mimg_sa2, inputs_embeds_2\u001b[38;5;241m=\u001b[39mdif)\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# TT-SSM:\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Siraj TM/RSCaMa/model/mamba_block.py:254\u001b[0m, in \u001b[0;36mCaMambaModel.forward\u001b[0;34m(self, inputs_embeds, inputs_embeds_2, cache_params, use_cache, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(mixer_block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, hidden_states, cache_params)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmixer_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_params_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_params_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    257\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Siraj TM/RSCaMa/model/mamba_block.py:208\u001b[0m, in \u001b[0;36mCaMambaBlock.forward\u001b[0;34m(self, hidden_states, hidden_states_2, cache_params, cache_params_2)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_in_fp32:\n\u001b[1;32m    206\u001b[0m     residual \u001b[38;5;241m=\u001b[39m residual\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 208\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_params_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_params_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Siraj TM/RSCaMa/model/mamba_block.py:180\u001b[0m, in \u001b[0;36mMambaMixer.forward\u001b[0;34m(self, hidden_states, hidden_states_2, cache_params, cache_params_2)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, hidden_states_2, cache_params: Optional[MambaCache] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, cache_params_2: Optional[MambaCache] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_fast_path_available:\u001b[38;5;66;03m# and \"cuda\" in self.x_proj.weight.device.type:\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda_kernels_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_params_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe fast path is not available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Siraj TM/RSCaMa/model/mamba_block.py:133\u001b[0m, in \u001b[0;36mMambaMixer.cuda_kernels_forward\u001b[0;34m(self, hidden_states, hidden_states_dif, cache_params, cache_params_2)\u001b[0m\n\u001b[1;32m    129\u001b[0m ssm_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_proj(hidden_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    130\u001b[0m time_step, B, C \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(\n\u001b[1;32m    131\u001b[0m     ssm_parameters, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_step_rank, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssm_state_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssm_state_size], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    132\u001b[0m )\n\u001b[0;32m--> 133\u001b[0m discrete_time_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m    135\u001b[0m ssm_parameters_back \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_proj_back(hidden_states_back\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_freq = 5000\n",
    "EPOCHS = num_epochs\n",
    "index_i = 0\n",
    "network='CLIP-ViT-B/32'\n",
    "clip_model_type = network.replace(\"CLIP-\", \"\")\n",
    "clip_model, preprocess = clip.load(clip_model_type,device=device)\n",
    "\n",
    "encoder.train()\n",
    "encoder_trans.train()\n",
    "decoder.train()\n",
    "\n",
    "decoder_optimizer.zero_grad()\n",
    "encoder_trans_optimizer.zero_grad()\n",
    "\n",
    "#Image_store = {id:get_image(batch_data[-1],preprocess=preprocess) for id,batch_data in tqdm(enumerate(train_loader))}\n",
    "\n",
    "benchmark = {}\n",
    "MAX_SCORE = [0]\n",
    "Prev_Epoch = False\n",
    "for epoch in range(5,EPOCHS):\n",
    "    loss_set = []\n",
    "    acc_set = []\n",
    "    for id,batch_data in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        imgA, imgB, seg_label, token_all, token_all_len, token, token_len, name = batch_data\n",
    "\n",
    "        #Texts = get_text_inputs(name,split='train')\n",
    "        start_time = time.time()\n",
    "        accum_steps = 64//64\n",
    "        \n",
    "        #Getting Data and moving to GPU if possible\n",
    "\n",
    "        #imgA = imgA.cuda(device)\n",
    "        #imgB = imgB.cuda(device)\n",
    "        #imgSM = imgSM.cuda(device)\n",
    "        token = token.cuda(device) \n",
    "        token_len = token_len.cuda(device)\n",
    "\n",
    "        #Feat1 and Feat2\n",
    "        '''with torch.no_grad():\n",
    "            \n",
    "            imgSM = ChangeCLIP.forward(xA=imgA,xB=imgB,Texts=Texts)'''\n",
    "        \n",
    "        #del imgA\n",
    "        #del imgB\n",
    "        \n",
    "        #imgA, imgB, seg_label = Image_store[id]\n",
    "\n",
    "        feat1,feat2 = encoder(\n",
    "            imgA.to(device),\n",
    "            imgB.to(device),\n",
    "            (seg_label > 0).int().to(device)\n",
    "            )\n",
    "                \n",
    "        featcap = encoder_trans(feat1,feat2)\n",
    "        \n",
    "        scores, caps_sorted, decode_lengths, sort_ind = decoder(featcap, token, token_len)\n",
    "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "        targets = caps_sorted[:, 1:]\n",
    "        \n",
    "        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n",
    "        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
    "        \n",
    "        '''seg_targets = imgSM[:,:,:,:1].permute(0,3,1,2)\n",
    "        seg_targets = seg_targets.float()  # Convert to float\n",
    "        \n",
    "        if seg_targets.max() > 1.0:\n",
    "            seg_targets = seg_targets / 255.0  # Normalize if needed\n",
    "        '''\n",
    "        # Ensure targets are long integers for captioning\n",
    "        \n",
    "        targets = targets.long()\n",
    "        loss = criterion_cap(scores, targets.to(torch.int64))\n",
    "        \n",
    "        loss = loss / accum_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if (id + 1) % accum_steps == 0 or (id + 1) == len(train_loader):\n",
    "            decoder_optimizer.step()\n",
    "            encoder_trans_optimizer.step()\n",
    "\n",
    "            # Adjust learning rate\n",
    "            decoder_lr_scheduler.step()\n",
    "            encoder_trans_lr_scheduler.step()\n",
    "\n",
    "            decoder_optimizer.zero_grad()\n",
    "            encoder_trans_optimizer.zero_grad()\n",
    "\n",
    "                \n",
    "        hist[index_i, 0] = time.time() - start_time #batch_time\n",
    "        hist[index_i, 1] = loss.item()  # train_loss\n",
    "        \n",
    "        accuracy = accuracy_v0(scores, targets, 5)\n",
    "        \n",
    "        hist[index_i, 2] = accuracy #top5\n",
    "        \n",
    "        index_i += 1\n",
    "        \n",
    "        if index_i % 5 == 0:\n",
    "            rem_print(f'Training Epoch: {epoch} | Index:{index_i} | Loss: {loss} | Top-5 Accuracy: {accuracy} ')\n",
    "        loss_set.append(loss.item())\n",
    "        acc_set.append(accuracy)\n",
    "\n",
    "    #print(f'Training Epoch: {epoch} | Index:{index_i} | Mean Loss: {np.mean(loss_set)}\\n')\n",
    "    \n",
    "    if (epoch % 5 == 0 and epoch) or Prev_Epoch:\n",
    "        print(f'Training Epoch: {epoch} | Index:{index_i} | Mean Loss: {np.mean(loss_set)} | Mean Accuracy : {np.mean(acc_set)}\\n')\n",
    "        benchmark[epoch] = model_validation(encoder, encoder_trans, decoder, val_loader)\n",
    "        print('\\n')\n",
    "        save_json(benchmark, f'data/Pre-Trained Models/Finetuning/benchmark.{VERSION}.json')\n",
    "\n",
    "        if benchmark[epoch]['Bleu_1']*100 > MAX_SCORE[0]:\n",
    "            MAX_SCORE[0] = benchmark[epoch]['Bleu_4'] * 100\n",
    "            print(f'\\nNew Best Score: {MAX_SCORE[0]} at epoch {epoch}\\n')\n",
    "            \n",
    "            torch.save(encoder.state_dict(),f'data/Pre-Trained Models/Finetuning/encoder_{VERSION}_best.pt')\n",
    "            torch.save(encoder_trans.state_dict(),f'data/Pre-Trained Models/Finetuning/encoder_trans_{VERSION}_best.pt')\n",
    "            torch.save(decoder.state_dict(),f'data/Pre-Trained Models/Finetuning/decoder_{VERSION}_best.pt')\n",
    "            \n",
    "            Prev_Epoch = True\n",
    "        else:\n",
    "            print(f'No Improvement at epoch {epoch}, Previous Best Bleu_1: {MAX_SCORE[0]}')\n",
    "            Prev_Epoch = False\n",
    "    print('\\n')\n",
    "    save_json(hist.tolist(), f'data/Pre-Trained Models/Finetuning/training_history.{VERSION}.json')\n",
    "\n",
    "#torch.save(encoder.state_dict(),f'data/Pre-Trained Models/Finetuning/encoder_{VERSION}_{epoch}.pt')\n",
    "#torch.save(encoder_trans.state_dict(),f'data/Pre-Trained Models/Finetuning/encoder_trans_{VERSION}_{epoch}.pt')\n",
    "#torch.save(decoder.state_dict(),f'data/Pre-Trained Models/Finetuning/decoder_{VERSION}_{epoch}.pt')\n",
    "\n",
    "save_json(benchmark, f'data/Pre-Trained Models/Finetuning/benchmark.{VERSION}.json')\n",
    "save_json(hist.tolist(), f'data/Pre-Trained Models/Finetuning/training_history.{VERSION}.json')\n",
    "plot_metrics_over_epochs(benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d302943d",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a39c4",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51a2e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = data.DataLoader(\n",
    "                LEVIRMCIDataset_Modified(data_folder=Dataset_Path, list_path='Change-Agent/Multi_change/data/LEVIR_MCI/',preprocess=preprocess, split='test', token_folder=token_path, vocab_file='vocab', max_length=42, allow_unk=1),\n",
    "                batch_size=1, shuffle=True, num_workers=24, pin_memory=True)\n",
    "\n",
    "nochange_list = [\"the scene is the same as before \", \"there is no difference \",\n",
    "                         \"the two scenes seem identical \", \"no change has occurred \",\n",
    "                         \"almost nothing has changed \"]\n",
    "\n",
    "l_resize1 = torch.nn.Upsample(size=(256, 256), mode='bilinear', align_corners=True)\n",
    "l_resize2 = torch.nn.Upsample(size=(256, 256), mode='bilinear', align_corners=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf935394",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33a3c334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(\n",
    "    torch.load(f'data/Pre-Trained Models/Finetuning/encoder_{VERSION}_best.pt')\n",
    ")\n",
    "encoder_trans.load_state_dict(\n",
    "    torch.load(f'data/Pre-Trained Models/Finetuning/encoder_trans_{VERSION}_best.pt')\n",
    ")\n",
    "decoder.load_state_dict(\n",
    "    torch.load(f'data/Pre-Trained Models/Finetuning/decoder_{VERSION}_best.pt')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5591919",
   "metadata": {},
   "source": [
    "### Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9176d592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Obtaining dependency information for torchinfo from https://files.pythonhosted.org/packages/72/25/973bd6128381951b23cdcd8a9870c6dcfc5606cb864df8eabd82e529f9c1/torchinfo-1.8.0-py3-none-any.whl.metadata\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b9734e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " INSPECTING MODEL INPUTS\n",
      "==================================================\n",
      "imgA shape: torch.Size([1, 3, 224, 224]), dtype: torch.float32, device: cpu\n",
      "imgB shape: torch.Size([1, 3, 224, 224]), dtype: torch.float32, device: cpu\n",
      "seg_input shape: torch.Size([1, 1, 224, 224]), dtype: torch.int32, device: cpu\n",
      " seg_input Min value: 0, Max value: 1\n",
      "==================================================\n",
      "\n",
      "--- Encoder Summary ---\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Encoder Summary ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Move model and data to CPU for the summary to isolate the issue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m summary(\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     38\u001b[0m         input_data\u001b[38;5;241m=\u001b[39m[imgA_sample\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), imgB_sample\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), seg_input\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)],\n\u001b[1;32m     39\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m         col_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_params\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmult_adds\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# --- Decoder Summary ---\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# To summarize the decoder, you need the output shape from the encoder part.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# We'll run a single forward pass on the CPU with the sample data to get it.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "#  Add this at the VERY TOP of your script for a more accurate error traceback\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# ... (rest of your imports, model definitions, etc.) ...\n",
    "\n",
    "# ===================================================================\n",
    "#  GET MODEL SUMMARIES HERE\n",
    "# ===================================================================\n",
    "\n",
    "# Get a sample batch from your dataloader\n",
    "# These tensors will likely be on your GPU by default\n",
    "sample_batch = next(iter(test_loader))\n",
    "imgA_sample, imgB_sample, seg_label_sample, _, _, _, _, _ = sample_batch\n",
    "\n",
    "\n",
    "# ---  STEP 1: Inspect your inputs ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" INSPECTING MODEL INPUTS\")\n",
    "print(\"=\"*50)\n",
    "seg_input = (seg_label_sample > 0).int()\n",
    "print(f\"imgA shape: {imgA_sample.shape}, dtype: {imgA_sample.dtype}, device: {imgA_sample.device}\")\n",
    "print(f\"imgB shape: {imgB_sample.shape}, dtype: {imgB_sample.dtype}, device: {imgB_sample.device}\")\n",
    "print(f\"seg_input shape: {seg_input.shape}, dtype: {seg_input.dtype}, device: {seg_input.device}\")\n",
    "print(f\" seg_input Min value: {seg_input.min()}, Max value: {seg_input.max()}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "# The error is very likely caused by the min/max values of `seg_input`.\n",
    "# Check if these values are valid for your model (e.g., not negative for an embedding layer).\n",
    "\n",
    "\n",
    "# --- STEP 2: Run the summary on the CPU ---\n",
    "print(\"--- Encoder Summary ---\")\n",
    "# Move model and data to CPU for the summary to isolate the issue\n",
    "summary(encoder.to('cpu'),\n",
    "        input_data=[imgA_sample.to('cpu'), imgB_sample.to('cpu'), seg_input.to('cpu')],\n",
    "        device=\"cpu\",\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n",
    "\n",
    "# --- Decoder Summary ---\n",
    "# To summarize the decoder, you need the output shape from the encoder part.\n",
    "# We'll run a single forward pass on the CPU with the sample data to get it.\n",
    "with torch.no_grad():\n",
    "    feat1_sample, feat2_sample = encoder.to('cpu')(imgA_sample.to('cpu'), imgB_sample.to('cpu'), (seg_label_sample > 0).int().to('cpu'))\n",
    "    feat_sample = encoder_trans.to('cpu')(feat1_sample.to('cpu'), feat2_sample.to('cpu'))\n",
    "    decoder_input_shape = feat_sample.shape\n",
    "\n",
    "# ... (Your code to get feat_sample and define the device) ...\n",
    "# Ensure all sample data and the model are on the same device\n",
    "feat_sample = feat_sample.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "\n",
    "print(\"\\n--- Decoder Summary ---\")\n",
    "# The traceback shows the decoder's forward pass needs caption data (for teacher-forcing).\n",
    "# We'll create dummy tensors with appropriate shapes to satisfy the model's signature.\n",
    "\n",
    "# Get the batch size from your feature sample\n",
    "batch_size = feat_sample.shape[0]\n",
    "\n",
    "# 1. Create a dummy tensor for `encoded_captions`\n",
    "# This is usually shaped (batch_size, max_caption_length).\n",
    "# Adjust `max_caption_length` if you know your model's specific value.\n",
    "max_caption_length = 42\n",
    "dummy_captions = torch.randint(low=1, high=1000, size=(batch_size, max_caption_length), device=device)\n",
    "\n",
    "# 2. Create a dummy tensor for `caption_lengths`\n",
    "# This is usually shaped (batch_size,)\n",
    "dummy_lengths = torch.full(size=(batch_size,), fill_value=max_caption_length, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "# 3. Provide all required arguments as a list to the `input_data` parameter\n",
    "summary(decoder.to('cpu'),\n",
    "        input_data=[feat_sample.to('cpu'), dummy_captions.to('cpu'), dummy_lengths.to('cpu')],\n",
    "        device='cpu',\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n",
    "\n",
    "# Move models back to GPU for testing\n",
    "encoder.to(device)\n",
    "encoder_trans.to(device)\n",
    "decoder.to(device)\n",
    "# ===================================================================\n",
    "\n",
    "\n",
    "# Your existing testing loop starts here\n",
    "test_start_time = time.time()\n",
    "references = list()\n",
    "hypotheses = list()\n",
    "\n",
    "# ... (the rest of your testing loop code) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a49c19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossAttentiveEncoder(\n",
       "  (h_embedding): Embedding(7, 384)\n",
       "  (w_embedding): Embedding(7, 384)\n",
       "  (CaMalayer_list): ModuleList(\n",
       "    (0-3): 4 x ModuleList(\n",
       "      (0-1): 2 x CaMambaModel(\n",
       "        (embeddings): Embedding(50280, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0): CaMambaBlock(\n",
       "            (norm): MambaRMSNorm()\n",
       "            (mixer): MambaMixer(\n",
       "              (conv1d): Conv1d(1536, 1536, kernel_size=(3,), stride=(1,), padding=(2,), groups=1536)\n",
       "              (conv1d_back): Conv1d(1536, 1536, kernel_size=(3,), stride=(1,), padding=(2,), groups=1536)\n",
       "              (act): SiLU()\n",
       "              (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (in_proj_dif): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
       "              (x_proj_back): Linear(in_features=1536, out_features=80, bias=False)\n",
       "              (x_proj_dif): Linear(in_features=1536, out_features=80, bias=False)\n",
       "              (x_proj_dif_back): Linear(in_features=1536, out_features=80, bias=False)\n",
       "              (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
       "              (dt_proj_back): Linear(in_features=48, out_features=1536, bias=True)\n",
       "              (dt_proj_dif): Linear(in_features=48, out_features=1536, bias=True)\n",
       "              (dt_proj_dif_back): Linear(in_features=48, out_features=1536, bias=True)\n",
       "              (linear_hid2): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (linear_hid2_back): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
       "              (out_LN): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm_f): MambaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fuselayer_list): ModuleList(\n",
       "    (0-3): 4 x ModuleList(\n",
       "      (0-1): 2 x CaMambaModel(\n",
       "        (embeddings): Embedding(50280, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0): CaMambaBlock(\n",
       "            (norm): MambaRMSNorm()\n",
       "            (mixer): MambaMixer(\n",
       "              (conv1d): Conv1d(1536, 1536, kernel_size=(3,), stride=(1,), padding=(2,), groups=1536)\n",
       "              (conv1d_back): Conv1d(1536, 1536, kernel_size=(3,), stride=(1,), padding=(2,), groups=1536)\n",
       "              (act): SiLU()\n",
       "              (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (in_proj_dif): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
       "              (x_proj_back): Linear(in_features=1536, out_features=80, bias=False)\n",
       "              (x_proj_dif): Linear(in_features=1536, out_features=80, bias=False)\n",
       "              (x_proj_dif_back): Linear(in_features=1536, out_features=80, bias=False)\n",
       "              (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
       "              (dt_proj_back): Linear(in_features=48, out_features=1536, bias=True)\n",
       "              (dt_proj_dif): Linear(in_features=48, out_features=1536, bias=True)\n",
       "              (dt_proj_dif_back): Linear(in_features=48, out_features=1536, bias=True)\n",
       "              (linear_hid2): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (linear_hid2_back): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
       "              (out_LN): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm_f): MambaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fuselayer_list_2): ModuleList()\n",
       "  (linear_dif): ModuleList()\n",
       "  (linear_img1): ModuleList()\n",
       "  (linear_img2): ModuleList()\n",
       "  (Dyconv_img1_list): ModuleList()\n",
       "  (Dyconv_img2_list): ModuleList()\n",
       "  (Conv1_list): ModuleList(\n",
       "    (0-3): 4 x Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (LN_list): ModuleList(\n",
       "    (0-3): 4 x resblock(\n",
       "      (left): Sequential(\n",
       "        (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU()\n",
       "        (5): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (6): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (act): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (act): Tanh()\n",
       "  (layerscan): CaMambaModel(\n",
       "    (embeddings): Embedding(50280, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0): CaMambaBlock(\n",
       "        (norm): MambaRMSNorm()\n",
       "        (mixer): MambaMixer(\n",
       "          (conv1d): Conv1d(1536, 1536, kernel_size=(3,), stride=(1,), padding=(2,), groups=1536)\n",
       "          (conv1d_back): Conv1d(1536, 1536, kernel_size=(3,), stride=(1,), padding=(2,), groups=1536)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (in_proj_dif): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
       "          (x_proj_back): Linear(in_features=1536, out_features=80, bias=False)\n",
       "          (x_proj_dif): Linear(in_features=1536, out_features=80, bias=False)\n",
       "          (x_proj_dif_back): Linear(in_features=1536, out_features=80, bias=False)\n",
       "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
       "          (dt_proj_back): Linear(in_features=48, out_features=1536, bias=True)\n",
       "          (dt_proj_dif): Linear(in_features=48, out_features=1536, bias=True)\n",
       "          (dt_proj_dif_back): Linear(in_features=48, out_features=1536, bias=True)\n",
       "          (linear_hid2): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (linear_hid2_back): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
       "          (out_LN): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): MambaRMSNorm()\n",
       "  )\n",
       "  (LN_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (cos): CosineSimilarity()\n",
       "  (cross_attention): CrossAttentionEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x CrossAttentionLayer(\n",
       "        (cross_attn_A_to_B): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (cross_attn_B_to_A): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm_A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_trans.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51567faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1016,  0.7099,  0.1865,  ..., -0.0747,  0.3210, -0.1381],\n",
       "         [ 0.2476,  1.2200,  0.0899,  ...,  0.0731,  0.1660, -0.0896],\n",
       "         [ 0.5294,  1.1039,  0.0724,  ..., -0.2011,  0.3734,  0.5038],\n",
       "         ...,\n",
       "         [ 0.2633,  0.4328,  0.0766,  ..., -0.2279,  0.4134,  0.0924],\n",
       "         [ 0.5186,  0.9856,  0.2324,  ...,  0.0081,  0.4800, -0.4232],\n",
       "         [-0.1222,  0.8506,  0.0174,  ..., -0.0981,  0.4955, -0.1269]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat1_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3926d418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_ EVALUATING AT BEAM SIZE 1:  82%| | 1579/1929 [01:31<00:20, 17.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 34\u001b[0m\n\u001b[1;32m     27\u001b[0m feat1,feat2 \u001b[38;5;241m=\u001b[39m encoder(\n\u001b[1;32m     28\u001b[0m     imgA\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     29\u001b[0m     imgB\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     30\u001b[0m     (seg_label \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#eat1,feat2,feat3 = encoder(imgA.to(device),imgB.to(device),seg_label.to(device))\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m feat \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_trans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeat2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m seq \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39msample(feat,k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m except_tokens \u001b[38;5;241m=\u001b[39m {word_vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<START>\u001b[39m\u001b[38;5;124m'\u001b[39m], word_vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<END>\u001b[39m\u001b[38;5;124m'\u001b[39m], word_vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<NULL>\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Siraj TM/RSCaMa/model/model_encoder_attMamba.py:517\u001b[0m, in \u001b[0;36mCrossAttentiveEncoder.forward\u001b[0;34m(self, img_A, img_B)\u001b[0m\n\u001b[1;32m    514\u001b[0m img_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_pos_embedding(img_A)\n\u001b[1;32m    515\u001b[0m img_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_pos_embedding(img_B)\n\u001b[0;32m--> 517\u001b[0m img_A,img_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_B\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# captioning\u001b[39;00m\n\u001b[1;32m    519\u001b[0m batch, c \u001b[38;5;241m=\u001b[39m img_A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], img_A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Siraj TM/RSCaMa/model/model_encoder_attMamba.py:272\u001b[0m, in \u001b[0;36mCrossAttentionEncoder.forward\u001b[0;34m(self, feat_A, feat_B)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, feat_A, feat_B):\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 272\u001b[0m         feat_A, feat_B \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_B\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m feat_A, feat_B\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Siraj TM/RSCaMa/model/model_encoder_attMamba.py:254\u001b[0m, in \u001b[0;36mCrossAttentionLayer.forward\u001b[0;34m(self, feat_A, feat_B)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, feat_A, feat_B):\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# A attends to B\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     A_attn, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn_A_to_B\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeat_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeat_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeat_B\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     feat_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_A(feat_A \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(A_attn))\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# B attends to A\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/activation.py:1207\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1192\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1203\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1205\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1206\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m-> 1207\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   1208\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_zero_attn,\n\u001b[1;32m   1209\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1210\u001b[0m         training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1211\u001b[0m         key_padding_mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1212\u001b[0m         need_weights\u001b[38;5;241m=\u001b[39mneed_weights,\n\u001b[1;32m   1213\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1214\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1215\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/envs/Siraj/lib/python3.9/site-packages/torch/nn/modules/module.py:1605\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1603\u001b[0m     _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m   1604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _parameters:\n\u001b[0;32m-> 1605\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_buffers\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1607\u001b[0m     _buffers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_buffers\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_start_time = time.time()\n",
    "references = list()  # references (true captions) for calculating BLEU-4 score\n",
    "hypotheses = list()  # hypotheses (predictions)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Batches\n",
    "    for idx,batch_data in enumerate(tqdm(test_loader,desc='test_' + \" EVALUATING AT BEAM SIZE \" + str(1))):\n",
    "        \n",
    "        imgA, imgB, seg_label, token_all, token_all_len, token, token_len, name = batch_data\n",
    "        \n",
    "        #Getting Data and moving to GPU if possible\n",
    "        imgA = imgA.cuda(device)\n",
    "        imgB = imgB.cuda(device)\n",
    "        #imgSM = imgSM.cuda(device)\n",
    "        token = token.cuda(device)\n",
    "        token_len = token_len.cuda(device)\n",
    "        \n",
    "        #Texts = get_text_inputs(name,split='test')\n",
    "        \n",
    "        \n",
    "        \n",
    "        #imgSM = ChangeCLIP.forward(xA=imgA,xB=imgB,Texts=Texts)\n",
    "        \n",
    "        #imgA, imgB, seg_label = Image_store_test[idx]\n",
    "\n",
    "        feat1,feat2 = encoder(\n",
    "            imgA.to(device),\n",
    "            imgB.to(device),\n",
    "            (seg_label > 0).int().to(device)\n",
    "            )\n",
    "\n",
    "        #eat1,feat2,feat3 = encoder(imgA.to(device),imgB.to(device),seg_label.to(device))\n",
    "        feat = encoder_trans(feat1,feat2)\n",
    "        seq = decoder.sample(feat,k=1)\n",
    "        \n",
    "        except_tokens = {word_vocab['<START>'], word_vocab['<END>'], word_vocab['<NULL>']}\n",
    "        img_token = token_all.tolist()\n",
    "        img_tokens = list(map(lambda c: [w for w in c if w not in except_tokens],\n",
    "                    img_token[0]))  # remove <start> and pads\n",
    "        references.append(img_tokens)\n",
    "        \n",
    "        pred_seq = [w for w in seq if w not in except_tokens]\n",
    "        hypotheses.append(pred_seq)\n",
    "        \n",
    "        pred_caption = \"\"\n",
    "        ref_caption = \"\"\n",
    "        for i in pred_seq:\n",
    "            pred_caption += (list(word_vocab.keys())[i]) + \" \"\n",
    "        ref_caption = \"\"\n",
    "        for i in img_tokens[0]:\n",
    "            ref_caption += (list(word_vocab.keys())[i]) + \" \"\n",
    "        ref_captions = \"\"\n",
    "        for i in img_tokens:\n",
    "            for j in i:\n",
    "                ref_captions += (list(word_vocab.keys())[j]) + \" \"\n",
    "            ref_captions += \".    \"\n",
    "            \n",
    "        test_time = time.time() - test_start_time\n",
    "\n",
    "        # Fast test during the training\n",
    "        # Calculate evaluation scores\n",
    "        \n",
    "\n",
    "from eval_func.bleu.bleu import Bleu\n",
    "from eval_func.rouge.rouge import Rouge\n",
    "from eval_func.cider.cider import Cider\n",
    "from eval_func.meteor.meteor import Meteor\n",
    "\n",
    "scorers = [\n",
    "    (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "    #(Meteor(), \"METEOR\"),\n",
    "    (Rouge(), \"ROUGE_L\"),\n",
    "    (Cider(), \"CIDEr\")\n",
    "]\n",
    "\n",
    "hypo = [[' '.join(hypo)] for hypo in [[str(x) for x in hypo] for hypo in tqdm(hypotheses)]]\n",
    "ref = [[' '.join(reft) for reft in reftmp] for reftmp in\n",
    "        [[[str(x) for x in reft] for reft in reftmp] for reftmp in tqdm(references)]]\n",
    "score = []\n",
    "method = []\n",
    "\n",
    "\n",
    "for scorer, method_i in tqdm(scorers):\n",
    "    score_i, scores_i = scorer.compute_score(ref, hypo)\n",
    "    score.extend(score_i) if isinstance(score_i, list) else score.append(score_i)\n",
    "    method.extend(method_i) if isinstance(method_i, list) else method.append(method_i)\n",
    "    #print(\"{} {}\".format(method_i, score_i))\n",
    "score_dict = dict(zip(method, score))\n",
    "\n",
    "get_eval_score#score_dict = get_eval_score(references, hypotheses)\n",
    "Bleu_1 = score_dict['Bleu_1']\n",
    "Bleu_2 = score_dict['Bleu_2']\n",
    "Bleu_3 = score_dict['Bleu_3']\n",
    "Bleu_4 = score_dict['Bleu_4']\n",
    "#Meteor = score_dict['METEOR']\n",
    "Rouge = score_dict['ROUGE_L']\n",
    "Cider = score_dict['CIDEr']\n",
    "\n",
    "print(f\"{VERSION} Results\")\n",
    "print(f'Testing:\\n Time: {test_time}s\\n BLEU-1: {Bleu_1*100}  %\\n BLEU-2: {Bleu_2*100}  %\\n BLEU-3: {Bleu_3*100}  %\\n BLEU-4: {Bleu_4*100}  %\\n Rouge: {Rouge*100}  %\\n Cider: {Cider}\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b5921",
   "metadata": {},
   "source": [
    "# Segmenting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110cc11a",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1fa961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "random.seed(3) #3\n",
    "Binarize = False\n",
    "\n",
    "# Assuming RSCD_Dict and mod are already defined\n",
    "for itr in range(5):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20, 15))  # Adjust figsize for better spacing\n",
    "    random_indices = [random.randint(1, 1900) for _ in range(1)]\n",
    "    ChangeCLIP.eval()\n",
    "\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        \n",
    "        batch_data = test_loader.dataset.__getitem__(idx)\n",
    "        \n",
    "        imgA, imgB, seg_label, token_all, token_all_len, token, token_len, name = batch_data\n",
    "        \n",
    "        Ground_Truths = [token_to_text(tokens) for tokens in token_all]\n",
    "        \n",
    "        Before = torch.tensor(imgA)\n",
    "        After = torch.tensor(imgB)\n",
    "        Ground_Truth = torch.tensor(seg_label)\n",
    "        \n",
    "        Texts =  [\"l\" for _ in range(4)]  #get_text_inputs(name,split='test')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            Pred = ChangeCLIP(xA=Before.unsqueeze(0).to(device), xB=After.unsqueeze(0).to(device), Texts=Texts)\n",
    "            feat1,feat2,feat3 = encoder(manual_preprocess(Before).unsqueeze(0).to(device),manual_preprocess(After).unsqueeze(0).to(device),manual_preprocess(Pred).to(device))\n",
    "            featcap = encoder_trans(feat1,feat2,feat3)\n",
    "            seq = decoder.sample(featcap,k=1)\n",
    "            \n",
    "        Pred_UB = Pred[0].permute(1,2,0).cpu().detach().numpy()\n",
    "        Pred_B = binarize(Pred)[0].permute(1,2,0).cpu().detach().numpy()\n",
    "\n",
    "        pred_seq = [w for w in seq if w not in except_tokens]\n",
    "        caption = [invert[token] for token in pred_seq]\n",
    "        \n",
    "        output = ''\n",
    "        for word in caption:\n",
    "            output += word + ' '\n",
    "            \n",
    "        print(f\"Predicted_Caption : {output}\")\n",
    "        for j,ground_truth in enumerate(Ground_Truths):\n",
    "            print(f'Ground Truth {j} : {ground_truth}')\n",
    "        print('\\n')\n",
    "        \n",
    "\n",
    "        # Plotting the panels for each index\n",
    "        axes[0].imshow(Before.permute(1,2,0))\n",
    "        axes[0].set_title(f'Before\\nIndex: {idx}')\n",
    "        axes[0].axis('off')\n",
    "        axes[1].imshow(After.permute(1,2,0))\n",
    "        axes[1].set_title(f'After\\nIndex: {idx}')\n",
    "        axes[1].axis('off')\n",
    "        axes[2].imshow(Ground_Truth, cmap='gray')\n",
    "        axes[2].set_title(f'Ground Truth\\nIndex: {idx}')\n",
    "        axes[2].axis('off')\n",
    "        axes[3].imshow(Pred_B, cmap='gray')\n",
    "        axes[3].set_title(f'Prediction\\nIndex: {idx}')\n",
    "        axes[3].axis('off')\n",
    "        axes[4].imshow(Pred_UB, cmap='gray')\n",
    "        axes[4].set_title(f'Prediction Unbinarized\\nIndex: {idx}')\n",
    "        axes[4].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure after completing all plots\n",
    "    #plt.savefig(f'/home/guest/Documents/Siraj TM/DATA/Predictions_{itr}.png', dpi=200)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a75f47",
   "metadata": {},
   "source": [
    "# Both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d83edb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Siraj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
